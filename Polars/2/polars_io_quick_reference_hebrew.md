# ğŸ“š ××“×¨×™×š ××”×™×¨: Polars I/O

## ×ª×•×›×Ÿ ×¢× ×™×™× ×™× ××”×™×¨
- [CSV](#csv)
- [Parquet](#parquet)
- [Delta Lake](#delta-lake)
- [JSON](#json)
- [Excel](#excel)
- [×¤×•×¨××˜×™× × ×•×¡×¤×™×](#×¤×•×¨××˜×™×-× ×•×¡×¤×™×)
- [×§×‘×¦×™× ××¨×•×‘×™×](#×§×‘×¦×™×-××¨×•×‘×™×)
- [×‘×¡×™×¡×™ × ×ª×•× ×™×](#×‘×¡×™×¡×™-× ×ª×•× ×™×)
- [×˜×™×¤×™× ×—×©×•×‘×™×](#×˜×™×¤×™×-×—×©×•×‘×™×)

---

## CSV

### ×§×¨×™××” (Read)

```python
import polars as pl

# ×‘×¡×™×¡×™
df = pl.read_csv('file.csv')

# ××ª×§×“×
df = pl.read_csv(
    'file.csv',
    has_header=False,              # ××™×Ÿ ×›×•×ª×¨×•×ª
    new_columns=['col1', 'col2'],  # ×©××•×ª ×¢××•×“×•×ª
    try_parse_dates=True,          # ×”××¨×ª ×ª××¨×™×›×™×
    schema_overrides={             # ×˜×™×¤×•×¡×™ × ×ª×•× ×™×
        'age': pl.Int8,
        'quantity': pl.Int32
    },
    separator='|',                 # ××¤×¨×™×“ ××—×¨
    encoding='utf8'                # ×§×™×“×•×“
)

# Lazy (×™×¢×™×œ!)
lf = pl.scan_csv('file.csv')
result = lf.filter(...).collect()
```

### ×›×ª×™×‘×” (Write)

```python
# ×‘×¡×™×¡×™
df.write_csv('output.csv')

# ××ª×§×“×
df.write_csv(
    'output.csv',
    include_header=False,          # ×œ×œ× ×›×•×ª×¨×•×ª
    separator='|',                 # ××¤×¨×™×“ ××—×¨
    datetime_format='%Y-%m-%d'     # ×¤×•×¨××˜ ×ª××¨×™×›×™×
)

# Streaming (×œ×§×‘×¦×™× ×¢× ×§×™×™×)
lf.sink_csv('output.csv')
```

---

## Parquet

### ×§×¨×™××”

```python
# ×‘×¡×™×¡×™
df = pl.read_parquet('file.parquet')

# ×¢××•×“×•×ª ×¡×¤×¦×™×¤×™×•×ª
df = pl.read_parquet(
    'file.parquet',
    columns=['col1', 'col2']      # ×¨×§ ××œ×”!
)

# ×¨×§ ×”×¡×›××”
schema = pl.read_parquet_schema('file.parquet')

# Lazy
lf = pl.scan_parquet('file.parquet')

# Partitioned
df = pl.read_parquet(
    'data_partitioned/',
    use_pyarrow=True,
    pyarrow_options={'partitioning': 'hive'}
)
```

### ×›×ª×™×‘×”

```python
# ×‘×¡×™×¡×™
df.write_parquet('output.parquet')

# ×¢× ×“×—×™×¡×”
df.write_parquet(
    'output.parquet',
    compression='zstd',           # ××•: snappy, lz4, gzip
    compression_level=10          # 1-22
)

# Partitioned
df.write_parquet(
    'output_partitioned/',
    use_pyarrow=True,
    pyarrow_options={
        'partition_cols': ['year', 'category'],
        'existing_data_behavior': 'overwrite_or_ignore'
    }
)

# Streaming
lf.sink_parquet('output.parquet')
```

---

## Delta Lake

### ×§×¨×™××”

```python
# ×‘×¡×™×¡×™
df = pl.read_delta('delta_table/')

# Lazy
lf = pl.scan_delta('delta_table/')

# Partitioned - ×—×œ×§ ×¡×¤×¦×™×¤×™
df = pl.read_delta(
    'delta_partitioned/',
    pyarrow_options={
        'partitions': [('year', '=', '2023')]
    }
)

# ××¢× ×Ÿ (S3)
df = pl.read_delta(
    's3://bucket/delta_table',
    storage_options={
        'aws_access_key_id': 'KEY',
        'aws_secret_access_key': 'SECRET',
        'aws_region': 'us-east-1'
    }
)
```

### ×›×ª×™×‘×”

```python
# ×™×¦×™×¨×” / ×”×—×œ×¤×”
df.write_delta(
    'delta_table/',
    mode='overwrite'              # ××•: append, error
)

# ×”×•×¡×¤×”
df.write_delta('delta_table/', mode='append')

# ×¢× Partitioning
df.write_delta(
    'delta_partitioned/',
    mode='overwrite',
    delta_write_options={
        'partition_by': 'category'
    }
)
```

---

## JSON

### ×§×¨×™××”

```python
# JSON ×¨×’×™×œ
df = pl.read_json('file.json')

# NDJSON (×©×•×¨×” ××—×¨ ×©×•×¨×”)
df = pl.read_ndjson('file.jsonl')

# Lazy (×¨×§ NDJSON!)
lf = pl.scan_ndjson('file.jsonl')
```

### ×›×ª×™×‘×”

```python
# JSON
df.write_json('output.json')

# NDJSON (××•××œ×¥ ×œ×§×‘×¦×™× ×’×“×•×œ×™×!)
df.write_ndjson('output.jsonl')
```

### ×¢×‘×•×“×” ×¢× JSON ××§×•× ×Ÿ

```python
# ×¤×™×¨×•×§ struct
df_unnested = df.unnest('nested_column')

# ×¤×™×¨×•×§ list
df_exploded = df.explode('list_column')
```

---

## Excel

### ×”×ª×§× ×”
```bash
pip install xlsx2csv xlsxwriter openpyxl
```

### ×§×¨×™××”

```python
# ×’×™×œ×™×•×Ÿ ××—×“
df = pl.read_excel(
    'file.xlsx',
    sheet_name='Sheet1',          # ×©× ×”×’×™×œ×™×•×Ÿ
    engine='xlsx2csv',            # ××”×™×¨!
    read_options={'try_parse_dates': True}
)

# ×›×œ ×”×’×™×œ×™×•× ×•×ª
import openpyxl
wb = openpyxl.load_workbook('file.xlsx')
for sheet in wb.sheetnames:
    df = pl.read_excel('file.xlsx', sheet_name=sheet)
```

### ×›×ª×™×‘×”

```python
# ×’×™×œ×™×•×Ÿ ××—×“
df.write_excel(
    'output.xlsx',
    worksheet='Data',
    header_format={'bold': True}
)

# ××¡×¤×¨ ×’×™×œ×™×•× ×•×ª
with pl.ExcelWriter('output.xlsx') as writer:
    df1.write_excel(workbook=writer, worksheet='Sheet1')
    df2.write_excel(workbook=writer, worksheet='Sheet2')

# ×¢× ×¢×™×¦×•×‘
df.write_excel(
    'styled.xlsx',
    worksheet='Report',
    header_format={
        'bold': True,
        'font_color': 'white',
        'bg_color': '#4472C4'
    },
    column_formats={
        'price': 'â‚ª#,##0.00'
    },
    autofit=True
)
```

---

## ×¤×•×¨××˜×™× × ×•×¡×¤×™×

### IPC (Arrow)

```python
# ×§×¨×™××”/×›×ª×™×‘×”
df = pl.read_ipc('file.arrow')
df.write_ipc('output.arrow')

# Lazy
lf = pl.scan_ipc('file.arrow')
lf.collect().write_ipc('output.arrow')  # sink_ipc ×œ× × ×ª××š
```

### Avro

```python
# ×§×¨×™××”/×›×ª×™×‘×”
df = pl.read_avro('file.avro')
df.write_avro('output.avro')
```

### Iceberg

```python
# ×¡×¨×™×§×” ×‘×œ×‘×“
lf = pl.scan_iceberg('catalog/db/table/metadata/file.metadata.json')
```

---

## ×§×‘×¦×™× ××¨×•×‘×™×

### ×›×ª×™×‘×”

```python
# ×—×œ×•×§×” ×œ×§×‘×¦×™× × ×¤×¨×“×™×
for name, group_df in df.group_by('category'):
    group_df.write_csv(f'output_{name[0]}.csv')
```

### ×§×¨×™××”

```python
# ×¢× wildcard
df = pl.read_csv('data_*.csv')
lf = pl.scan_csv('data_*.csv')

# ×§×¨×™××ª ××¡×¤×¨ ×§×‘×¦×™× ×¡×¤×¦×™×¤×™×™×
import glob
files = glob.glob('data/*.parquet')
lf = pl.scan_parquet(files)

# ×˜×¢×™× ×” ××§×‘×™×œ×” ×©×œ ×›×œ ×”×§×‘×¦×™×
lfs = [pl.scan_csv(f) for f in glob.glob('*.csv')]
dfs = pl.collect_all(lfs)
```

---

## ×‘×¡×™×¡×™ × ×ª×•× ×™×

### ×”×ª×§× ×”

```bash
# ConnectorX (××”×™×¨!)
pip install connectorx

# ADBC (××•××œ×¥!)
pip install adbc-driver-postgresql pyarrow

# SQLAlchemy
pip install sqlalchemy psycopg2  # ××• pg8000
```

### ×§×¨×™××”

```python
# ConnectorX
uri = 'postgresql://user:pass@localhost:5432/db'
df = pl.read_database_uri('SELECT * FROM table', uri)

# ADBC (××•××œ×¥!)
df = pl.read_database_uri(
    'SELECT * FROM table',
    uri,
    engine='adbc'
)

# SQLAlchemy
from sqlalchemy import create_engine
con_string = 'postgresql+pg8000://user:pass@localhost:5432/db'
engine = create_engine(con_string)
df = pl.read_database('SELECT * FROM table', connection=engine)
```

### ×›×ª×™×‘×”

```python
# ADBC
df.write_database(
    table_name='schema.table',
    connection=uri,
    engine='adbc',
    if_table_exists='append'      # ××•: replace, fail
)

# SQLAlchemy
df.write_database(
    table_name='schema.table',
    connection=con_string,
    engine='sqlalchemy',
    if_table_exists='replace'
)
```

---

## ×˜×™×¤×™× ×—×©×•×‘×™×

### ğŸ¯ ×˜×‘×œ×ª ×‘×—×™×¨×ª ×¤×•×¨××˜

| ××¦×‘ | ×¤×•×¨××˜ ××•××œ×¥ | ×œ××”? |
|-----|-------------|------|
| ×©×™×ª×•×£ ×¢× ×× ×©×™× | CSV | ×¤×©×•×˜ ×•× ×¤×•×¥ |
| ××—×¡×•×Ÿ ××¨×•×š ×˜×•×•×— | Parquet | ×“×—×•×¡ ×•××”×™×¨ |
| Data Lake | Delta Lake | ACID + versioning |
| API / Web | JSON/NDJSON | ×ª×§×Ÿ ××™× ×˜×¨× ×˜ |
| ×§×‘×¦×™× ×¢× ×§×™×™× | Parquet + Lazy | ××•×¤×˜×™××œ×™ |
| × ×ª×•× ×™× ×–×× ×™×™× | IPC/Arrow | ××”×™×¨ ×‘×™×•×ª×¨ |
| ×“×•×—×•×ª ×¢×¡×§×™×™× | Excel | ××•×›×¨ |

### âš¡ ××•×¤×˜×™××™×–×¦×™×”

```python
# âŒ ×œ× ×™×¢×™×œ
df = pl.read_csv('huge.csv')
df = df.filter(pl.col('age') > 30)

# âœ… ×™×¢×™×œ!
df = (
    pl.scan_csv('huge.csv')
    .filter(pl.col('age') > 30)
    .collect()
)
```

### ğŸ’¾ ×—×™×¡×›×•×Ÿ ×‘×–×™×›×¨×•×Ÿ

```python
# ×‘×—×™×¨×ª ×˜×™×¤×•×¡×™× ×§×˜× ×™×
schema_overrides = {
    'age': pl.Int8,        # -128 ×¢×“ 127
    'quantity': pl.Int16,  # -32K ×¢×“ 32K
    'price': pl.Float32    # ×‘××§×•× Float64
}

df = pl.read_csv('data.csv', schema_overrides=schema_overrides)
```

### ğŸ—œï¸ ×”×©×•×•××ª ×“×—×™×¡×•×ª (Parquet)

| ×“×—×™×¡×” | ××”×™×¨×•×ª | ×’×•×“×œ | ××•××œ×¥ ×œ... |
|-------|---------|------|-----------|
| `uncompressed` | âš¡âš¡âš¡ | âŒ | ×‘×“×™×§×•×ª |
| `snappy` | âš¡âš¡ | âœ… | ×©×™××•×© ×™×•××™×•××™ |
| `lz4` | âš¡âš¡ | âœ… | ××”×™×¨×•×ª |
| `zstd` | âš¡ | âœ…âœ… | **××•××œ×¥!** |
| `gzip` | âš¡ | âœ…âœ… | ××—×¡×•×Ÿ |

### ğŸ› ×¤×ª×¨×•×Ÿ ×‘×¢×™×•×ª

#### Out of Memory
```python
# ×‘××§×•× read_*, ×”×©×ª××© ×‘-scan_*
lf = pl.scan_parquet('huge.parquet')
result = lf.filter(...).select(...).collect()
```

#### ×ª××¨×™×›×™× ×œ× × ×›×•× ×™×
```python
df = pl.read_csv('data.csv', try_parse_dates=True)

# ××• ×™×“× ×™×ª:
df = df.with_columns(
    pl.col('date').str.strptime(pl.Date, '%d/%m/%Y')
)
```

#### ×‘×¢×™×•×ª encoding
```python
df = pl.read_csv('data.csv', encoding='utf8-lossy')
# ××•: encoding='latin1'
```

#### ×§×•×‘×¥ CSV ××©×•×‘×©
```python
df = pl.read_csv(
    'data.csv',
    ignore_errors=True,           # ×“×œ×’ ×¢×œ ×©×•×¨×•×ª ×‘×¢×™×™×ª×™×•×ª
    truncate_ragged_lines=True    # ×—×ª×•×š ×©×•×¨×•×ª ××¨×•×›×•×ª ××“×™
)
```

---

## ğŸ”¥ ×˜×™×¤×™× ××ª×§×“××™×

### Streaming ×¢× ×¡×™× ×•×Ÿ

```python
# ×§×•×¨× + ××¡× ×Ÿ + ×›×•×ª×‘ ×‘×–×¨× ××—×“
(
    pl.scan_csv('input.csv')
    .filter(pl.col('price') > 1000)
    .select(['customer_id', 'price', 'date'])
    .sink_parquet('expensive_items.parquet')
)
```

### ×¢×™×‘×•×“ ×‘××¦×•×•×” (Batch Processing)

```python
# ×¢×™×‘×•×“ 10,000 ×©×•×¨×•×ª ×‘×›×œ ×¤×¢×
for batch in pl.read_csv_batched('huge.csv', batch_size=10_000):
    process_batch(batch)
    batch.write_csv('output.csv', include_header=False)
```

### ×§×¨×™××” ×-URL

```python
# ×™×©×™×¨×•×ª ××”××™× ×˜×¨× ×˜
url = 'https://example.com/data.csv'
df = pl.read_csv(url)

# ××• parquet
df = pl.read_parquet('https://example.com/data.parquet')
```

### ×¢×‘×•×“×” ×¢× S3

```python
# AWS S3
storage_options = {
    'aws_access_key_id': 'KEY',
    'aws_secret_access_key': 'SECRET',
    'aws_region': 'us-east-1'
}

df = pl.read_parquet(
    's3://bucket/file.parquet',
    storage_options=storage_options
)

df.write_parquet(
    's3://bucket/output.parquet',
    storage_options=storage_options
)
```

---

## ğŸ“ ×“×•×’×××•×ª ××”×™×¨×•×ª

### ×”××¨×ª CSV ×œ-Parquet

```python
pl.read_csv('data.csv').write_parquet('data.parquet', compression='zstd')
```

### ×”××¨×ª Excel ×œ-CSV

```python
pl.read_excel('data.xlsx').write_csv('data.csv')
```

### ××™×–×•×’ ×§×‘×¦×™× ××¨×•×‘×™×

```python
df = pl.read_csv('data_*.csv')
df.write_parquet('merged.parquet')
```

### ×¡×™× ×•×Ÿ ×•×©××™×¨×”

```python
(
    pl.scan_csv('input.csv')
    .filter(pl.col('age') > 18)
    .sink_csv('adults.csv')
)
```

### ×™×¦×™×¨×ª ×“×•×— Excel ××¨×•×‘×” ×’×™×œ×™×•× ×•×ª

```python
df = pl.read_csv('data.csv')

with pl.ExcelWriter('report.xlsx') as writer:
    df.write_excel(workbook=writer, worksheet='Raw')
    df.group_by('category').agg(pl.col('price').sum()) \
      .write_excel(workbook=writer, worksheet='Summary')
```

---

## ğŸ“ Cheat Sheet - ×¤×§×•×“×•×ª ×—×•×‘×”

```python
# ×§×¨×™××”
df = pl.read_csv('file.csv')
df = pl.read_parquet('file.parquet')
df = pl.read_json('file.json')
df = pl.read_excel('file.xlsx')

# ×§×¨×™××” Lazy
lf = pl.scan_csv('file.csv')
lf = pl.scan_parquet('file.parquet')
lf = pl.scan_ndjson('file.jsonl')

# ×›×ª×™×‘×”
df.write_csv('out.csv')
df.write_parquet('out.parquet')
df.write_json('out.json')
df.write_excel('out.xlsx')

# Streaming
lf.sink_csv('out.csv')
lf.sink_parquet('out.parquet')

# ×‘×¡×™×¡ × ×ª×•× ×™×
df = pl.read_database_uri('SELECT * FROM t', 'postgresql://...')
df.write_database(table_name='t', connection='postgresql://...')

# ×§×‘×¦×™× ××¨×•×‘×™×
df = pl.read_csv('data_*.csv')
dfs = pl.collect_all([pl.scan_csv(f) for f in files])
```

---

## ğŸ“š ××©××‘×™×

- **×ª×™×¢×•×“ ×¨×©××™:** https://docs.pola.rs/
- **GitHub:** https://github.com/pola-rs/polars
- **Discord:** https://discord.gg/4UfP5cfBE7
- **YouTube:** https://www.youtube.com/@polarsDataFrame

---

## ğŸš€ ×¡×™×•×

**×”××“×¨×™×š ×”××”×™×¨ ×”×•×©×œ×!**

×–×›×•×¨:
- âœ… CSV = ×¤×©×•×˜ ×•× ×¤×•×¥
- âœ… Parquet = ××”×™×¨ ×•×“×—×•×¡
- âœ… Delta = ××¨×’×•× ×™ ×•×‘×˜×•×—
- âœ… Lazy = ×—×•×‘×” ×œ×§×‘×¦×™× ×’×“×•×œ×™×!

**×‘×”×¦×œ×—×”! ğŸ‰**
