{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“š ××“×¨×™×š ××§×™×£: ×§×¨×™××” ×•×›×ª×™×‘×” ×©×œ ×§×‘×¦×™× ×‘-Polars\n",
    "\n",
    "**×‘×¨×•×›×™× ×”×‘××™× ×œ××“×¨×™×š ×”××§×™×£ ×œ×¢×‘×•×“×” ×¢× ×§×‘×¦×™× ×‘-Polars!**\n",
    "\n",
    "××“×¨×™×š ×–×” ××›×¡×” ××ª ×›×œ ×¤×•×¨××˜×™ ×”×§×‘×¦×™× ×”× ×¤×•×¦×™× ×•××¡×¤×§ ×”×¡×‘×¨×™× ××¤×•×¨×˜×™×, ×“×•×’×××•×ª ×§×•×“, ×•×ª×¨×’×™×œ×™× ××™× ×˜×¨××§×˜×™×‘×™×™×.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‘ ×ª×•×›×Ÿ ×¢× ×™×™× ×™×\n",
    "\n",
    "1. [×§×¨×™××” ×•×›×ª×™×‘×” ×©×œ ×§×‘×¦×™ CSV](#1-×§×¨×™××”-×•×›×ª×™×‘×”-×©×œ-×§×‘×¦×™-csv)\n",
    "2. [×§×¨×™××” ×•×›×ª×™×‘×” ×©×œ ×§×‘×¦×™ Parquet](#2-×§×¨×™××”-×•×›×ª×™×‘×”-×©×œ-×§×‘×¦×™-parquet)\n",
    "3. [×§×¨×™××” ×•×›×ª×™×‘×” ×©×œ ×˜×‘×œ××•×ª Delta Lake](#3-×§×¨×™××”-×•×›×ª×™×‘×”-×©×œ-×˜×‘×œ××•×ª-delta-lake)\n",
    "4. [×§×¨×™××” ×•×›×ª×™×‘×” ×©×œ ×§×‘×¦×™ JSON](#4-×§×¨×™××”-×•×›×ª×™×‘×”-×©×œ-×§×‘×¦×™-json)\n",
    "5. [×§×¨×™××” ×•×›×ª×™×‘×” ×©×œ ×§×‘×¦×™ Excel](#5-×§×¨×™××”-×•×›×ª×™×‘×”-×©×œ-×§×‘×¦×™-excel)\n",
    "6. [×¤×•×¨××˜×™× × ×•×¡×¤×™× (IPC, Avro, Iceberg)](#6-×¤×•×¨××˜×™×-× ×•×¡×¤×™×)\n",
    "7. [×¢×‘×•×“×” ×¢× ×§×‘×¦×™× ××¨×•×‘×™×](#7-×¢×‘×•×“×”-×¢×-×§×‘×¦×™×-××¨×•×‘×™×)\n",
    "8. [×¢×‘×•×“×” ×¢× ×‘×¡×™×¡×™ × ×ª×•× ×™×](#8-×¢×‘×•×“×”-×¢×-×‘×¡×™×¡×™-× ×ª×•× ×™×)\n",
    "9. [×¡×™×›×•× ×•×˜×™×¤×™×](#9-×¡×™×›×•×-×•×˜×™×¤×™×)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ×”×›× ×” ×¨××©×•× ×™×ª\n",
    "\n",
    "### ×”×ª×§× ×ª Polars\n",
    "\n",
    "×§×•×“× ×›×œ, × ×•×•×“× ×©-Polars ××•×ª×§×Ÿ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ×”×ª×§× ×ª Polars (×”×¨×¥ ×¨×§ ×¤×¢× ××—×ª)\n",
    "# !pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×™×™×‘×•× ×”×¡×¤×¨×™×™×”\n",
    "import polars as pl\n",
    "\n",
    "# ×‘×“×™×§×ª ×’×¨×¡×”\n",
    "print(f\"Polars version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. ×§×¨×™××” ×•×›×ª×™×‘×” ×©×œ ×§×‘×¦×™ CSV\n",
    "\n",
    "### ğŸ“– ××” ×–×” CSV?\n",
    "\n",
    "**CSV (Comma-Separated Values)** ×”×•× ×¤×•×¨××˜ ×§×•×‘×¥ ×˜×§×¡×˜ ×¤×©×•×˜ ×”××©××© ×œ××—×¡×•×Ÿ × ×ª×•× ×™× ×˜×‘×œ××™×™×. ×›×œ ×©×•×¨×” ××™×™×¦×’×ª ×¨×©×•××”, ×•×›×œ ×¢×¨×š ××•×¤×¨×“ ×‘×¤×¡×™×§ (××• ×ª×• ××—×¨).\n",
    "\n",
    "**×™×ª×¨×•× ×•×ª:**\n",
    "- ğŸ“ ×§×œ ×œ×§×¨×™××” ×•×¢×¨×™×›×”\n",
    "- ğŸ”„ ×ª×•×× ×œ××’×•×•×Ÿ ×›×œ×™×\n",
    "- ğŸ’¾ ×§×•×‘×¥ ×˜×§×¡×˜ ×¤×©×•×˜\n",
    "\n",
    "**×—×¡×¨×•× ×•×ª:**\n",
    "- ğŸ“ ××™×Ÿ ×©××™×¨×ª ×˜×™×¤×•×¡×™ × ×ª×•× ×™×\n",
    "- ğŸŒ ××™×˜×™ ×™×—×¡×™×ª ×œ×§×‘×¦×™× ×’×“×•×œ×™×\n",
    "- ğŸ’¾ ×’×•×“×œ ×§×•×‘×¥ ×’×“×•×œ ×™×—×¡×™×ª\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” ×§×¨×™××” ×‘×¡×™×¡×™×ª ×©×œ CSV\n",
    "\n",
    "×”×“×¨×š ×”×¤×©×•×˜×” ×‘×™×•×ª×¨ ×œ×§×¨×•× ×§×•×‘×¥ CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×§×¨×™××” ×‘×¡×™×¡×™×ª ×©×œ ×§×•×‘×¥ CSV\n",
    "df = pl.read_csv('../data/customer_shopping_data.csv')\n",
    "\n",
    "# ×”×¦×’×ª 5 ×”×©×•×¨×•×ª ×”×¨××©×•× ×•×ª\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ’¡ ×”×¡×‘×¨:\n",
    "\n",
    "- `pl.read_csv()` - ×”×¤×•× ×§×¦×™×” ×”×¢×™×§×¨×™×ª ×œ×§×¨×™××ª CSV\n",
    "- `df.head()` - ××¦×™×’ ××ª 5 ×”×©×•×¨×•×ª ×”×¨××©×•× ×•×ª (×‘×¨×™×¨×ª ××—×“×œ)\n",
    "- Polars ××–×”×” ××•×˜×•××˜×™×ª ××ª ×˜×™×¤×•×¡×™ ×”× ×ª×•× ×™× ×‘×¢××•×“×•×ª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš« ×¢×‘×•×“×” ×¢× ×§×‘×¦×™× ×œ×œ× ×›×•×ª×¨×•×ª (Headers)\n",
    "\n",
    "×œ×¤×¢××™× ×§×‘×¦×™ CSV ×œ× ×›×•×œ×œ×™× ×©×•×¨×ª ×›×•×ª×¨×•×ª. ×‘×•××• × ×¨××” ××™×š ××˜×¤×œ×™× ×‘××¦×‘ ×–×”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# × ×™×¡×™×•×Ÿ ×¨××©×•×Ÿ - Polars ×—×•×©×‘ ×©×”×©×•×¨×” ×”×¨××©×•× ×” ×”×™× ×›×•×ª×¨×ª\n",
    "df_wrong = pl.read_csv('../data/customer_shopping_data_no_header.csv')\n",
    "print(\"âŒ ×‘×¢×™×™×ª×™ - ×”×©×•×¨×” ×”×¨××©×•× ×” ×”×¤×›×” ×œ×›×•×ª×¨×ª:\")\n",
    "df_wrong.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ×”×“×¨×š ×”× ×›×•× ×” - ××•××¨×™× ×œ-Polars ×©××™×Ÿ ×›×•×ª×¨×•×ª\n",
    "df_correct = pl.read_csv(\n",
    "    '../data/customer_shopping_data_no_header.csv',\n",
    "    has_header=False  # ğŸ‘ˆ ×–×” ×”×¤×¨××˜×¨ ×”×—×©×•×‘!\n",
    ")\n",
    "print(\"âœ… × ×›×•×Ÿ - Polars ×™×¦×¨ ×›×•×ª×¨×•×ª ××•×˜×•××˜×™×•×ª (column_1, column_2...):\")\n",
    "df_correct.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ’¡ ×”×¡×‘×¨:\n",
    "\n",
    "- `has_header=False` - ××•×“×™×¢ ×œ-Polars ×©××™×Ÿ ×©×•×¨×ª ×›×•×ª×¨×•×ª\n",
    "- Polars ×™×•×¦×¨ ×›×•×ª×¨×•×ª ××•×˜×•××˜×™×•×ª: `column_1`, `column_2`, ×•×›×•'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ·ï¸ ×”×’×“×¨×ª ×©××•×ª ×¢××•×“×•×ª ××•×ª×××™× ××™×©×™×ª\n",
    "\n",
    "×‘××§×•× ×œ×§×‘×œ ×©××•×ª ×¢××•×“×•×ª ×’× ×¨×™×™×, × ×•×›×œ ×œ×”×’×“×™×¨ ×©××•×ª ××©×œ× ×•:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×”×’×“×¨×ª ×¨×©×™××ª ×©××•×ª ×¢××•×“×•×ª\n",
    "column_names = [\n",
    "    'invoice_no',      # ××¡×¤×¨ ×—×©×‘×•× ×™×ª\n",
    "    'customer_id',     # ××–×”×” ×œ×§×•×—\n",
    "    'gender',          # ××™×Ÿ\n",
    "    'age',             # ×’×™×œ\n",
    "    'category',        # ×§×˜×’×•×¨×™×”\n",
    "    'quantity',        # ×›××•×ª\n",
    "    'price',           # ××—×™×¨\n",
    "    'payment_method',  # ×××¦×¢×™ ×ª×©×œ×•×\n",
    "    'invoice_date',    # ×ª××¨×™×š ×—×©×‘×•× ×™×ª\n",
    "    'shopping_mall'    # ×§× ×™×•×Ÿ\n",
    "]\n",
    "\n",
    "# ×§×¨×™××” ×¢× ×©××•×ª ×¢××•×“×•×ª ××•×ª×××™×\n",
    "df_named = pl.read_csv(\n",
    "    '../data/customer_shopping_data_no_header.csv',\n",
    "    has_header=False,\n",
    "    new_columns=column_names  # ğŸ‘ˆ ×¨×©×™××ª ×”×©××•×ª ×©×œ× ×•\n",
    ")\n",
    "\n",
    "df_named.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ’¡ ×”×¡×‘×¨:\n",
    "\n",
    "- `new_columns` - ××§×‘×œ ×¨×©×™××” ×©×œ ×©××•×ª ×¢××•×“×•×ª\n",
    "- ××¡×¤×¨ ×”×©××•×ª ×—×™×™×‘ ×œ×”×ª××™× ×œ××¡×¤×¨ ×”×¢××•×“×•×ª ×‘×§×•×‘×¥\n",
    "- ×©××•×ª ×”×¢××•×“×•×ª ×™×›×•×œ×™× ×œ×”×™×•×ª ×‘×›×œ ×©×¤×” (×›×•×œ×œ ×¢×‘×¨×™×ª!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“… × ×™×ª×•×— ××•×˜×•××˜×™ ×©×œ ×ª××¨×™×›×™×\n",
    "\n",
    "Polars ×™×›×•×œ ×œ×–×”×•×ª ×•×œ×”××™×¨ ×ª××¨×™×›×™× ××•×˜×•××˜×™×ª:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×§×¨×™××” ×¢× × ×™×ª×•×— ×ª××¨×™×›×™×\n",
    "df_with_dates = pl.read_csv(\n",
    "    '../data/customer_shopping_data_no_header.csv',\n",
    "    has_header=False,\n",
    "    new_columns=column_names,\n",
    "    try_parse_dates=True  # ğŸ‘ˆ ×× ×¡×” ×œ×–×”×•×ª ×ª××¨×™×›×™× ××•×˜×•××˜×™×ª\n",
    ")\n",
    "\n",
    "print(\"ğŸ“‹ ×¡×›××ª ×”× ×ª×•× ×™× (Data Schema):\")\n",
    "print(df_with_dates.schema)\n",
    "print(\"\\nğŸ“Š ×”× ×ª×•× ×™×:\")\n",
    "df_with_dates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ’¡ ×©×™××• ×œ×‘:\n",
    "\n",
    "- ×¢××•×“×ª `invoice_date` ×”×©×ª× ×ª×” ×-`str` ×œ-`date`\n",
    "- ×¢×›×©×™×• × ×™×ª×Ÿ ×œ×‘×¦×¢ ×¤×¢×•×œ×•×ª ×©×œ ×ª××¨×™×›×™× (×—×™×©×•×‘ ×”×¤×¨×©×™×, ×¡×™× ×•×Ÿ ×œ×¤×™ ×˜×•×•×—, ×•×›×•')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ ×©×œ×™×˜×” ××“×•×™×§×ª ×‘×˜×™×¤×•×¡×™ × ×ª×•× ×™×\n",
    "\n",
    "×œ×¤×¢××™× × ×¨×¦×” ×œ×©×œ×•×˜ ×‘×“×™×•×§ ×¢×œ ×˜×™×¤×•×¡ ×›×œ ×¢××•×“×” (×œ×—×¡×•×š ×–×™×›×¨×•×Ÿ ××• ×œ×× ×•×¢ ×©×’×™××•×ª):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×‘×“×™×§×”: ××™×–×” ×˜×™×¤×•×¡ ×œ×¢××•×“×ª 'age' ×›×¨×’×¢?\n",
    "print(f\"×˜×™×¤×•×¡ age ×œ×¤× ×™: {df_with_dates['age'].dtype}\")\n",
    "\n",
    "# ×§×¨×™××” ×¢× ×”×’×“×¨×ª ×˜×™×¤×•×¡×™× ××•×ª×××™×\n",
    "df_optimized = pl.read_csv(\n",
    "    '../data/customer_shopping_data_no_header.csv',\n",
    "    has_header=False,\n",
    "    new_columns=column_names,\n",
    "    try_parse_dates=True,\n",
    "    schema_overrides={\n",
    "        'age': pl.Int8,        # ğŸ‘ˆ Int8 = -128 ×¢×“ 127 (×—×•×¡×š ×–×™×›×¨×•×Ÿ!)\n",
    "        'quantity': pl.Int32   # ğŸ‘ˆ Int32 = ××¡×¤×¨×™× ×©×œ××™× ×¨×’×™×œ×™×\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"×˜×™×¤×•×¡ age ××—×¨×™: {df_optimized['age'].dtype}\")\n",
    "print(\"\\nğŸ“‹ ×¡×›××” ××œ××”:\")\n",
    "print(df_optimized.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ’¡ ××“×•×¢ ×–×” ×—×©×•×‘?\n",
    "\n",
    "- **Int64** (×‘×¨×™×¨×ª ××—×“×œ): 8 ×‘×ª×™× ×œ×›×œ ×¢×¨×š\n",
    "- **Int8**: ×¨×§ 1 ×‘×™×™×˜ ×œ×›×œ ×¢×¨×š! (×—×™×¡×›×•×Ÿ ×©×œ ×¤×™ 8)\n",
    "- ×¢×‘×•×¨ ××™×œ×™×•× ×™ ×©×•×¨×•×ª, ×”×”×‘×“×œ ××©××¢×•×ª×™\n",
    "\n",
    "**×˜×™×¤×•×¡×™ ××¡×¤×¨×™× × ×¤×•×¦×™×:**\n",
    "- `Int8`: -128 ×¢×“ 127\n",
    "- `Int16`: -32,768 ×¢×“ 32,767\n",
    "- `Int32`: -2.1B ×¢×“ 2.1B\n",
    "- `Int64`: ××¡×¤×¨×™× ×¢× ×§×™×™×"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¾ ×›×ª×™×‘×ª ×§×‘×¦×™ CSV\n",
    "\n",
    "×¢×›×©×™×• × ×œ××“ ××™×š ×œ×©××•×¨ DataFrame ×›×§×•×‘×¥ CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×©××™×¨×” ×‘×¡×™×¡×™×ª\n",
    "df_optimized.write_csv('../data/output/shopping_data_output.csv')\n",
    "print(\"âœ… ×”×§×•×‘×¥ × ×©××¨ ×‘×”×¦×œ×—×”!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×©××™×¨×” ××ª×§×“××ª - ×‘×œ×™ ×›×•×ª×¨×•×ª, ×¢× ××¤×¨×™×“ ××—×¨\n",
    "df_optimized.write_csv(\n",
    "    '../data/output/shopping_data_custom.csv',\n",
    "    include_header=False,  # ×œ×œ× ×©×•×¨×ª ×›×•×ª×¨×•×ª\n",
    "    separator='|'          # ××¤×¨×™×“: | ×‘××§×•× ,\n",
    ")\n",
    "print(\"âœ… × ×©××¨ ×¢× ×”×’×“×¨×•×ª ××•×ª×××•×ª!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âš™ï¸ ×¤×¨××˜×¨×™× ×©×™××•×©×™×™× × ×•×¡×¤×™×:\n",
    "\n",
    "```python\n",
    "df.write_csv(\n",
    "    path='output.csv',\n",
    "    include_header=True,     # ×”×× ×œ×›×œ×•×œ ×›×•×ª×¨×•×ª\n",
    "    separator=',',           # ×ª×• ××¤×¨×™×“\n",
    "    quote_char='\"',          # ×ª×• ×¦×™×˜×•×˜\n",
    "    datetime_format='%Y-%m-%d'  # ×¤×•×¨××˜ ×ª××¨×™×›×™×\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš¡ Lazy Loading - ×¢×™×‘×•×“ ×™×¢×™×œ ×©×œ ×§×‘×¦×™× ×¢× ×§×™×™×\n",
    "\n",
    "**××” ×–×” Lazy Loading?**\n",
    "\n",
    "×‘××§×•× ×œ×˜×¢×•×Ÿ ××ª ×›×œ ×”×§×•×‘×¥ ×œ×–×™×›×¨×•×Ÿ, Polars:\n",
    "1. ğŸ“‹ ×§×•×¨× ×¨×§ ××ª ×”××˜×-×“××˜×”\n",
    "2. ğŸ¯ ××‘×™×Ÿ ××™×œ×• ×—×™×©×•×‘×™× ×“×¨×•×©×™×\n",
    "3. âš¡ ××¨×™×¥ ×¨×§ ××ª ××” ×©×¦×¨×™×š\n",
    "4. ğŸ’ª ×™×›×•×œ ×œ×¢×‘×“ ×§×‘×¦×™× ×’×“×•×œ×™× ××”×–×™×›×¨×•×Ÿ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×™×¦×™×¨×ª LazyFrame (×œ× ×˜×•×¢×Ÿ ××ª ×”× ×ª×•× ×™×!)\n",
    "lf = pl.scan_csv(\n",
    "    '../data/customer_shopping_data_no_header.csv',\n",
    "    has_header=False,\n",
    "    new_columns=column_names,\n",
    "    try_parse_dates=True,\n",
    "    schema_overrides={'age': pl.Int8, 'quantity': pl.Int32}\n",
    ")\n",
    "\n",
    "print(\"ğŸ” ×–×” LazyFrame - ×¢×“×™×™×Ÿ ×œ× ×˜×¢× ×• × ×ª×•× ×™×!\")\n",
    "print(type(lf))\n",
    "\n",
    "# ×›×“×™ ×œ×¨××•×ª × ×ª×•× ×™×, ×¦×¨×™×š ×œ×§×¨×•× ×œ-collect()\n",
    "result = lf.head().collect()\n",
    "print(\"\\nğŸ“Š ×¢×›×©×™×• ×”× ×ª×•× ×™× × ×˜×¢× ×•:\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ’ª ×“×•×’××” ×œ×¢×•×¦××” ×©×œ Lazy:\n",
    "\n",
    "× × ×™×— ×©×™×© ×œ× ×• ×§×•×‘×¥ ×¢× ×§, ×•×× ×—× ×• ×¨×•×¦×™× ×¨×§ 10 ×©×•×¨×•×ª ××§×˜×’×•×¨×™×” ××¡×•×™××ª:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy - ×™×§×¨× ×¨×§ ××ª ××” ×©×¦×¨×™×š!\n",
    "result = (\n",
    "    pl.scan_csv('../data/customer_shopping_data_no_header.csv',\n",
    "                has_header=False, new_columns=column_names)\n",
    "    .filter(pl.col('category') == 'Clothing')  # ×¡×™× ×•×Ÿ\n",
    "    .select(['customer_id', 'price', 'quantity'])  # ×‘×—×™×¨×ª ×¢××•×“×•×ª\n",
    "    .head(10)  # ×¨×§ 10 ×©×•×¨×•×ª\n",
    "    .collect()  # ×¨×§ ×¢×›×©×™×• ××¨×™×¥!\n",
    ")\n",
    "\n",
    "print(\"âš¡ Polars ×§×¨× ×¨×§ ××ª ××” ×©×“×¨×•×© - ××”×™×¨ ×××•×“!\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“¤ Streaming - ×›×ª×™×‘×” ×™×©×™×¨×” ×œ×“×™×¡×§\n",
    "\n",
    "`sink_csv()` ×©×•××¨ ×™×©×™×¨×•×ª ×œ×§×•×‘×¥ **××‘×œ×™ ×œ×˜×¢×•×Ÿ ×”×›×œ ×œ×–×™×›×¨×•×Ÿ**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×¢×™×‘×•×“ ×•×”×©××¨×” ×‘-streaming mode\n",
    "(\n",
    "    pl.scan_csv('../data/customer_shopping_data_no_header.csv',\n",
    "                has_header=False, new_columns=column_names)\n",
    "    .filter(pl.col('age') > 30)\n",
    "    .sink_csv('../data/output/adults_only.csv')  # ğŸ‘ˆ ×™×©×™×¨×•×ª ×œ×“×™×¡×§!\n",
    ")\n",
    "\n",
    "print(\"âœ… ×”×§×•×‘×¥ × ×©××¨ ×‘×¦×•×¨×” ××•×¤×˜×™××œ×™×ª!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ“ ×ª×¨×’×™×œ 1: ×§×¨×™××ª CSV\n",
    "\n",
    "**××©×™××”:** ×§×¨× ××ª ×”×§×•×‘×¥ `customer_shopping_data.csv` ×•:\n",
    "1. ×”××¨ ××ª ×¢××•×“×ª ×”×ª××¨×™×›×™× ×œ×˜×™×¤×•×¡ `date`\n",
    "2. ×©× ×” ××ª `age` ×œ-`Int8` ×•-`quantity` ×œ-`Int16`\n",
    "3. ×¡× ×Ÿ ×¨×§ ×¨×›×™×©×•×ª ××¢×œ 1000 ×©\"×—\n",
    "4. ×”×¦×’ ××ª 5 ×”×©×•×¨×•×ª ×”×¨××©×•× ×•×ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’ª × ×¡×” ×‘×¢×¦××š ×›××Ÿ:\n",
    "\n",
    "# df = pl.read_csv(...)\n",
    "# ...\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ’¡ ×¤×ª×¨×•×Ÿ (×œ×—×¥ ×œ×”×¦×’×”)</summary>\n",
    "\n",
    "```python\n",
    "df_exercise = pl.read_csv(\n",
    "    '../data/customer_shopping_data.csv',\n",
    "    try_parse_dates=True,\n",
    "    schema_overrides={\n",
    "        'age': pl.Int8,\n",
    "        'quantity': pl.Int16\n",
    "    }\n",
    ").filter(pl.col('price') > 1000)\n",
    "\n",
    "df_exercise.head()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ“š ×¡×™×›×•×: CSV\n",
    "\n",
    "| ×¤×¢×•×œ×” | ×¤×•× ×§×¦×™×” | ×©×™××•×© |\n",
    "|--------|---------|-------|\n",
    "| ×§×¨×™××” ×¨×’×™×œ×” | `pl.read_csv()` | ×§×‘×¦×™× ×§×˜× ×™×-×‘×™× ×•× ×™×™× |\n",
    "| ×§×¨×™××” lazy | `pl.scan_csv()` | ×§×‘×¦×™× ×’×“×•×œ×™×, ××•×¤×˜×™××™×–×¦×™×” |\n",
    "| ×›×ª×™×‘×” ×¨×’×™×œ×” | `df.write_csv()` | ×©××™×¨×” ×¡×˜× ×“×¨×˜×™×ª |\n",
    "| ×›×ª×™×‘×” streaming | `lf.sink_csv()` | ×§×‘×¦×™× ×¢× ×§×™×™× |\n",
    "\n",
    "**×˜×™×¤×™× ××¨×›×–×™×™×:**\n",
    "- âœ… ×”×©×ª××© ×‘-`try_parse_dates=True` ×œ×ª××¨×™×›×™×\n",
    "- âœ… ××•×¤×˜×™××™×–×¦×™×”: `schema_overrides` ×œ×—×™×¡×›×•×Ÿ ×‘×–×™×›×¨×•×Ÿ\n",
    "- âœ… ×§×‘×¦×™× ×’×“×•×œ×™×: `scan_csv()` + `collect()`\n",
    "- âœ… ×§×‘×¦×™× ×¢× ×§×™×™×: `scan_csv()` + `sink_csv()`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ×§×¨×™××” ×•×›×ª×™×‘×” ×©×œ ×§×‘×¦×™ Parquet\n",
    "\n",
    "### ğŸ“– ××” ×–×” Parquet?\n",
    "\n",
    "**Apache Parquet** ×”×•× ×¤×•×¨××˜ ×§×•×‘×¥ **×¢××•×“×ª×™** (columnar) ×”×××•×—×¡×Ÿ ×‘×¦×•×¨×” **×“×—×•×¡×”** ×•**×™×¢×™×œ×”**.\n",
    "\n",
    "**×œ××” Parquet ×¢×“×™×£ ×¢×œ CSV?**\n",
    "\n",
    "| ×ª×›×•× ×” | CSV | Parquet |\n",
    "|--------|-----|----------|\n",
    "| ×’×•×“×œ ×§×•×‘×¥ | ğŸ“Š ×’×“×•×œ | ğŸ“‰ ×§×˜×Ÿ (×“×—×•×¡) |\n",
    "| ××”×™×¨×•×ª ×§×¨×™××” | ğŸŒ ××™×˜×™ | âš¡ ××”×™×¨ ×××•×“ |\n",
    "| ×©××™×¨×ª ×˜×™×¤×•×¡×™× | âŒ ×œ× | âœ… ×›×Ÿ |\n",
    "| ×§×¨×™××” ×—×œ×§×™×ª | âŒ ×§×•×¨× ×”×›×œ | âœ… ×¨×§ ×¢××•×“×•×ª × ×“×¨×©×•×ª |\n",
    "\n",
    "**××ª×™ ×œ×”×©×ª××© ×‘-Parquet?**\n",
    "- ğŸ“Š ××—×¡×•×Ÿ ××¨×•×š ×˜×•×•×—\n",
    "- âš¡ ×¦×•×¨×š ×‘×‘×™×¦×•×¢×™× ×’×‘×•×”×™×\n",
    "- ğŸ’¾ ×§×‘×¦×™× ×’×“×•×œ×™×\n",
    "- ğŸ”„ ×¢×‘×•×“×” ×¢× × ×ª×•× ×™× ×‘×¢× ×Ÿ (S3, Azure, GCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” ×§×¨×™××” ×‘×¡×™×¡×™×ª ×©×œ Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×§×¨×™××” ×¤×©×•×˜×”\n",
    "df_parquet = pl.read_parquet('../data/venture_funding_deals.parquet')\n",
    "\n",
    "print(\"ğŸ“‹ ××™×“×¢ ×¢×œ ×”-DataFrame:\")\n",
    "print(f\"××¡×¤×¨ ×©×•×¨×•×ª: {len(df_parquet):,}\")\n",
    "print(f\"××¡×¤×¨ ×¢××•×“×•×ª: {len(df_parquet.columns)}\")\n",
    "print(f\"\\n×©××•×ª ×¢××•×“×•×ª: {df_parquet.columns}\")\n",
    "\n",
    "df_parquet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ ×§×¨×™××ª ×¢××•×“×•×ª ×¡×¤×¦×™×¤×™×•×ª ×‘×œ×‘×“\n",
    "\n",
    "**××—×“ ×”×™×ª×¨×•× ×•×ª ×”×’×“×•×œ×™× ×©×œ Parquet**: × ×™×ª×Ÿ ×œ×§×¨×•× ×¨×§ ×—×œ×§ ××”×¢××•×“×•×ª!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×§×¨×™××ª ×¢××•×“×•×ª ×¡×¤×¦×™×¤×™×•×ª ×‘×œ×‘×“\n",
    "df_selected = pl.read_parquet(\n",
    "    '../data/venture_funding_deals.parquet',\n",
    "    columns=['Company', 'Amount', 'Valuation', 'Industry']  # ğŸ‘ˆ ×¨×§ ××œ×”!\n",
    ")\n",
    "\n",
    "print(\"âš¡ × ×§×¨××• ×¨×§ 4 ×¢××•×“×•×ª - ×—×¡×›× ×• ×–××Ÿ ×•×–×™×›×¨×•×Ÿ!\")\n",
    "df_selected.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ’¡ ×œ××” ×–×” ××”×™×¨?\n",
    "\n",
    "Parquet ×××•×—×¡×Ÿ ×‘×¦×•×¨×” ×¢××•×“×ª×™×ª:\n",
    "```\n",
    "CSV (×©×•×¨×ª×™):           Parquet (×¢××•×“×ª×™):\n",
    "row1: A, B, C         col_A: [A, A, A, ...]\n",
    "row2: A, B, C         col_B: [B, B, B, ...]\n",
    "row3: A, B, C         col_C: [C, C, C, ...]\n",
    "```\n",
    "\n",
    "×›×“×™ ×œ×§×¨×•× ×¢××•×“×” ××—×ª, Parquet ×¤×©×•×˜ ×§×•×¤×¥ ×œ××™×§×•× ×©×œ×”!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“‹ ×§×¨×™××ª ××˜×-×“××˜×” ×‘×œ×‘×“ (×œ×œ× ×”× ×ª×•× ×™×)\n",
    "\n",
    "×œ×¤×¢××™× × ×¨×¦×” ×œ×“×¢×ª ××” ×™×© ×‘×§×•×‘×¥ ×‘×œ×™ ×œ×˜×¢×•×Ÿ ××•×ª×•:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×§×¨×™××ª ×”×¡×›××” ×‘×œ×‘×“ (×××•×“ ××”×™×¨!)\n",
    "schema = pl.read_parquet_schema('../data/venture_funding_deals.parquet')\n",
    "\n",
    "print(\"ğŸ“‹ ×¡×›××ª ×”×§×•×‘×¥:\")\n",
    "for column_name, data_type in schema.items():\n",
    "    print(f\"  {column_name:<20} {data_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¾ ×›×ª×™×‘×ª ×§×‘×¦×™ Parquet\n",
    "\n",
    "×‘×•××• × ×©××•×¨ DataFrame ×›×§×•×‘×¥ Parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×”××¨×ª CSV ×œ-Parquet\n",
    "df_to_save = pl.read_csv('../data/customer_shopping_data.csv')\n",
    "\n",
    "# ×©××™×¨×” ×¤×©×•×˜×”\n",
    "df_to_save.write_parquet('../data/output/shopping_data.parquet')\n",
    "print(\"âœ… × ×©××¨ ×‘×”×¦×œ×—×”!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ—œï¸ ×‘×—×™×¨×ª ××œ×’×•×¨×™×ª× ×“×—×™×¡×”\n",
    "\n",
    "Parquet ×ª×•××š ×‘××¡×¤×¨ ××œ×’×•×¨×™×ª××™ ×“×—×™×¡×”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# × ×©×•×•×” ×’×“×œ×™ ×§×‘×¦×™× ×¢× ×“×—×™×¡×•×ª ×©×•× ×•×ª\n",
    "compressions = ['uncompressed', 'snappy', 'gzip', 'lz4', 'zstd']\n",
    "sizes = {}\n",
    "\n",
    "for comp in compressions:\n",
    "    output_path = f'../data/output/test_{comp}.parquet'\n",
    "    \n",
    "    # ×©××™×¨×” ×¢× ×“×—×™×¡×” ×¡×¤×¦×™×¤×™×ª\n",
    "    df_to_save.write_parquet(\n",
    "        output_path,\n",
    "        compression=comp,\n",
    "        compression_level=10 if comp != 'uncompressed' else None\n",
    "    )\n",
    "    \n",
    "    # ××“×™×“×ª ×’×•×“×œ\n",
    "    size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "    sizes[comp] = size_mb\n",
    "\n",
    "# ×”×¦×’×ª ×ª×•×¦××•×ª\n",
    "print(\"ğŸ“Š ×”×©×•×•××ª ×’×“×œ×™ ×§×‘×¦×™×:\")\n",
    "print(\"=\" * 40)\n",
    "for comp, size in sorted(sizes.items(), key=lambda x: x[1]):\n",
    "    print(f\"{comp:<15} {size:>8.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ¯ ××™×–×” ×“×—×™×¡×” ×œ×‘×—×•×¨?\n",
    "\n",
    "| ×“×—×™×¡×” | ××”×™×¨×•×ª | ×’×•×“×œ | ××ª×™ ×œ×”×©×ª××© |\n",
    "|-------|---------|------|-------------|\n",
    "| `uncompressed` | âš¡âš¡âš¡ | âŒ ×’×“×•×œ | ×‘×“×™×§×•×ª ××”×™×¨×•×ª |\n",
    "| `snappy` | âš¡âš¡ | âœ… ×˜×•×‘ | **×‘×¨×™×¨×ª ××—×“×œ ××•××œ×¦×ª** |\n",
    "| `lz4` | âš¡âš¡ | âœ… ×˜×•×‘ | ××”×™×¨ ×××•×“ |\n",
    "| `gzip` | âš¡ | âœ…âœ… ×§×˜×Ÿ | ××—×¡×•×Ÿ ××¨×•×š ×˜×•×•×— |\n",
    "| `zstd` | âš¡ | âœ…âœ… ×§×˜×Ÿ | ××™×–×•×Ÿ ×˜×•×‘ (××•××œ×¥!) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×”××œ×¦×”: zstd ×¢× ×¨××ª ×“×—×™×¡×” 10\n",
    "df_to_save.write_parquet(\n",
    "    '../data/output/shopping_optimized.parquet',\n",
    "    compression='zstd',\n",
    "    compression_level=10  # 1-22, 10 = ××™×–×•×Ÿ ×˜×•×‘\n",
    ")\n",
    "print(\"âœ… × ×©××¨ ×¢× ×“×—×™×¡×” ××•×¤×˜×™××œ×™×ª!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš¡ Lazy Loading ×‘-Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×¡×¨×™×§×” lazy\n",
    "lf_parquet = pl.scan_parquet('../data/venture_funding_deals.parquet')\n",
    "\n",
    "# ×¢×™×‘×•×“ ××•×¨×›×‘ - ×”×›×œ lazy!\n",
    "result = (\n",
    "    lf_parquet\n",
    "    .filter(pl.col('Industry') == 'Artificial intelligence')\n",
    "    .select(['Company', 'Amount', 'Valuation'])\n",
    "    .head(5)\n",
    "    .collect()  # ×¨×§ ×¢×›×©×™×• ××¨×™×¥!\n",
    ")\n",
    "\n",
    "print(\"ğŸ¤– ×—×‘×¨×•×ª AI ×¢× ×”××™××•×Ÿ ×”×’×‘×•×” ×‘×™×•×ª×¨:\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Partitioned Parquet - ×§×‘×¦×™× ××—×•×œ×§×™×\n",
    "\n",
    "**××” ×–×” Partitioning?**\n",
    "\n",
    "×‘××§×•× ×§×•×‘×¥ ××—×“ ×¢× ×§, ××—×œ×§×™× ××ª ×”× ×ª×•× ×™× ×œ×ª×™×§×™×•×ª ×œ×¤×™ ×¢×¨×š:\n",
    "```\n",
    "data/\n",
    "â”œâ”€â”€ Industry=Fintech/\n",
    "â”‚   â””â”€â”€ data.parquet\n",
    "â”œâ”€â”€ Industry=AI/\n",
    "â”‚   â””â”€â”€ data.parquet\n",
    "â””â”€â”€ Industry=Healthcare/\n",
    "    â””â”€â”€ data.parquet\n",
    "```\n",
    "\n",
    "**×™×ª×¨×•×Ÿ**: ×§×¨×™××” ××”×™×¨×” ×©×œ ×¨×§ ×”× ×ª×•× ×™× ×”×¨×œ×•×•× ×˜×™×™×!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×§×¨×™××ª parquet ××—×•×œ×§ (×¦×¨×™×š pyarrow)\n",
    "df_partitioned = pl.read_parquet(\n",
    "    '../data/venture_funding_deals_partitioned',\n",
    "    use_pyarrow=True,\n",
    "    pyarrow_options={'partitioning': 'hive'}  # ğŸ‘ˆ ××¦×‘ Hive standard\n",
    ")\n",
    "\n",
    "print(f\"âœ… × ×˜×¢× ×• {len(df_partitioned):,} ×©×•×¨×•×ª\")\n",
    "print(f\"×ª×¢×©×™×•×ª: {df_partitioned['Industry'].n_unique()}\")\n",
    "df_partitioned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×›×ª×™×‘×” ×©×œ parquet ××—×•×œ×§\n",
    "df_to_partition = pl.read_parquet('../data/venture_funding_deals.parquet')\n",
    "\n",
    "df_to_partition.write_parquet(\n",
    "    '../data/output/funding_by_industry',\n",
    "    use_pyarrow=True,\n",
    "    pyarrow_options={\n",
    "        'partition_cols': ['Industry'],  # ğŸ‘ˆ ×—×œ×•×§×” ×œ×¤×™ Industry\n",
    "        'existing_data_behavior': 'overwrite_or_ignore'\n",
    "    }\n",
    ")\n",
    "print(\"âœ… × ×•×¦×¨×” ××‘× ×” ×ª×™×§×™×•×ª ××—×•×œ×§!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ“ ×ª×¨×’×™×œ 2: Parquet\n",
    "\n",
    "**××©×™××”:**\n",
    "1. ×§×¨× ××ª `venture_funding_deals.parquet`\n",
    "2. ×¡× ×Ÿ ×¨×§ ×¢×¡×§××•×ª ××¢×œ $500M\n",
    "3. ×©××•×¨ ×›-parquet ×¢× ×“×—×™×¡×ª `zstd`\n",
    "4. ×”×©×•×•×” ××ª ×’×•×“×œ ×”×§×•×‘×¥ ×”××§×•×¨×™ ×œ×—×“×©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’ª × ×¡×” ×‘×¢×¦××š:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ“š ×¡×™×›×•×: Parquet\n",
    "\n",
    "**××ª×™ ×œ×”×©×ª××© ×‘-Parquet?**\n",
    "- âœ… ××—×¡×•×Ÿ ××¨×•×š ×˜×•×•×—\n",
    "- âœ… ×§×‘×¦×™× ×’×“×•×œ×™× (>100MB)\n",
    "- âœ… ×¦×•×¨×š ×‘×‘×™×¦×•×¢×™×\n",
    "- âœ… ×¢×‘×•×“×” ×¢× ×¢× ×Ÿ\n",
    "\n",
    "**×˜×™×¤×™×:**\n",
    "- ğŸ’¾ ×“×—×™×¡×”: ×”×©×ª××© ×‘-`zstd` ××• `snappy`\n",
    "- ğŸ“‚ Partitioning: ×œ× ×ª×•× ×™× ×’×“×•×œ×™× ×¢× ×§×‘×•×¦×•×ª ×œ×•×’×™×•×ª\n",
    "- âš¡ Lazy: ×”×©×ª××© ×‘-`scan_parquet()` ×œ×§×‘×¦×™× ×’×“×•×œ×™×\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ×§×¨×™××” ×•×›×ª×™×‘×” ×©×œ ×˜×‘×œ××•×ª Delta Lake\n",
    "\n",
    "### ğŸ“– ××” ×–×” Delta Lake?\n",
    "\n",
    "**Delta Lake** ×”×•× **×¤×•×¨××˜ ××—×¡×•×Ÿ** ××ª×§×“× ×©×‘× ×•×™ ×¢×œ Parquet ×•××•×¡×™×£:\n",
    "- ğŸ”„ **ACID Transactions** - ×¢×¡×§××•×ª ××•×‘×˜×—×•×ª\n",
    "- ğŸ“œ **Version History** - ×”×™×¡×˜×•×¨×™×™×ª ×©×™× ×•×™×™×\n",
    "- â®ï¸ **Time Travel** - ×—×–×¨×” ×œ×’×¨×¡××•×ª ×§×•×“××•×ª\n",
    "- ğŸ” **Schema Evolution** - ×©×™× ×•×™ ××‘× ×” ×‘×˜×•×—\n",
    "\n",
    "**×œ××™ ×–×” ××ª××™×?**\n",
    "- ğŸ¢ Data Lakes ××¨×’×•× ×™×™×\n",
    "- ğŸ”„ ×¦× ×¨×•×ª × ×ª×•× ×™× (Data Pipelines)\n",
    "- ğŸ“Š Data Warehouses ××•×“×¨× ×™×™×"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” ×§×¨×™××ª Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×§×¨×™××” ×‘×¡×™×¡×™×ª\n",
    "df_delta = pl.read_delta('../data/venture_funding_deals_delta')\n",
    "\n",
    "print(f\"ğŸ“Š × ×˜×¢× ×• {len(df_delta):,} ×©×•×¨×•×ª\")\n",
    "df_delta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¾ ×›×ª×™×‘×ª Delta Table\n",
    "\n",
    "**×©×œ×•×©×” ××¦×‘×™ ×›×ª×™×‘×”:**\n",
    "- `append` - ×”×•×¡×¤×” ×œ× ×ª×•× ×™× ×§×™×™××™×\n",
    "- `overwrite` - ××—×™×§×” ×•×›×ª×™×‘×” ××—×“×©\n",
    "- `error` - ×©×’×™××” ×× ×”×˜×‘×œ×” ×§×™×™××ª (×‘×¨×™×¨×ª ××—×“×œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×™×¦×™×¨×ª Delta Table ×—×“×©×”\n",
    "df_to_save = pl.read_csv('../data/customer_shopping_data.csv')\n",
    "\n",
    "df_to_save.write_delta(\n",
    "    '../data/output/shopping_delta',\n",
    "    mode='overwrite'  # ğŸ‘ˆ ××—×™×§×” ×•×”×—×œ×¤×”\n",
    ")\n",
    "print(\"âœ… Delta Table × ×•×¦×¨×”!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×”×•×¡×¤×ª × ×ª×•× ×™× ×—×“×©×™×\n",
    "new_data = pl.DataFrame({\n",
    "    'invoice_no': ['I999999'],\n",
    "    'customer_id': ['C999999'],\n",
    "    'gender': ['Male'],\n",
    "    'age': [25],\n",
    "    'category': ['Electronics'],\n",
    "    'quantity': [1],\n",
    "    'price': [1500.0],\n",
    "    'payment_method': ['Credit Card'],\n",
    "    'invoice_date': ['1/1/2024'],\n",
    "    'shopping_mall': ['Test Mall']\n",
    "})\n",
    "\n",
    "new_data.write_delta(\n",
    "    '../data/output/shopping_delta',\n",
    "    mode='append'  # ğŸ‘ˆ ×”×•×¡×¤×” ×‘×œ×‘×“\n",
    ")\n",
    "print(\"âœ… × ×ª×•× ×™× ×—×“×©×™× × ×•×¡×¤×•!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Partitioned Delta Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×›×ª×™×‘×” ×¢× partitioning\n",
    "df_funding = pl.read_parquet('../data/venture_funding_deals.parquet')\n",
    "\n",
    "df_funding.write_delta(\n",
    "    '../data/output/funding_delta_partitioned',\n",
    "    mode='overwrite',\n",
    "    delta_write_options={\n",
    "        'partition_by': 'Industry'  # ğŸ‘ˆ ×—×œ×•×§×” ×œ×¤×™ ×ª×¢×©×™×™×”\n",
    "    }\n",
    ")\n",
    "print(\"âœ… Delta Table ××—×•×œ×§×ª × ×•×¦×¨×”!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ ×§×¨×™××ª Partition ×¡×¤×¦×™×¤×™\n",
    "\n",
    "×‘××§×•× ×œ×˜×¢×•×Ÿ ×”×›×œ, × ×§×¨× ×¨×§ partition ××—×“:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×§×¨×™××ª ×¨×§ ×—×‘×¨×•×ª Fintech\n",
    "df_fintech_only = pl.read_delta(\n",
    "    '../data/output/funding_delta_partitioned',\n",
    "    pyarrow_options={\n",
    "        'partitions': [('Industry', '=', 'Fintech')]  # ğŸ‘ˆ ×¡×™× ×•×Ÿ!\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š × ×˜×¢× ×• ×¨×§ {len(df_fintech_only):,} ×©×•×¨×•×ª ×-Fintech\")\n",
    "df_fintech_only.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš¡ Lazy Loading ×‘-Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×¡×¨×™×§×” lazy ×©×œ Delta Table\n",
    "lf_delta = pl.scan_delta('../data/venture_funding_deals_delta')\n",
    "\n",
    "result = (\n",
    "    lf_delta\n",
    "    .filter(pl.col('Industry') == 'Fintech')\n",
    "    .select(['Company', 'Amount'])\n",
    "    .head()\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â˜ï¸ ×¢×‘×•×“×” ×¢× Delta ×‘-Cloud (S3, Azure, GCS)\n",
    "\n",
    "Delta Lake × ×”×“×¨ ×œ×¢×‘×•×“×” ×¢× ××—×¡×•×Ÿ ×‘×¢× ×Ÿ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×“×•×’××”: ×§×¨×™××” ×-S3\n",
    "# (×“×•×¨×© ×”×’×“×¨×ª credentials)\n",
    "\n",
    "storage_options = {\n",
    "    'aws_access_key_id': 'YOUR_KEY',\n",
    "    'aws_secret_access_key': 'YOUR_SECRET',\n",
    "    'aws_region': 'us-east-1'\n",
    "}\n",
    "\n",
    "# df = pl.read_delta(\n",
    "#     's3://my-bucket/my-delta-table',\n",
    "#     storage_options=storage_options\n",
    "# )\n",
    "\n",
    "print(\"ğŸ’¡ ×–×• ×“×•×’××” - ×“×•×¨×© ×”×’×“×¨×ª AWS credentials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ“ ×ª×¨×’×™×œ 3: Delta Lake\n",
    "\n",
    "**××©×™××”:**\n",
    "1. ×¦×•×¨ Delta Table ××§×•×‘×¥ CSV\n",
    "2. ×”×•×¡×£ ×©×•×¨×•×ª ×—×“×©×•×ª (append)\n",
    "3. ×§×¨× ××ª ×”×˜×‘×œ×” ×•×‘×“×•×§ ×©×”×©×•×¨×•×ª ×”×ª×•×•×¡×¤×•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’ª × ×¡×” ×‘×¢×¦××š:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ“š ×¡×™×›×•×: Delta Lake\n",
    "\n",
    "**××ª×™ ×œ×”×©×ª××© ×‘-Delta Lake?**\n",
    "- âœ… ×¦× ×¨×•×ª × ×ª×•× ×™× ×™×™×¦×•×¨\n",
    "- âœ… ×¦×•×¨×š ×‘-ACID transactions\n",
    "- âœ… Data Lakes ××¨×’×•× ×™×™×\n",
    "- âœ… ×¢×‘×•×“×” ×¢× ×¢× ×Ÿ\n",
    "\n",
    "**×™×ª×¨×•× ×•×ª ××¨×›×–×™×™×:**\n",
    "- ğŸ”’ ×‘×˜×™×—×•×ª: ACID transactions\n",
    "- ğŸ“œ ×”×™×¡×˜×•×¨×™×”: ×’×¨×¡××•×ª ×•-Time Travel\n",
    "- âš¡ ×‘×™×¦×•×¢×™×: ×‘× ×•×™ ×¢×œ Parquet\n",
    "- ğŸ“ Partitioning: ××•×¤×˜×™××™×–×¦×™×” ××•×˜×•××˜×™×ª\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ×§×¨×™××” ×•×›×ª×™×‘×” ×©×œ ×§×‘×¦×™ JSON\n",
    "\n",
    "### ğŸ“– ××” ×–×” JSON?\n",
    "\n",
    "**JSON (JavaScript Object Notation)** ×”×•× ×¤×•×¨××˜ ×˜×§×¡×˜ ×œ××—×¡×•×Ÿ × ×ª×•× ×™× ××•×‘× ×™×.\n",
    "\n",
    "**×©× ×™ ×¡×•×’×™×:**\n",
    "1. **JSON** ×¨×’×™×œ - ××¢×¨×š ×©×œ ××•×‘×™×™×§×˜×™×: `[{...}, {...}]`\n",
    "2. **NDJSON** (Newline Delimited) - ××•×‘×™×™×§×˜ ×‘×›×œ ×©×•×¨×”: `{...}\\n{...}\\n`\n",
    "\n",
    "**××ª×™ ×œ×”×©×ª××©?**\n",
    "- ğŸŒ APIs ×•-Web Services\n",
    "- ğŸ“¦ × ×ª×•× ×™× ××§×•× × ×™× (nested)\n",
    "- ğŸ”„ ×ª×¦×•×¨×•×ª (configurations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” ×§×¨×™××ª JSON ×¨×’×™×œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×§×¨×™××” ×‘×¡×™×¡×™×ª\n",
    "df_json = pl.read_json('../data/world_population.json')\n",
    "\n",
    "print(f\"ğŸ“Š × ×˜×¢× ×• {len(df_json):,} ×©×•×¨×•×ª\")\n",
    "print(f\"ğŸŒ ××“×™× ×•×ª: {df_json['country'].n_unique()}\")\n",
    "\n",
    "# ×”×¦×’×ª ×—×œ×§ ××”×¢××•×“×•×ª\n",
    "df_json.select(['country', 'pop2023', 'area', 'density']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¾ ×›×ª×™×‘×ª JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×©××™×¨×” ×›-JSON\n",
    "df_sample = df_json.head(10)\n",
    "df_sample.write_json('../data/output/population_sample.json')\n",
    "\n",
    "print(\"âœ… JSON × ×©××¨!\")\n",
    "\n",
    "# ×‘×•××• × ×¨××” ××™×š ×”×§×•×‘×¥ × ×¨××”\n",
    "with open('../data/output/population_sample.json', 'r') as f:\n",
    "    print(\"\\nğŸ“„ ×ª×•×›×Ÿ ×”×§×•×‘×¥ (50 ×ª×•×•×™× ×¨××©×•× ×™×):\")\n",
    "    print(f.read()[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ NDJSON - JSON ×©×•×¨×” ××—×¨ ×©×•×¨×”\n",
    "\n",
    "**×œ××” NDJSON?**\n",
    "- âš¡ ××”×™×¨ ×™×•×ª×¨ ×œ×§×‘×¦×™× ×’×“×•×œ×™×\n",
    "- ğŸ’¾ ×—×•×¡×š ×–×™×›×¨×•×Ÿ\n",
    "- ğŸ”„ ××ª××™× ×œ-streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×§×¨×™××ª NDJSON\n",
    "df_ndjson = pl.read_ndjson('../data/world_population.jsonl')\n",
    "\n",
    "print(f\"ğŸ“Š × ×˜×¢× ×• {len(df_ndjson):,} ×©×•×¨×•×ª ×-NDJSON\")\n",
    "df_ndjson.select(['country', 'pop2023', 'density']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×›×ª×™×‘×ª NDJSON\n",
    "df_sample.write_ndjson('../data/output/population_sample.jsonl')\n",
    "\n",
    "print(\"âœ… NDJSON × ×©××¨!\")\n",
    "\n",
    "# ×”×‘×“×œ ×‘×¤×•×¨××˜:\n",
    "with open('../data/output/population_sample.jsonl', 'r') as f:\n",
    "    print(\"\\nğŸ“„ NDJSON - ×›×œ ×©×•×¨×” = ××•×‘×™×™×§×˜ × ×¤×¨×“:\")\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 2:  # ×¨×§ 2 ×©×•×¨×•×ª ×¨××©×•× ×•×ª\n",
    "            print(line[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš¡ Lazy Loading ×‘-NDJSON\n",
    "\n",
    "×¨×§ NDJSON ×ª×•××š ×‘-lazy loading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×¡×¨×™×§×” lazy\n",
    "lf_ndjson = pl.scan_ndjson('../data/world_population.jsonl')\n",
    "\n",
    "# ××¦×™××ª 5 ×”××“×™× ×•×ª ×”×¦×¤×•×¤×•×ª ×‘×™×•×ª×¨\n",
    "result = (\n",
    "    lf_ndjson\n",
    "    .select(['country', 'density', 'pop2023'])\n",
    "    .sort('density', descending=True)\n",
    "    .head(5)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(\"ğŸ™ï¸ ×”××“×™× ×•×ª ×”×¦×¤×•×¤×•×ª ×‘×™×•×ª×¨:\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸŒ³ ×¢×‘×•×“×” ×¢× JSON ××§×•× ×Ÿ\n",
    "\n",
    "JSON ×™×›×•×œ ×œ×”×›×™×œ ××‘× ×™× ××•×¨×›×‘×™×:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×“×•×’××” ×œ-JSON ××§×•× ×Ÿ\n",
    "nested_json = '''\n",
    "[\n",
    "    {\n",
    "        \"name\": \"John\",\n",
    "        \"age\": 30,\n",
    "        \"address\": {\n",
    "            \"city\": \"New York\",\n",
    "            \"zip\": \"10001\"\n",
    "        },\n",
    "        \"hobbies\": [\"reading\", \"gaming\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Jane\",\n",
    "        \"age\": 25,\n",
    "        \"address\": {\n",
    "            \"city\": \"Los Angeles\",\n",
    "            \"zip\": \"90001\"\n",
    "        },\n",
    "        \"hobbies\": [\"sports\", \"music\", \"travel\"]\n",
    "    }\n",
    "]\n",
    "'''\n",
    "\n",
    "# ×§×¨×™××” ×-string\n",
    "import io\n",
    "df_nested = pl.read_json(io.StringIO(nested_json))\n",
    "\n",
    "print(\"ğŸŒ³ JSON ××§×•× ×Ÿ:\")\n",
    "df_nested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×¤×™×¨×•×§ (unnest) ×©×œ ×¢××•×“×ª struct\n",
    "df_unnested = df_nested.unnest('address')\n",
    "\n",
    "print(\"ğŸ“¦ ××—×¨×™ unnest:\")\n",
    "df_unnested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ“ ×ª×¨×’×™×œ 4: JSON\n",
    "\n",
    "**××©×™××”:**\n",
    "1. ×§×¨× ××ª `world_population.jsonl`\n",
    "2. ×¡× ×Ÿ ××“×™× ×•×ª ×¢× ××•×›×œ×•×¡×™×™×” ××¢×œ 100M\n",
    "3. ×©××•×¨ ×›-NDJSON ×—×“×©\n",
    "4. ×§×¨× ×‘×—×–×¨×” ×•×‘×“×•×§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’ª × ×¡×” ×‘×¢×¦××š:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ“š ×¡×™×›×•×: JSON\n",
    "\n",
    "| ×¤×•×¨××˜ | Lazy Support | ××ª×™ ×œ×”×©×ª××© |\n",
    "|--------|--------------|------------|\n",
    "| JSON | âŒ | ×§×‘×¦×™× ×§×˜× ×™×, APIs |\n",
    "| NDJSON | âœ… | ×§×‘×¦×™× ×’×“×•×œ×™×, streaming |\n",
    "\n",
    "**×˜×™×¤×™×:**\n",
    "- ğŸ“¦ NDJSON ××”×™×¨ ×•×—×¡×›×•× ×™ ×™×•×ª×¨\n",
    "- ğŸŒ³ ×”×©×ª××© ×‘-`unnest()` ×œ××‘× ×™× ××§×•× × ×™×\n",
    "- âš¡ Lazy: ×¨×§ NDJSON ×ª×•××š\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ×§×¨×™××” ×•×›×ª×™×‘×” ×©×œ ×§×‘×¦×™ Excel\n",
    "\n",
    "### ğŸ“– ××” ×–×” Excel?\n",
    "\n",
    "×§×‘×¦×™ Excel (`.xlsx`, `.xls`) ×”× ×¤×•×¨××˜ × ×¤×•×¥ ×‘×¢×•×œ× ×”×¢×¡×§×™.\n",
    "\n",
    "**××ª×’×¨×™×:**\n",
    "- ğŸ“Š ×™×›×•×œ ×œ×”×›×™×œ ××¡×¤×¨ ×’×™×œ×™×•× ×•×ª (sheets)\n",
    "- ğŸ¨ ×¢×™×¦×•×‘, × ×•×¡×—××•×ª, ××§×¨×•\n",
    "- ğŸŒ ××™×˜×™ ×™×—×¡×™×ª\n",
    "\n",
    "**×”×¢×¨×”:** ×¦×¨×™×š ×œ×”×ª×§×™×Ÿ ×ª×•×¡×¤×™×!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×”×ª×§× ×ª ×—×‘×™×œ×•×ª × ×“×¨×©×•×ª (×”×¨×¥ ×¤×¢× ××—×ª)\n",
    "# !pip install xlsx2csv xlsxwriter openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¾ ×›×ª×™×‘×ª ×§×•×‘×¥ Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×”×›× ×ª × ×ª×•× ×™×\n",
    "df_to_excel = pl.read_csv('../data/customer_shopping_data.csv').head(100)\n",
    "\n",
    "# ×›×ª×™×‘×” ×‘×¡×™×¡×™×ª\n",
    "df_to_excel.write_excel(\n",
    "    '../data/output/shopping_data.xlsx',\n",
    "    worksheet='Sales Data',  # ×©× ×”×’×™×œ×™×•×Ÿ\n",
    "    header_format={'bold': True}  # ×›×•×ª×¨×•×ª ××•×“×’×©×•×ª\n",
    ")\n",
    "\n",
    "print(\"âœ… ×§×•×‘×¥ Excel × ×•×¦×¨!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” ×§×¨×™××ª ×§×•×‘×¥ Excel\n",
    "\n",
    "Excel ×™×›×•×œ ×œ×”×›×™×œ ××¡×¤×¨ ×’×™×œ×™×•× ×•×ª, ××– ×¦×¨×™×š ×œ×¦×™×™×Ÿ ××™×–×”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×§×¨×™××” ××’×™×œ×™×•×Ÿ ×¡×¤×¦×™×¤×™\n",
    "df_from_excel = pl.read_excel(\n",
    "    '../data/output/shopping_data.xlsx',\n",
    "    sheet_name='Sales Data',  # ğŸ‘ˆ ×©× ×”×’×™×œ×™×•×Ÿ\n",
    "    engine='xlsx2csv',  # ×× ×•×¢ ××”×™×¨\n",
    "    read_options={'try_parse_dates': True}  # × ×™×ª×•×— ×ª××¨×™×›×™×\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š × ×˜×¢× ×• {len(df_from_excel):,} ×©×•×¨×•×ª ×-Excel\")\n",
    "df_from_excel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“š ×¢×‘×•×“×” ×¢× ××¡×¤×¨ ×’×™×œ×™×•× ×•×ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×™×¦×™×¨×ª workbook ×¢× ××¡×¤×¨ ×’×™×œ×™×•× ×•×ª\n",
    "with pl.ExcelWriter('../data/output/multi_sheet.xlsx') as writer:\n",
    "    # ×’×™×œ×™×•×Ÿ 1: ×›×œ ×”× ×ª×•× ×™×\n",
    "    df_to_excel.write_excel(\n",
    "        workbook=writer,\n",
    "        worksheet='All Data'\n",
    "    )\n",
    "    \n",
    "    # ×’×™×œ×™×•×Ÿ 2: ×¨×§ ×‘×’×“×™×\n",
    "    df_clothing = df_to_excel.filter(pl.col('category') == 'Clothing')\n",
    "    df_clothing.write_excel(\n",
    "        workbook=writer,\n",
    "        worksheet='Clothing Only'\n",
    "    )\n",
    "    \n",
    "    # ×’×™×œ×™×•×Ÿ 3: ×¡×˜×˜×™×¡×˜×™×§×•×ª\n",
    "    stats = df_to_excel.group_by('category').agg([\n",
    "        pl.col('price').mean().alias('avg_price'),\n",
    "        pl.col('quantity').sum().alias('total_quantity')\n",
    "    ])\n",
    "    stats.write_excel(\n",
    "        workbook=writer,\n",
    "        worksheet='Statistics'\n",
    "    )\n",
    "\n",
    "print(\"âœ… Workbook ×¢× 3 ×’×™×œ×™×•× ×•×ª × ×•×¦×¨!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¨ ×¢×™×¦×•×‘ ××ª×§×“×"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×™×¦×™×¨×ª Excel ××¢×•×¦×‘\n",
    "df_styled = df_to_excel.head(20)\n",
    "\n",
    "# ×¤×•×¨××˜×™× ××•×ª×××™× ××™×©×™×ª\n",
    "df_styled.write_excel(\n",
    "    '../data/output/styled_report.xlsx',\n",
    "    worksheet='Report',\n",
    "    header_format={\n",
    "        'bold': True,\n",
    "        'font_color': 'white',\n",
    "        'bg_color': '#4472C4',\n",
    "        'border': 1\n",
    "    },\n",
    "    column_formats={\n",
    "        'price': 'â‚ª#,##0.00',  # ×¤×•×¨××˜ ××˜×‘×¢\n",
    "        'invoice_date': 'dd/mm/yyyy'  # ×¤×•×¨××˜ ×ª××¨×™×š\n",
    "    },\n",
    "    column_widths={\n",
    "        'customer_id': 120,\n",
    "        'shopping_mall': 150\n",
    "    },\n",
    "    autofit=True  # ×”×ª×××” ××•×˜×•××˜×™×ª ×œ×ª×•×›×Ÿ\n",
    ")\n",
    "\n",
    "print(\"ğŸ¨ ×“×•×— ××¢×•×¦×‘ × ×•×¦×¨!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### âš ï¸ ×”×’×‘×œ×•×ª ×—×©×•×‘×•×ª\n",
    "\n",
    "- ğŸ“ Excel ××•×’×‘×œ ×œ-**1,048,576 ×©×•×¨×•×ª**\n",
    "- ğŸŒ ××™×˜×™ ×œ×§×‘×¦×™× ×’×“×•×œ×™×\n",
    "- ğŸ’¾ ×’×•×“×œ ×§×•×‘×¥ ×’×“×•×œ\n",
    "\n",
    "**×”××œ×¦×”:** ×œ× ×ª×•× ×™× ×’×“×•×œ×™×, ×”×©×ª××© ×‘-Parquet ××• CSV!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ×ª×¨×’×™×œ 5: Excel\n",
    "\n",
    "**××©×™××”:** ×¦×•×¨ ×“×•×— Excel ×¢× 3 ×’×™×œ×™×•× ×•×ª:\n",
    "1. \"Raw Data\" - ×›×œ ×”× ×ª×•× ×™×\n",
    "2. \"Summary\" - ×¡×™×›×•× ×œ×¤×™ ×§×˜×’×•×¨×™×”\n",
    "3. \"High Value\" - ×¨×§ ×¨×›×™×©×•×ª ××¢×œ 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’ª × ×¡×” ×‘×¢×¦××š:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ“š ×¡×™×›×•×: Excel\n",
    "\n",
    "**××ª×™ ×œ×”×©×ª××©:**\n",
    "- âœ… ×“×•×—×•×ª ×¢×¡×§×™×™×\n",
    "- âœ… ×©×™×ª×•×£ ×¢× ××©×ª××©×™ Excel\n",
    "- âœ… × ×ª×•× ×™× ×§×˜× ×™×-×‘×™× ×•× ×™×™× (<100K ×©×•×¨×•×ª)\n",
    "\n",
    "**×—×¡×¨×•× ×•×ª:**\n",
    "- âŒ ××™×˜×™\n",
    "- âŒ ××•×’×‘×œ ×‘×’×•×“×œ\n",
    "- âŒ ×œ× ××ª××™× ×œ××•×˜×•××¦×™×”\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ×¡×™×›×•× ×•×˜×™×¤×™×\n",
    "\n",
    "### ğŸ¯ ×˜×‘×œ×ª ×”×”×—×œ×˜×•×ª - ××™×–×” ×¤×•×¨××˜ ×œ×‘×—×•×¨?\n",
    "\n",
    "| ×¦×•×¨×š | ×¤×•×¨××˜ ××•××œ×¥ | ×œ××”? |\n",
    "|------|-------------|------|\n",
    "| ×©×™×ª×•×£ ×¢× ×× ×©×™× | CSV | ×§×œ ×œ×¤×ª×™×—×” ×‘×›×œ ×ª×•×›× ×” |\n",
    "| ×“×•×—×•×ª ×¢×¡×§×™×™× | Excel | ××•×›×¨ ×•× ×•×— ×œ×× ×”×œ×™× |\n",
    "| ××—×¡×•×Ÿ ××¨×•×š ×˜×•×•×— | Parquet | ×“×—×•×¡, ××”×™×¨, ×©×•××¨ ×˜×™×¤×•×¡×™× |\n",
    "| Data Lake ××¨×’×•× ×™ | Delta Lake | ACID, versioning |\n",
    "| API / Web | JSON/NDJSON | ×ª×§×Ÿ ××™× ×˜×¨× ×˜ |\n",
    "| ×§×‘×¦×™× ×¢× ×§×™×™× | Parquet + Lazy | ××•×¤×˜×™××œ×™ ×œ×‘×™×¦×•×¢×™× |\n",
    "| × ×ª×•× ×™× ×–×× ×™×™× | IPC/Arrow | ××”×™×¨ ×‘×™×•×ª×¨ (×‘×™× ××¨×™) |\n",
    "\n",
    "---\n",
    "\n",
    "### âš¡ ×˜×™×¤×™× ×œ×‘×™×¦×•×¢×™×\n",
    "\n",
    "#### 1. ×”×©×ª××© ×‘-Lazy Loading ×œ×§×‘×¦×™× ×’×“×•×œ×™×\n",
    "\n",
    "```python\n",
    "# âŒ ×œ× ×™×¢×™×œ\n",
    "df = pl.read_csv('huge_file.csv')\n",
    "df = df.filter(pl.col('age') > 30)\n",
    "\n",
    "# âœ… ×™×¢×™×œ!\n",
    "result = (\n",
    "    pl.scan_csv('huge_file.csv')\n",
    "    .filter(pl.col('age') > 30)\n",
    "    .collect()\n",
    ")\n",
    "```\n",
    "\n",
    "#### 2. ×”×©×ª××© ×‘-schema_overrides ×œ×—×™×¡×›×•×Ÿ ×‘×–×™×›×¨×•×Ÿ\n",
    "\n",
    "```python\n",
    "# âŒ ××‘×–×‘×– ×–×™×›×¨×•×Ÿ\n",
    "df = pl.read_csv('data.csv')  # age ×”×•×¤×š ×œ-Int64\n",
    "\n",
    "# âœ… ×—×•×¡×š ×¤×™ 8!\n",
    "df = pl.read_csv('data.csv', schema_overrides={'age': pl.Int8})\n",
    "```\n",
    "\n",
    "#### 3. ×‘×—×¨ ××ª ×¤×•×¨××˜ ×”×“×—×™×¡×” ×”× ×›×•×Ÿ\n",
    "\n",
    "```python\n",
    "# ×œ××—×¡×•×Ÿ ××¨×•×š ×˜×•×•×—\n",
    "df.write_parquet('archive.parquet', compression='zstd', compression_level=10)\n",
    "\n",
    "# ×œ×¢×‘×•×“×” ×™×•××™×•××™×ª\n",
    "df.write_parquet('working.parquet', compression='snappy')\n",
    "```\n",
    "\n",
    "#### 4. ×”×©×ª××© ×‘-Partitioning ×œ× ×ª×•× ×™× ×’×“×•×œ×™×\n",
    "\n",
    "```python\n",
    "# ×§×¨×™××” ××”×™×¨×” ×©×œ ×¨×§ ×”× ×ª×•× ×™× ×”×¨×œ×•×•× ×˜×™×™×\n",
    "df.write_parquet(\n",
    "    'data_partitioned',\n",
    "    use_pyarrow=True,\n",
    "    pyarrow_options={'partition_cols': ['year', 'month']}\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ› ï¸ ×¤×ª×¨×•×Ÿ ×‘×¢×™×•×ª × ×¤×•×¦×•×ª\n",
    "\n",
    "#### ×‘×¢×™×” 1: \"Out of Memory\"\n",
    "\n",
    "**×¤×ª×¨×•×Ÿ:**\n",
    "```python\n",
    "# ×‘××§×•× read_*, ×”×©×ª××© ×‘-scan_*\n",
    "lf = pl.scan_parquet('huge.parquet')\n",
    "result = lf.filter(...).select(...).collect()\n",
    "```\n",
    "\n",
    "#### ×‘×¢×™×” 2: ×ª××¨×™×›×™× ×œ× ××–×•×”×™× × ×›×•×Ÿ\n",
    "\n",
    "**×¤×ª×¨×•×Ÿ:**\n",
    "```python\n",
    "# ××¤×©×¨ try_parse_dates\n",
    "df = pl.read_csv('data.csv', try_parse_dates=True)\n",
    "\n",
    "# ××• ×”×’×“×¨ ×¤×•×¨××˜ ×¡×¤×¦×™×¤×™\n",
    "df = df.with_columns(\n",
    "    pl.col('date').str.strptime(pl.Date, '%d/%m/%Y')\n",
    ")\n",
    "```\n",
    "\n",
    "#### ×‘×¢×™×” 3: ×§×•×‘×¥ CSV ×¢× encoding ×œ× ×ª×§×™×Ÿ\n",
    "\n",
    "**×¤×ª×¨×•×Ÿ:**\n",
    "```python\n",
    "# × ×¡×” encodings ×©×•× ×™×\n",
    "df = pl.read_csv('data.csv', encoding='utf8-lossy')\n",
    "# ××•\n",
    "df = pl.read_csv('data.csv', encoding='latin1')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š ××©××‘×™× × ×•×¡×¤×™×\n",
    "\n",
    "- ğŸ“– [Polars Documentation](https://docs.pola.rs/)\n",
    "- ğŸ’¬ [Polars Discord](https://discord.gg/4UfP5cfBE7)\n",
    "- ğŸ™ [Polars GitHub](https://github.com/pola-rs/polars)\n",
    "- ğŸ“º [Polars YouTube Channel](https://www.youtube.com/@polarsDataFrame)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ‰ ×¡×™×•×\n",
    "\n",
    "**××–×œ ×˜×•×‘! ×¡×™×™××ª ××ª ×”××“×¨×™×š ×”××§×™×£ ×œ×§×¨×™××” ×•×›×ª×™×‘×” ×©×œ ×§×‘×¦×™× ×‘-Polars! ğŸŠ**\n",
    "\n",
    "×¢×›×©×™×• ××ª×” ×™×•×“×¢:\n",
    "- âœ… ××™×š ×œ×¢×‘×•×“ ×¢× ×›×œ ×¤×•×¨××˜×™ ×”×§×‘×¦×™× ×”× ×¤×•×¦×™×\n",
    "- âœ… ××ª×™ ×œ×”×©×ª××© ×‘×›×œ ×¤×•×¨××˜\n",
    "- âœ… ××™×š ×œ××•×¤×˜×™××™×™×– ×‘×™×¦×•×¢×™× ×•×–×™×›×¨×•×Ÿ\n",
    "- âœ… ××™×š ×œ×¤×ª×•×¨ ×‘×¢×™×•×ª × ×¤×•×¦×•×ª\n",
    "\n",
    "**×”××©×š ×œ×œ××•×“ ×•×œ×ª×¨×’×œ! ğŸš€**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
