{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“š ××“×¨×™×š ××§×™×£: ×¢×‘×•×“×” ×¢× ××§×•×¨×•×ª × ×ª×•× ×™× ×‘×¢× ×Ÿ ×‘-Polars\n",
    "\n",
    "# ×ª×§×œ×•×ª ×‘×”×ª×—×‘×¨×•×ª!!!\n",
    "\n",
    "## ×¤×¨×§ 11: Working With Common Cloud Sources\n",
    "\n",
    "---\n",
    "\n",
    "×‘×¨×•×›×™× ×”×‘××™× ×œ××“×¨×™×š ×”××§×™×£ ×œ×¢×‘×•×“×” ×¢× ××§×•×¨×•×ª × ×ª×•× ×™× ×‘×¢× ×Ÿ! ×‘××“×¨×™×š ×–×” ×ª×œ××“×• ×›×™×¦×“ ×œ×§×¨×•× ×•×œ×›×ª×•×‘ × ×ª×•× ×™× ××¤×œ×˜×¤×•×¨××•×ª ×¢× ×Ÿ ××•×‘×™×œ×•×ª ×‘×××¦×¢×•×ª ×¡×¤×¨×™×™×ª Polars.\n",
    "\n",
    "### ğŸ“‹ ×ª×•×›×Ÿ ×¢× ×™×™× ×™×\n",
    "\n",
    "1. [×××–×•×Ÿ S3 (Amazon S3)](#amazon-s3)\n",
    "   - [×§×¨×™××” ×-S3](#reading-from-s3)\n",
    "   - [×›×ª×™×‘×” ×œ-S3](#writing-to-s3)\n",
    "   - [×§×¨×™××” ×¢× ××™××•×ª](#reading-with-authentication)\n",
    "   - [×¢×‘×•×“×” ×¢× PyArrow Dataset](#working-with-pyarrow)\n",
    "\n",
    "2. [Azure Blob Storage](#azure-blob-storage)\n",
    "   - [×§×¨×™××” ×-Azure](#reading-from-azure)\n",
    "   - [×›×ª×™×‘×” ×œ-Azure](#writing-to-azure)\n",
    "   - [×¢×‘×•×“×” ×¢× ××©×ª× ×™ ×¡×‘×™×‘×”](#azure-environment-variables)\n",
    "   - [Azure Data Lake Storage](#azure-data-lake)\n",
    "\n",
    "3. [Google Cloud Storage (GCS)](#google-cloud-storage)\n",
    "   - [×§×¨×™××” ×-GCS](#reading-from-gcs)\n",
    "   - [××™××•×ª ×¢× Service Account](#gcs-authentication)\n",
    "   - [×›×ª×™×‘×” ×œ-GCS](#writing-to-gcs)\n",
    "\n",
    "4. [BigQuery](#bigquery)\n",
    "   - [×§×¨×™××” ×-BigQuery](#reading-from-bigquery)\n",
    "   - [×›×ª×™×‘×” ×œ-BigQuery](#writing-to-bigquery)\n",
    "\n",
    "5. [Snowflake](#snowflake)\n",
    "   - [×—×™×‘×•×¨ ×œ-Snowflake](#connecting-to-snowflake)\n",
    "   - [×§×¨×™××ª × ×ª×•× ×™×](#reading-from-snowflake)\n",
    "   - [×©×™×˜×•×ª ×—×™×‘×•×¨ × ×•×¡×¤×•×ª](#alternative-snowflake-methods)\n",
    "\n",
    "6. [×ª×¨×’×™×œ×™× ××¢×©×™×™×](#exercises)\n",
    "7. [×¡×™×›×•× ×•××©××‘×™× × ×•×¡×¤×™×](#summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ××˜×¨×•×ª ×”×œ××™×“×”\n",
    "\n",
    "×‘×¡×™×•× ××“×¨×™×š ×–×” ×ª×“×¢×•:\n",
    "\n",
    "âœ… ×›×™×¦×“ ×œ×”×ª×—×‘×¨ ×œ××§×•×¨×•×ª × ×ª×•× ×™× ×‘×¢× ×Ÿ (S3, Azure, GCS, BigQuery, Snowflake)\n",
    "\n",
    "âœ… ××™×š ×œ×§×¨×•× ×•×œ×›×ª×•×‘ ×§×‘×¦×™× ×‘×¤×•×¨××˜×™× ×©×•× ×™× (CSV, Parquet) ××”×¢× ×Ÿ\n",
    "\n",
    "âœ… ×›×™×¦×“ ×œ× ×”×œ ××™××•×ª ×•×”×¨×©××•×ª ×‘×¦×•×¨×” ×××•×‘×˜×—×ª\n",
    "\n",
    "âœ… ×“×¨×›×™× ×™×¢×™×œ×•×ª ×œ×¢×‘×•×“ ×¢× × ×ª×•× ×™× ×’×“×•×œ×™× ×‘×¢× ×Ÿ\n",
    "\n",
    "âœ… ×©×™×œ×•×‘ Polars ×¢× ×›×œ×™ ×¢× ×Ÿ × ×¤×•×¦×™×\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ ×”×ª×§× ×ª ×—×‘×™×œ×•×ª × ×“×¨×©×•×ª\n",
    "\n",
    "×œ×¤× ×™ ×©××ª×—×™×œ×™×, × ×•×•×“× ×©×›×œ ×”×—×‘×™×œ×•×ª ×”× ×“×¨×©×•×ª ××•×ª×§× ×•×ª:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”×ª×§× ×ª ×—×‘×™×œ×•×ª × ×“×¨×©×•×ª (×”×¡×¨ ××ª ×”-# ×œ×”×¨×¦×”)\n",
    "# !pip install polars\n",
    "# !pip install s3fs              # ×¢×‘×•×¨ Amazon S3\n",
    "# !pip install adlfs             # ×¢×‘×•×¨ Azure Blob Storage\n",
    "# !pip install gcsfs             # ×¢×‘×•×¨ Google Cloud Storage\n",
    "# !pip install google-cloud-bigquery\n",
    "# !pip install pyarrow\n",
    "# !pip install snowflake-connector-python\n",
    "# !pip install connectorx        # ×¢×‘×•×¨ ×—×™×‘×•×¨ ×œ××¡×“×™ × ×ª×•× ×™×\n",
    "# !pip install adbc-driver-snowflake  # ×¢×‘×•×¨ Snowflake"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"amazon-s3\"></a>\n",
    "# 1ï¸âƒ£ Amazon S3 (Simple Storage Service)\n",
    "\n",
    "## ğŸ” ××‘×•×\n",
    "\n",
    "**Amazon S3** ×”×•× ×©×™×¨×•×ª ××—×¡×•×Ÿ ××•×‘×™×™×§×˜×™× ×©×œ AWS ×”×××¤×©×¨ ××—×¡×•×Ÿ ×•×”×—×–×¨×ª ×›××•×™×•×ª ×’×“×•×œ×•×ª ×©×œ × ×ª×•× ×™×. \n",
    "\n",
    "### ×œ××” ×œ×”×©×ª××© ×‘-S3?\n",
    "- ğŸŒ **×–××™× ×•×ª ×’×‘×•×”×”** - 99.999999999% (11 ×ª×©×™×¢×™×•×ª)\n",
    "- ğŸ’° **×¢×œ×•×ª × ××•×›×”** - ×ª×©×œ×•× ×œ×¤×™ ×©×™××•×©\n",
    "- ğŸ“ˆ **××“×¨×’×™×•×ª** - ×ª×•××š ×‘×›××•×™×•×ª × ×ª×•× ×™× ×‘×œ×ª×™ ××•×’×‘×œ×•×ª\n",
    "- ğŸ” **××‘×˜×—×”** - ×©×›×‘×•×ª ××‘×˜×—×” ××ª×§×“××•×ª\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reading-from-s3\"></a>\n",
    "## ğŸ“– ×§×¨×™××” ×-S3 - ×”×©×™×˜×” ×”×¤×©×•×˜×”\n",
    "\n",
    "×”×“×¨×š ×”×›×™ ×¤×©×•×˜×” ×œ×§×¨×•× ×§×•×‘×¥ ×-S3 ×”×™× ×‘×××¦×¢×•×ª ×”×¤×•× ×§×¦×™×” `pl.read_csv()` ×¢× × ×ª×™×‘ S3."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import polars as pl\n",
    "\n",
    "# ×”×“×¤×¡×ª ×’×¨×¡×ª Polars\n",
    "print(f\"ğŸ»â€â„ï¸ Polars version: {pl.__version__}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ ×“×•×’××”: ×§×¨×™××ª ×§×•×‘×¥ CSV ×-S3"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×©×œ×‘ 1: ×”×’×“×¨×ª × ×ª×™×‘ ×”×§×•×‘×¥ ×‘-S3\n",
    "# ×¤×•×¨××˜: s3://bucket-name/path/to/file.csv\n",
    "s3_file_path = 's3://polars-cookbook-demo-yk/titanic_dataset.csv'\n",
    "\n",
    "# ×©×œ×‘ 2: ×§×¨×™××ª ×”×§×•×‘×¥\n",
    "df = pl.read_csv(s3_file_path)\n",
    "\n",
    "# ×©×œ×‘ 3: ×”×¦×’×ª 5 ×”×©×•×¨×•×ª ×”×¨××©×•× ×•×ª\n",
    "print(\"âœ… ×”× ×ª×•× ×™× × ×§×¨××• ×‘×”×¦×œ×—×” ×-S3!\")\n",
    "print(f\"ğŸ“Š ××¡×¤×¨ ×©×•×¨×•×ª: {df.shape[0]:,}\")\n",
    "print(f\"ğŸ“‹ ××¡×¤×¨ ×¢××•×“×•×ª: {df.shape[1]}\")\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ×”×¡×‘×¨ ××¤×•×¨×˜:\n",
    "\n",
    "1. **× ×ª×™×‘ S3**: `s3://bucket-name/file.csv`\n",
    "   - `s3://` - ×¤×¨×•×˜×•×§×•×œ S3\n",
    "   - `bucket-name` - ×©× ×”-bucket (×›××• ×ª×™×§×™×™×” ×¨××©×™×ª)\n",
    "   - `file.csv` - × ×ª×™×‘ ×”×§×•×‘×¥ ×‘×ª×•×š ×”-bucket\n",
    "\n",
    "2. **×§×¨×™××” ××•×˜×•××˜×™×ª**: Polars ××–×”×” ××ª ×”×¤×¨×•×˜×•×§×•×œ `s3://` ×•××˜×¤×œ ×‘××•×¤×Ÿ ××•×˜×•××˜×™ ×‘×—×™×‘×•×¨\n",
    "\n",
    "3. **×œ×œ× ××™××•×ª**: ×“×•×’××” ×–×• ×¢×•×‘×“×ª ×¢× bucket ×¤×•××‘×™ (public)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"writing-to-s3\"></a>\n",
    "## ğŸ“ ×›×ª×™×‘×” ×œ-S3 - ×©××™×¨×ª ×§×‘×¦×™×\n",
    "\n",
    "×›×“×™ ×œ×›×ª×•×‘ ×§×‘×¦×™× ×œ-S3, × ×©×ª××© ×‘×¡×¤×¨×™×™×ª `s3fs` ×©××¡×¤×§×ª ×××©×§ ×§×‘×¦×™× ×œ-S3."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import s3fs\n",
    "\n",
    "# ×©×œ×‘ 1: ×™×¦×™×¨×ª ××•×¤×¢ ×©×œ S3FileSystem\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "# ×©×œ×‘ 2: ×”×’×“×¨×ª × ×ª×™×‘ ×”×™×¢×“ ×‘-S3\n",
    "s3_parquet_file_path = 's3://polars-cookbook-demo-yk/titanic_dataset.parquet'\n",
    "\n",
    "# ×©×œ×‘ 3: ×¤×ª×™×—×ª ×”×§×•×‘×¥ ×‘××¦×‘ ×›×ª×™×‘×” ×•×›×ª×™×‘×ª ×”-DataFrame\n",
    "with fs.open(s3_parquet_file_path, mode='wb') as f:\n",
    "    df.write_parquet(f)\n",
    "\n",
    "print(\"âœ… ×”×§×•×‘×¥ × ×©××¨ ×‘×”×¦×œ×—×” ×œ-S3 ×‘×¤×•×¨××˜ Parquet!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ×”×¡×‘×¨ ××¤×•×¨×˜:\n",
    "\n",
    "1. **S3FileSystem**: ××—×œ×§×” ×”××¡×¤×§×ª ×××©×§ ×“××•×™-×§×•×‘×¥ ×œ×¢×‘×•×“×” ×¢× S3\n",
    "\n",
    "2. **Context Manager (`with`)**: ××‘×˜×™×— ×¡×’×™×¨×” × ×›×•× ×” ×©×œ ×”×—×™×‘×•×¨\n",
    "\n",
    "3. **××¦×‘ ×›×ª×™×‘×” (`'wb'`)**:\n",
    "   - `w` = write (×›×ª×™×‘×”)\n",
    "   - `b` = binary (×‘×™× ××¨×™, × ×“×¨×© ×œ-Parquet)\n",
    "\n",
    "4. **×¤×•×¨××˜ Parquet**:\n",
    "   - âœ… ×“×—×•×¡ ×•×™×¢×™×œ (50-80% ×¤×—×•×ª ××§×•×)\n",
    "   - âœ… ×§×¨×™××” ××”×™×¨×” ×™×•×ª×¨\n",
    "   - âœ… ×©×•××¨ ×˜×™×¤×•×¡×™ × ×ª×•× ×™×\n",
    "\n",
    "### ğŸ’¡ ×˜×™×¤ ×—×©×•×‘:\n",
    "> **Parquet vs CSV**: ×”×©×ª××©×• ×‘-Parquet ×œ× ×ª×•× ×™× ×’×“×•×œ×™× ×•-CSV ×¨×§ ×œ×§×‘×¦×™× ×§×˜× ×™× ××• ×œ×©×™×ª×•×£ ×× ×•×©×™.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reading-with-authentication\"></a>\n",
    "## ğŸ” ×§×¨×™××” ×¢× ××™××•×ª (Authentication)\n",
    "\n",
    "×¢×‘×•×¨ buckets ×¤×¨×˜×™×™×, × ×“×¨×© ××™××•×ª ×‘×××¦×¢×•×ª AWS credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ×©×™×˜×” 1: ×©×™××•×© ×‘-storage_options"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”×’×“×¨×ª ×¤×¨××˜×¨×™ ××™××•×ª\n",
    "storage_options = {\n",
    "    'aws_access_key_id': 'YOUR_ACCESS_KEY_ID',          # ××¤×ª×— ×’×™×©×”\n",
    "    'aws_secret_access_key': 'YOUR_SECRET_ACCESS_KEY',  # ××¤×ª×— ×¡×•×“×™\n",
    "    'aws_region': 'us-east-1'                            # ××–×•×¨ AWS\n",
    "}\n",
    "\n",
    "# ×§×¨×™××” ×¢× Lazy evaluation\n",
    "lf = pl.scan_parquet(s3_parquet_file_path, storage_options=storage_options)\n",
    "\n",
    "# ×”×¨×¦×ª ×”×—×™×©×•×‘\n",
    "result = lf.head().collect()\n",
    "print(\"âœ… × ×ª×•× ×™× × ×§×¨××• ×‘×”×¦×œ×—×” ×¢× ××™××•×ª!\")\n",
    "result"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ×”×¡×‘×¨ ×¢×œ Lazy Evaluation:\n",
    "\n",
    "**`scan_parquet()`** ××—×–×™×¨ `LazyFrame` ×‘××§×•× `DataFrame`:\n",
    "\n",
    "| ×ª×›×•× ×” | DataFrame | LazyFrame |\n",
    "|-------|-----------|------------|\n",
    "| âš¡ ×‘×™×¦×•×¢ | ××™×™×“×™ | ×“×—×•×™ (Lazy) |\n",
    "| ğŸ’¾ ×–×™×›×¨×•×Ÿ | ×˜×•×¢×Ÿ ×”×›×œ | ×˜×•×¢×Ÿ ×œ×¤×™ ×¦×•×¨×š |\n",
    "| ğŸ¯ ××•×¤×˜×™××™×–×¦×™×” | ×‘×¡×™×¡×™×ª | ××ª×§×“××ª |\n",
    "| ğŸ“Š ××ª×™ ×œ×”×©×ª××© | × ×ª×•× ×™× ×§×˜× ×™× | × ×ª×•× ×™× ×’×“×•×œ×™× |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ×©×™×˜×” 2: ×›×ª×™×‘×” ×¢× ××™××•×ª"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×¦×™×¨×ª S3FileSystem ×¢× credentials\n",
    "fs = s3fs.S3FileSystem(\n",
    "    key='YOUR_ACCESS_KEY_ID',\n",
    "    secret='YOUR_SECRET_ACCESS_KEY'\n",
    ")\n",
    "\n",
    "# ×›×ª×™×‘×” ×××•××ª×ª\n",
    "with fs.open(s3_parquet_file_path, mode='wb') as f:\n",
    "    df.write_parquet(f)\n",
    "\n",
    "print(\"âœ… ×”×§×•×‘×¥ × ×©××¨ ×œ-S3 ×¤×¨×˜×™ ×‘×”×¦×œ×—×”!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš ï¸ ××–×”×¨×ª ××‘×˜×—×”:\n",
    "\n",
    "> **×œ×¢×•×œ× ××œ ×ª×©××¨×• credentials ×‘×§×•×“!**\n",
    "> \n",
    "> ×“×¨×›×™× ×××•×‘×˜×—×•×ª:\n",
    "> 1. ××©×ª× ×™ ×¡×‘×™×‘×” (Environment Variables)\n",
    "> 2. AWS IAM Roles\n",
    "> 3. ×§×‘×¦×™ ×§×•× ×¤×™×’×•×¨×¦×™×” ××•×¦×¤× ×™×\n",
    "> 4. AWS Secrets Manager\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"working-with-pyarrow\"></a>\n",
    "## ğŸ¹ ×¢×‘×•×“×” ×¢× PyArrow Dataset\n",
    "\n",
    "PyArrow ×××¤×©×¨ ×§×¨×™××” ×™×¢×™×œ×” ×¢× ×¡×™× ×•×Ÿ ×œ×¤× ×™ ×”×˜×¢×™× ×”."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pyarrow.dataset as ds\n",
    "\n",
    "# ×©×œ×‘ 1: ×™×¦×™×¨×ª Dataset ×-S3\n",
    "dataset = ds.dataset(\n",
    "    s3_parquet_file_path, \n",
    "    format='parquet',      # ×¤×•×¨××˜ ×”×§×•×‘×¥\n",
    "    filesystem=fs          # ××¢×¨×›×ª ×”×§×‘×¦×™× (S3)\n",
    ")\n",
    "\n",
    "# ×©×œ×‘ 2: ×¡×™× ×•×Ÿ ××ª×§×“× ×œ×¤× ×™ ×§×¨×™××”\n",
    "df_filtered = (\n",
    "    pl.scan_pyarrow_dataset(dataset)\n",
    "    .filter(pl.col('Age') <= 30)  # ×¨×§ × ×•×¡×¢×™× ×¢×“ ×’×™×œ 30\n",
    "    .collect()                     # ×”×¨×¦×ª ×”×—×™×©×•×‘\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š ×¡×•× ×Ÿ ×œ-{df_filtered.shape[0]:,} ×©×•×¨×•×ª (×’×™×œ <= 30)\")\n",
    "df_filtered.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ×™×ª×¨×•× ×•×ª PyArrow Dataset:\n",
    "\n",
    "1. **Predicate Pushdown** ğŸš€\n",
    "   - ×”×¡×™× ×•×Ÿ ×§×•×¨×” ×‘-S3 ×œ×¤× ×™ ×”×˜×¢×™× ×”\n",
    "   - ×—×•×¡×š ×–××Ÿ ×•×¨×•×—×‘ ×¤×¡\n",
    "\n",
    "2. **Partition Pruning** ğŸ“\n",
    "   - ×§×•×¨× ×¨×§ partitions ×¨×œ×•×•× ×˜×™×™×\n",
    "   - ××™×“×™××œ×™ ×œ× ×ª×•× ×™× ××—×•×œ×§×™×\n",
    "\n",
    "3. **Lazy Evaluation** âš¡\n",
    "   - ××—×©×‘ ×¨×§ ××” ×©× ×“×¨×©\n",
    "   - ××•×¤×˜×™××™×–×¦×™×” ××•×˜×•××˜×™×ª\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š ×ª×¨×’×™×œ 1: ×¢×‘×•×“×” ×¢× S3\n",
    "\n",
    "### ğŸ¯ ××©×™××”:\n",
    "×§×¨× ××ª dataset Titanic ×-S3, ×¡× ×Ÿ × ×•×¡×¢×™× ×©× ×©×¨×“×• (Survived=1) ×•×‘× ×™ 18-40, ×•×”×¦×’ ××ª ×©×›×™×—×•×ª ×”××™×Ÿ."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”×ª×—×œ ×›××Ÿ - ××œ× ××ª ×”×§×•×“\n",
    "\n",
    "# ×©×œ×‘ 1: ×§×¨× ××ª ×”× ×ª×•× ×™× ×-S3\n",
    "s3_path = 's3://polars-cookbook-demo-yk/titanic_dataset.csv'\n",
    "# df = ...\n",
    "\n",
    "# ×©×œ×‘ 2: ×¡× ×Ÿ ×œ×¤×™ ×”×ª× ××™×\n",
    "# df_survivors = df.filter(...)\n",
    "\n",
    "# ×©×œ×‘ 3: ×—×©×‘ ×©×›×™×—×•×ª ×œ×¤×™ ××™×Ÿ\n",
    "# result = df_survivors...\n",
    "\n",
    "# ×”×“×¤×¡ ×ª×•×¦××”\n",
    "# result"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ ×¤×ª×¨×•×Ÿ:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×¤×ª×¨×•×Ÿ ××•×¡×ª×¨ - ×œ×—×¥ ×œ×”×¨×¦×”\n",
    "\n",
    "df = pl.read_csv('s3://polars-cookbook-demo-yk/titanic_dataset.csv')\n",
    "\n",
    "result = (\n",
    "    df\n",
    "    .filter(\n",
    "        (pl.col('Survived') == 1) &\n",
    "        (pl.col('Age') >= 18) &\n",
    "        (pl.col('Age') <= 40)\n",
    "    )\n",
    "    .group_by('Sex')\n",
    "    .agg([\n",
    "        pl.len().alias('Count'),\n",
    "        pl.col('Age').mean().alias('Avg_Age')\n",
    "    ])\n",
    "    .sort('Count', descending=True)\n",
    ")\n",
    "\n",
    "print(\"âœ… × ×™×ª×•×— ×”×•×©×œ×!\")\n",
    "result"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"azure-blob-storage\"></a>\n",
    "# 2ï¸âƒ£ Azure Blob Storage\n",
    "\n",
    "## ğŸ” ××‘×•×\n",
    "\n",
    "**Azure Blob Storage** ×”×•× ×©×™×¨×•×ª ××—×¡×•×Ÿ ××•×‘×™×™×§×˜×™× ×©×œ Microsoft Azure, ×“×•××” ×œ-S3 ×©×œ AWS.\n",
    "\n",
    "### ×œ××” ×œ×”×©×ª××© ×‘-Azure Blob?\n",
    "- â˜ï¸ **××™× ×˜×’×¨×¦×™×” ×¢× Azure** - ×—×œ×§ ××”××§×•×¡×™×¡×˜× ×©×œ Microsoft\n",
    "- ğŸ’¼ **×ª××™×›×” ××¨×’×•× ×™×ª** - ××ª××™× ×œ×¡×‘×™×‘×•×ª Enterprise\n",
    "- ğŸ”„ **×¨××•×ª ××—×¡×•×Ÿ** - Hot, Cool, Archive\n",
    "- ğŸŒ **×–××™× ×•×ª ×’×œ×•×‘×œ×™×ª** - data centers ×‘×¨×—×‘×™ ×”×¢×•×œ×\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reading-from-azure\"></a>\n",
    "## ğŸ“– ×§×¨×™××” ×-Azure Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”×’×“×¨×ª ×¤×¨××˜×¨×™ ××™××•×ª ×œAzure\n",
    "storage_options = {\n",
    "    'account_name': 'YOUR_ACCOUNT_NAME',      # ×©× ×—×©×‘×•×Ÿ ×”××—×¡×•×Ÿ\n",
    "    'access_key': 'YOUR_ACCOUNT_KEY',         # ××¤×ª×— ×’×™×©×”\n",
    "    'client_id': 'YOUR_CLIENT_ID',            # (××•×¤×¦×™×•× ×œ×™) ×¢×‘×•×¨ Service Principal\n",
    "    'client_secret': 'YOUR_CLIENT_SECRET',    # (××•×¤×¦×™×•× ×œ×™)\n",
    "    'tenant_id': 'YOUR_TENANT_ID'             # (××•×¤×¦×™×•× ×œ×™)\n",
    "}\n",
    "\n",
    "# × ×ª×™×‘ Blob\n",
    "# ×¤×•×¨××˜: az://container-name/path/to/file\n",
    "blob_csv_file_path = 'az://demo/titanic_dataset.csv'\n",
    "\n",
    "# ×§×¨×™××ª ×”×§×•×‘×¥\n",
    "df = pl.read_csv(blob_csv_file_path, storage_options=storage_options)\n",
    "\n",
    "print(\"âœ… ×”× ×ª×•× ×™× × ×§×¨××• ×‘×”×¦×œ×—×” ×-Azure!\")\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ×”×¡×‘×¨ ×¢×œ ××‘× ×” ×”× ×ª×™×‘:\n",
    "\n",
    "```\n",
    "az://container-name/path/to/file.csv\n",
    "â”‚   â”‚                â”‚\n",
    "â”‚   â”‚                â””â”€ × ×ª×™×‘ ×”×§×•×‘×¥ ×‘container\n",
    "â”‚   â””â”€ ×©× ×”-container (×“×•××” ×œ-bucket ×‘-S3)\n",
    "â””â”€ ×¤×¨×•×˜×•×§×•×œ Azure\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"writing-to-azure\"></a>\n",
    "## ğŸ“ ×›×ª×™×‘×” ×œ-Azure Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import adlfs  # Azure DataLake File System\n",
    "\n",
    "# ×™×¦×™×¨×ª ××•×¤×¢ ×©×œ AzureBlobFileSystem\n",
    "fs = adlfs.AzureBlobFileSystem(\n",
    "    account_name='YOUR_ACCOUNT_NAME',\n",
    "    account_key='YOUR_ACCOUNT_KEY'\n",
    ")\n",
    "\n",
    "# × ×ª×™×‘ ×”×™×¢×“\n",
    "blob_parquet_file_path = 'az://demo/titanic_dataset.parquet'\n",
    "\n",
    "# ×›×ª×™×‘×”\n",
    "with fs.open(blob_parquet_file_path, mode='wb') as f:\n",
    "    df.write_parquet(f)\n",
    "\n",
    "print(\"âœ… ×”×§×•×‘×¥ × ×©××¨ ×œ-Azure Blob ×‘×”×¦×œ×—×”!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ ×˜×™×¤×™× ×œ×¢×‘×•×“×” ×¢× Azure:\n",
    "\n",
    "1. **Container Names**: ×—×™×™×‘×™× ×œ×”×™×•×ª ×‘××•×ª×™×•×ª ×§×˜× ×•×ª\n",
    "2. **Access Tiers**: ×‘×—×¨×• ××ª ×”×¨××” ×”××ª××™××” (Hot/Cool/Archive)\n",
    "3. **SAS Tokens**: ××œ×˜×¨× ×˜×™×‘×” ×××•×‘×˜×—×ª ×œ-account keys\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” ×§×¨×™××” ×¢× Scan (Lazy)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×§×¨×™××” ×¢×¦×œ×” (Lazy) ×¢× ××¤×©×¨×•×ª ×œ××•×¤×˜×™××™×–×¦×™×”\n",
    "lf = pl.scan_parquet(blob_parquet_file_path, storage_options=storage_options)\n",
    "\n",
    "# ×‘×™×¦×•×¢ ×¤×¢×•×œ×•×ª ×œ×œ× ×˜×¢×™× ×ª ×›×œ ×”× ×ª×•× ×™×\n",
    "result = lf.head().collect()\n",
    "\n",
    "print(\"âœ… × ×ª×•× ×™× × ×§×¨××• ×‘×¦×•×¨×” ×××•××ª×ª!\")\n",
    "result"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¹ ×©×™××•×© ×‘-PyArrow Dataset ×¢× Azure"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pyarrow.dataset as ds\n",
    "\n",
    "# ×™×¦×™×¨×ª dataset\n",
    "dataset = ds.dataset(\n",
    "    blob_parquet_file_path,\n",
    "    filesystem=fs,\n",
    "    format='parquet'\n",
    ")\n",
    "\n",
    "# ×§×¨×™××” ×¢× ×¡×™× ×•×Ÿ\n",
    "df = (\n",
    "    pl.scan_pyarrow_dataset(dataset)\n",
    "    .filter(pl.col('Age') <= 30)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š × ××¦××• {df.shape[0]:,} ×©×•×¨×•×ª ×¢× ×’×™×œ <= 30\")\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"azure-environment-variables\"></a>\n",
    "## ğŸŒ ×¢×‘×•×“×” ×¢× ××©×ª× ×™ ×¡×‘×™×‘×”\n",
    "\n",
    "×“×¨×š ×××•×‘×˜×—×ª ×™×•×ª×¨ ×œ××—×¡×Ÿ credentials:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# ×”×’×“×¨×ª ××©×ª× ×™ ×¡×‘×™×‘×”\n",
    "os.environ['AZURE_STORAGE_ACCOUNT_NAME'] = 'YOUR_ACCOUNT_NAME'\n",
    "os.environ['AZURE_STORAGE_ACCOUNT_KEY'] = 'YOUR_ACCOUNT_KEY'\n",
    "\n",
    "# ×›×¢×ª Polars ×™×§×¨× ××•×˜×•××˜×™×ª ××”××©×ª× ×™×\n",
    "df = pl.read_csv(blob_csv_file_path, storage_options=storage_options)\n",
    "\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš™ï¸ ×”×’×“×¨×ª ××©×ª× ×™ ×¡×‘×™×‘×” ×‘××¢×¨×›×ª:\n",
    "\n",
    "**Windows (PowerShell):**\n",
    "```powershell\n",
    "$env:AZURE_STORAGE_ACCOUNT_NAME=\"your_account\"\n",
    "$env:AZURE_STORAGE_ACCOUNT_KEY=\"your_key\"\n",
    "```\n",
    "\n",
    "**Linux/Mac:**\n",
    "```bash\n",
    "export AZURE_STORAGE_ACCOUNT_NAME=\"your_account\"\n",
    "export AZURE_STORAGE_ACCOUNT_KEY=\"your_key\"\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"azure-data-lake\"></a>\n",
    "## ğŸï¸ Azure Data Lake Storage (ADLS)\n",
    "\n",
    "ADLS ×”×•× ×©×™×¨×•×ª ××—×¡×•×Ÿ ××ª×§×“× ×™×•×ª×¨, ××•×¤×˜×™××œ×™ ×œ×‘×™×’ ×“××˜×”."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”×—×œ×¤×ª account name ×œ-ADLS\n",
    "storage_options['account_name'] = 'YOUR_ADLS_ACCOUNT_NAME'\n",
    "storage_options['account_key'] = 'YOUR_ADLS_ACCOUNT_KEY'\n",
    "\n",
    "# ×§×¨×™××” ×ADLS (××•×ª×• API ×›××• Blob)\n",
    "df = pl.read_csv(blob_csv_file_path, storage_options=storage_options)\n",
    "\n",
    "print(\"âœ… × ×ª×•× ×™× × ×§×¨××• ×-ADLS!\")\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Blob Storage vs ADLS:\n",
    "\n",
    "| ×ª×›×•× ×” | Blob Storage | ADLS Gen2 |\n",
    "|--------|--------------|------------|\n",
    "| ğŸ¯ ×©×™××•×© | ×§×‘×¦×™× ×›×œ×œ×™×™× | ×‘×™×’ ×“××˜×” |\n",
    "| ğŸ“ ×”×™×¨×¨×›×™×” | ×©×˜×•×—×” | ×¢×¦×™×ª |\n",
    "| ğŸ” ××‘×˜×—×” | ×‘×¡×™×¡×™×ª | ××ª×§×“××ª (ACL) |\n",
    "| ğŸ’° ××—×™×¨ | × ××•×š | ×’×‘×•×” ×™×•×ª×¨ |\n",
    "| âš¡ ×‘×™×¦×•×¢×™× | ×˜×•×‘ | ××¢×•×œ×” |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š ×ª×¨×’×™×œ 2: ×¢×‘×•×“×” ×¢× Azure\n",
    "\n",
    "### ğŸ¯ ××©×™××”:\n",
    "×¦×•×¨ DataFrame ×—×“×© ×¢× ×¨×§ ×”× ×•×¡×¢×™× ×××—×œ×§×” ×¨××©×•× ×” (Pclass=1), ×©××•×¨ ××•×ª×• ×œ-Azure Blob ×‘×¤×•×¨××˜ Parquet."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”×ª×—×œ ×›××Ÿ - ××œ× ××ª ×”×§×•×“\n",
    "\n",
    "# ×©×œ×‘ 1: ×¡× ×Ÿ ××—×œ×§×” ×¨××©×•× ×”\n",
    "# first_class = ...\n",
    "\n",
    "# ×©×œ×‘ 2: ×©××•×¨ ×œ-Azure\n",
    "# output_path = 'az://demo/first_class_passengers.parquet'\n",
    "# ...\n",
    "\n",
    "# ×©×œ×‘ 3: ×§×¨× ×—×–×¨×” ×•×•×•×“×\n",
    "# verification = ...\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"google-cloud-storage\"></a>\n",
    "# 3ï¸âƒ£ Google Cloud Storage (GCS)\n",
    "\n",
    "## ğŸ” ××‘×•×\n",
    "\n",
    "**Google Cloud Storage** ×”×•× ×©×™×¨×•×ª ××—×¡×•×Ÿ ××•×‘×™×™×§×˜×™× ×©×œ Google Cloud Platform.\n",
    "\n",
    "### ×œ××” ×œ×”×©×ª××© ×‘-GCS?\n",
    "- ğŸš€ **×‘×™×¦×•×¢×™× ×’×‘×•×”×™×** - ×¨×©×ª ×”×’×œ×•×‘×œ×™×ª ×©×œ Google\n",
    "- ğŸ”„ **××™× ×˜×’×¨×¦×™×” ×¢× GCP** - BigQuery, Dataflow, ×•×›×•'\n",
    "- ğŸ’ **Storage Classes** - Standard, Nearline, Coldline, Archive\n",
    "- ğŸ” **××‘×˜×—×”** - ×”×¦×¤× ×” ××•×˜×•××˜×™×ª\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reading-from-gcs\"></a>\n",
    "## ğŸ“– ×§×¨×™××” ×-GCS - ×’×™×©×” ×¤×•××‘×™×ª"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# × ×ª×™×‘ GCS\n",
    "# ×¤×•×¨××˜: gs://bucket-name/path/to/file\n",
    "gcs_csv_file_path = 'gs://polars_cookbook_demo_yk/titanic_dataset.csv'\n",
    "\n",
    "# ×§×¨×™××” ×¤×©×•×˜×” ×bucket ×¤×•××‘×™\n",
    "df = pl.read_csv(gcs_csv_file_path)\n",
    "\n",
    "print(\"âœ… ×”× ×ª×•× ×™× × ×§×¨××• ×-GCS!\")\n",
    "print(f\"ğŸ“Š {df.shape[0]:,} ×©×•×¨×•×ª Ã— {df.shape[1]} ×¢××•×“×•×ª\")\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ××‘× ×” × ×ª×™×‘ GCS:\n",
    "\n",
    "```\n",
    "gs://bucket-name/folder/file.csv\n",
    "â”‚   â”‚            â”‚      â”‚\n",
    "â”‚   â”‚            â”‚      â””â”€ ×©× ×”×§×•×‘×¥\n",
    "â”‚   â”‚            â””â”€ ×ª×™×§×™×•×ª (××•×¤×¦×™×•× ×œ×™)\n",
    "â”‚   â””â”€ ×©× ×”-bucket\n",
    "â””â”€ ×¤×¨×•×˜×•×§×•×œ GCS\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gcs-authentication\"></a>\n",
    "## ğŸ” ××™××•×ª ×¢× Service Account\n",
    "\n",
    "×¢×‘×•×¨ buckets ×¤×¨×˜×™×™×, × ×©×ª××© ×‘-Service Account credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ×©×™×˜×” 1: ×§×•×‘×¥ JSON"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# × ×ª×™×‘ ×œ×§×•×‘×¥ credentials (JSON)\n",
    "credentials_file_path = 'YOUR_FILE_NAME.json'\n",
    "\n",
    "# ×”×¢×‘×¨×ª ×”× ×ª×™×‘ ×›-token\n",
    "storage_options = {'token': credentials_file_path}\n",
    "\n",
    "df = pl.read_csv(gcs_csv_file_path, storage_options=storage_options)\n",
    "\n",
    "print(\"âœ… ×§×¨×™××” ×××•××ª×ª ×‘×”×¦×œ×—×”!\")\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ××™×š ×œ×™×¦×•×¨ Service Account?\n",
    "\n",
    "1. ×¢×‘×•×¨ ×œ-[Google Cloud Console](https://console.cloud.google.com/)\n",
    "2. IAM & Admin â†’ Service Accounts\n",
    "3. Create Service Account\n",
    "4. ×”×•×¨×“ ××ª ×§×•×‘×¥ ×”-JSON\n",
    "5. ×©××•×¨ ×‘××§×•× ×××•×‘×˜×—\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ×©×™×˜×” 2: ×˜×¢×™× ×ª credentials ×›-dictionary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "def read_json_into_dict(file_path):\n",
    "    \"\"\"×˜×•×¢×Ÿ ×§×•×‘×¥ JSON ×•××—×–×™×¨ dictionary\"\"\"\n",
    "    try:\n",
    "        with open(file_path) as file:\n",
    "            return json.load(file)\n",
    "    except Exception as err:\n",
    "        print(f\"âŒ ×©×’×™××” ×‘×§×¨×™××ª ×”×§×•×‘×¥: {err}\")\n",
    "        raise err\n",
    "\n",
    "# ×˜×¢×™× ×ª ×”-credentials\n",
    "storage_options = read_json_into_dict(credentials_file_path)\n",
    "\n",
    "# ×©×™××•×©\n",
    "df = pl.read_csv(gcs_csv_file_path, storage_options=storage_options)\n",
    "\n",
    "print(\"âœ… × ×ª×•× ×™× × ×˜×¢× ×• ×¢× credentials dictionary!\")\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"writing-to-gcs\"></a>\n",
    "## ğŸ“ ×›×ª×™×‘×” ×œ-GCS"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import gcsfs\n",
    "\n",
    "# ×™×¦×™×¨×ª GCSFileSystem\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "\n",
    "# × ×ª×™×‘ ×™×¢×“\n",
    "gcs_parquet_file_path = 'gs://polars_cookbook_demo_yk/titanic_dataset.parquet'\n",
    "\n",
    "# ×›×ª×™×‘×”\n",
    "with fs.open(gcs_parquet_file_path, mode='wb') as f:\n",
    "    df.write_parquet(f)\n",
    "\n",
    "print(\"âœ… ×”×§×•×‘×¥ × ×©××¨ ×œ-GCS ×‘×¤×•×¨××˜ Parquet!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ×›×ª×™×‘×” ×¢× ××™××•×ª:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×¦×™×¨×ª filesystem ×¢× credentials\n",
    "fs = gcsfs.GCSFileSystem(token=credentials_file_path)\n",
    "\n",
    "with fs.open(gcs_parquet_file_path, mode='wb') as f:\n",
    "    df.write_parquet(f)\n",
    "\n",
    "print(\"âœ… ×›×ª×™×‘×” ×××•××ª×ª ×”×•×©×œ××”!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¹ PyArrow Dataset ×¢× GCS"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pyarrow.dataset as ds\n",
    "\n",
    "# ×™×¦×™×¨×ª dataset\n",
    "dataset = ds.dataset(\n",
    "    gcs_parquet_file_path,\n",
    "    filesystem=fs,\n",
    "    format='parquet'\n",
    ")\n",
    "\n",
    "# ×§×¨×™××” ×¢× ×¡×™× ×•×Ÿ ×™×¢×™×œ\n",
    "df = (\n",
    "    pl.scan_pyarrow_dataset(dataset)\n",
    "    .filter(pl.col('Age') <= 30)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(f\"ğŸ¯ ×¡×•× ×Ÿ ×œ-{df.shape[0]:,} × ×•×¡×¢×™× ×¦×¢×™×¨×™×\")\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ ×˜×™×¤×™× ×œ-GCS:\n",
    "\n",
    "1. **Bucket Naming**: \n",
    "   - ×©××•×ª ×™×™×—×•×“×™×™× globally\n",
    "   - ×¨×§ ××•×ª×™×•×ª ×§×˜× ×•×ª, ××¡×¤×¨×™×, ××§×¤×™×\n",
    "\n",
    "2. **Storage Classes**:\n",
    "   - Standard: ×’×™×©×” ×ª×›×•×¤×”\n",
    "   - Nearline: ×¤×¢× ×‘×—×•×“×©\n",
    "   - Coldline: ×¤×¢× ×‘×¨×‘×¢×•×Ÿ\n",
    "   - Archive: ×¤×¢× ×‘×©× ×”\n",
    "\n",
    "3. **Lifecycle Policies**: ×”×¢×‘×¨ ×§×‘×¦×™× ×™×©× ×™× ××•×˜×•××˜×™×ª ×œ××—×œ×§×•×ª ×–×•×œ×•×ª ×™×•×ª×¨\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š ×ª×¨×’×™×œ 3: ×¢×‘×•×“×” ×¢× GCS\n",
    "\n",
    "### ğŸ¯ ××©×™××”:\n",
    "1. ×§×¨× × ×ª×•× ×™× ×-GCS\n",
    "2. ×—×©×‘ ××ª ×©×™×¢×•×¨ ×”×”×™×©×¨×“×•×ª ×œ×¤×™ ××—×œ×§×” (Pclass)\n",
    "3. ×©××•×¨ ××ª ×”×ª×•×¦××” ×œ-GCS ×›-CSV ×—×“×©"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”×ª×—×œ ×›××Ÿ\n",
    "\n",
    "# ×©×œ×‘ 1: ×§×¨× × ×ª×•× ×™×\n",
    "# ...\n",
    "\n",
    "# ×©×œ×‘ 2: ×—×©×‘ ×©×™×¢×•×¨ ×”×™×©×¨×“×•×ª\n",
    "# survival_rate = ...\n",
    "\n",
    "# ×©×œ×‘ 3: ×©××•×¨ ×ª×•×¦××”\n",
    "# output_path = 'gs://your-bucket/survival_by_class.csv'\n",
    "# ..."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"bigquery\"></a>\n",
    "# 4ï¸âƒ£ BigQuery - ××¡×“ × ×ª×•× ×™× ×¢× ×Ÿ ×©×œ Google\n",
    "\n",
    "## ğŸ” ××‘×•×\n",
    "\n",
    "**BigQuery** ×”×•× ××¡×“ × ×ª×•× ×™× serverless ×•×××¡×™×‘×™ ×‘××™×•×—×“ ×©×œ Google.\n",
    "\n",
    "### ×œ××” BigQuery?\n",
    "- âš¡ **××”×™×¨×•×ª** - ×©××™×œ×ª×•×ª ×¢×œ ×˜×¨×”-×‘×™×™×˜×™× ×‘×©× ×™×•×ª\n",
    "- ğŸ“Š **SQL** - ×©×¤×ª ×©××™×œ×ª×•×ª ××•×›×¨×ª\n",
    "- ğŸ’° **×ª××—×•×¨** - ×ª×©×œ×•× ×œ×¤×™ ×©××™×œ×ª×•×ª\n",
    "- ğŸ”„ **××™× ×˜×’×¨×¦×™×”** - ×¢× ×›×œ ×©×™×¨×•×ª×™ GCP\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reading-from-bigquery\"></a>\n",
    "## ğŸ“– ×§×¨×™××” ×-BigQuery\n",
    "\n",
    "### ×©×™×˜×” 1: ConnectorX (××•××œ×¥)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×¤×¨×˜×™ ×”×—×™×‘×•×¨\n",
    "project = 'sandbox-366819'\n",
    "dataset = 'polars_cookbook_demo_yk'\n",
    "table = 'titanic_dataset'\n",
    "\n",
    "# ×©××™×œ×ª×ª SQL\n",
    "query = f'''\n",
    "    SELECT *\n",
    "    FROM `{project}.{dataset}.{table}`\n",
    "    LIMIT 1000\n",
    "'''\n",
    "\n",
    "# × ×ª×™×‘ ×œ-credentials\n",
    "credentials_file_path = 'YOUR_CREDENTIALS_FILE_PATH.json'\n",
    "\n",
    "# ×‘× ×™×™×ª URI\n",
    "uri = f'bigquery://{credentials_file_path}'\n",
    "\n",
    "# ×§×¨×™××”\n",
    "df = pl.read_database_uri(\n",
    "    query,\n",
    "    uri,\n",
    "    engine='connectorx'  # ×× ×•×¢ ×—×™×‘×•×¨ ××”×™×¨\n",
    ")\n",
    "\n",
    "print(\"âœ… × ×ª×•× ×™× × ×§×¨××• ×-BigQuery!\")\n",
    "print(f\"ğŸ“Š {df.shape[0]:,} ×©×•×¨×•×ª × ×˜×¢× ×•\")\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ×”×¡×‘×¨ ×¢×œ ×”×¤×•×¨××˜:\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM `project.dataset.table`\n",
    "      â”‚       â”‚       â”‚\n",
    "      â”‚       â”‚       â””â”€ ×©× ×”×˜×‘×œ×”\n",
    "      â”‚       â””â”€ ×©× ×”-dataset\n",
    "      â””â”€ ID ×”×¤×¨×•×™×§×˜\n",
    "```\n",
    "\n",
    "**×©×™××• ×œ×‘**: ×©×™××•×© ×‘-backticks (\\`) ×¡×‘×™×‘ ×”×©× ×”××œ×!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ×©×™×˜×” 2: BigQuery Client"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# ×™×¦×™×¨×ª client\n",
    "client = bigquery.Client.from_service_account_json(credentials_file_path)\n",
    "\n",
    "# ×”×¨×¦×ª ×©××™×œ×ª×”\n",
    "query_job = client.query(query)\n",
    "rows = query_job.result()\n",
    "\n",
    "# ×”××¨×” ×œ-Polars ×“×¨×š Arrow\n",
    "df = pl.from_arrow(rows.to_arrow())\n",
    "\n",
    "print(\"âœ… × ×ª×•× ×™× × ×˜×¢× ×• ×“×¨×š BigQuery Client!\")\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ ×™×ª×¨×•× ×•×ª Arrow:\n",
    "\n",
    "- **Zero-copy** - ×œ×œ× ×”×¢×ª×§×ª ×–×™×›×¨×•×Ÿ\n",
    "- **××”×™×¨** - 10-100x ××”×™×¨ ×JSON\n",
    "- **×˜×™×¤×•×¡×™×** - ×©×•××¨ ××ª ×›×œ ×”××™×“×¢ ×¢×œ ×”× ×ª×•× ×™×\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ×©×™×˜×” 3: read_database"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×©×™××•×© ×‘-connection object\n",
    "# (client ×›×‘×¨ ×”×•×’×“×¨ ××¢×œ×”)\n",
    "\n",
    "df = pl.read_database(query, connection=client)\n",
    "\n",
    "print(\"âœ… ×§×¨×™××” ×™×©×™×¨×” ××”×—×™×‘×•×¨!\")\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"writing-to-bigquery\"></a>\n",
    "## ğŸ“ ×›×ª×™×‘×” ×œ-BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import io\n",
    "\n",
    "# ×˜×‘×œ×ª ×™×¢×“\n",
    "destination = f'{project}.{dataset}.titanic_dataset_v2'\n",
    "\n",
    "# ×”××¨×” ×œ-CSV ×‘×–×™×›×¨×•×Ÿ\n",
    "with io.BytesIO() as stream:\n",
    "    # ×›×ª×™×‘×” ×œ×–×™×›×¨×•×Ÿ\n",
    "    df.write_csv(stream)\n",
    "    \n",
    "    # ×—×–×¨×” ×œ×”×ª×—×œ×”\n",
    "    stream.seek(0)\n",
    "    \n",
    "    # ×”×¢×œ××” ×œ-BigQuery\n",
    "    job = client.load_table_from_file(\n",
    "        stream,\n",
    "        destination=destination,\n",
    "        project=project,\n",
    "        job_config=bigquery.LoadJobConfig(\n",
    "            autodetect=True,  # ×–×™×”×•×™ ××•×˜×•××˜×™ ×©×œ ×¡×›××”\n",
    "            source_format=bigquery.SourceFormat.CSV,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "# ×”××ª× ×” ×œ×¡×™×•×\n",
    "job.result()\n",
    "\n",
    "print(f\"âœ… {df.shape[0]:,} ×©×•×¨×•×ª × ×©××¨×• ×œ-BigQuery!\")\n",
    "print(f\"ğŸ“ ×˜×‘×œ×”: {destination}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ×”×¡×‘×¨ ×¢×œ ×”×ª×”×œ×™×š:\n",
    "\n",
    "1. **BytesIO**: \"×§×•×‘×¥\" ×‘×–×™×›×¨×•×Ÿ (×œ× ×›×•×ª×‘ ×œ×“×™×¡×§)\n",
    "2. **seek(0)**: ×—×–×¨×” ×œ×”×ª×—×œ×ª ×”-stream\n",
    "3. **autodetect**: BigQuery ××–×”×” ××ª ×˜×™×¤×•×¡×™ ×”×¢××•×“×•×ª\n",
    "4. **job.result()**: ×××ª×™×Ÿ ×œ×¡×™×•× ×”×¢×œ××”\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’° ×˜×™×¤×™× ×œ×—×™×¡×›×•×Ÿ ×‘-BigQuery:\n",
    "\n",
    "1. **×”×©×ª××©×• ×‘-LIMIT**: ××œ ×ª×©×œ×¤×• ×™×•×ª×¨ ×××” ×©×¦×¨×™×š\n",
    "   ```sql\n",
    "   SELECT * FROM table LIMIT 1000\n",
    "   ```\n",
    "\n",
    "2. **×‘×—×¨×• ×¢××•×“×•×ª ×¡×¤×¦×™×¤×™×•×ª**: \n",
    "   ```sql\n",
    "   SELECT name, age FROM table  -- âœ… ×˜×•×‘\n",
    "   SELECT * FROM table          -- âŒ ×™×§×¨\n",
    "   ```\n",
    "\n",
    "3. **×¡× × ×• ××•×§×“×**: ×”×©×ª××©×• ×‘-WHERE\n",
    "   ```sql\n",
    "   SELECT * FROM table WHERE date >= '2024-01-01'\n",
    "   ```\n",
    "\n",
    "4. **Partitioned Tables**: ×—×œ×§×• ×œ×¤×™ ×ª××¨×™×š\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š ×ª×¨×’×™×œ 4: BigQuery\n",
    "\n",
    "### ğŸ¯ ××©×™××”:\n",
    "×›×ª×•×‘ ×©××™×œ×ª×” ×©××—×–×™×¨×” ××ª 10 ×”×›×¨×˜×™×¡×™× ×”×™×§×¨×™× ×‘×™×•×ª×¨, ×§×¨× ×œ-Polars, ×•×”×¦×’."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×›×ª×•×‘ ×©××™×œ×ª×”\n",
    "expensive_tickets_query = \"\"\"\n",
    "-- ×”×©×œ× ××ª ×”×©××™×œ×ª×”\n",
    "SELECT \n",
    "    ...\n",
    "\"\"\"\n",
    "\n",
    "# ×§×¨× × ×ª×•× ×™×\n",
    "# ...\n",
    "\n",
    "# ×”×¦×’ ×ª×•×¦××•×ª\n",
    "# ..."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"snowflake\"></a>\n",
    "# 5ï¸âƒ£ Snowflake - ××¡×“ ×”× ×ª×•× ×™× ×”×¢× × ×™\n",
    "\n",
    "## ğŸ” ××‘×•×\n",
    "\n",
    "**Snowflake** ×”×•× ×¤×œ×˜×¤×•×¨××ª data warehouse ××•×“×¨× ×™×ª ×”××‘×•×¡×¡×ª ×›×•×œ×” ×¢×œ ×”×¢× ×Ÿ.\n",
    "\n",
    "### ×œ××” Snowflake?\n",
    "- â„ï¸ **××¨×›×™×˜×§×˜×•×¨×” ×™×™×—×•×“×™×ª** - ×”×¤×¨×“×” ×‘×™×Ÿ ×—×™×©×•×‘ ×•××—×¡×•×Ÿ\n",
    "- ğŸ”„ **×©×™×ª×•×£ × ×ª×•× ×™×** - Data Sharing ×‘×™×Ÿ ××¨×’×•× ×™×\n",
    "- ğŸ“Š **×‘×™×¦×•×¢×™×** - ××•×¤×˜×™××™×–×¦×™×” ××•×˜×•××˜×™×ª\n",
    "- ğŸ’¼ **Enterprise** - ×¤×ª×¨×•×Ÿ ×œ××¨×’×•× ×™× ×’×“×•×œ×™×\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"connecting-to-snowflake\"></a>\n",
    "## ğŸ”Œ ×—×™×‘×•×¨ ×œ-Snowflake\n",
    "\n",
    "### ×”×›× ×ª ×”×¤×¨××˜×¨×™×:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×¤×¨×˜×™ ×—×™×‘×•×¨\n",
    "username = 'YOUR_USERNAME'\n",
    "password = 'YOUR_PASS'\n",
    "account = 'your-account.region'  # ×œ×“×•×’××”: abc12345.us-east-1\n",
    "database = 'POLARS_COOKBOOK_DEMO_YK'\n",
    "warehouse = 'COMPUTE_WH'          # Warehouse ×œ×—×™×©×•×‘\n",
    "role = 'ACCOUNTADMIN'             # ×ª×¤×§×™×“ ×”××©×ª××©\n",
    "schema = 'SANDBOX'\n",
    "table = 'TITANIC_DATASET'\n",
    "\n",
    "print(\"âœ… ×¤×¨××˜×¨×™ ×—×™×‘×•×¨ ×”×•×’×“×¨×•!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ×”×¡×‘×¨ ×¢×œ ××•×©×’×™×:\n",
    "\n",
    "- **Account**: ××–×”×” ×™×™×—×•×“×™ ×©×œ ×—×©×‘×•×Ÿ Snowflake ×©×œ×›×\n",
    "- **Warehouse**: ××©××‘ ×—×™×©×•×‘ (×›××• CPU/RAM)\n",
    "- **Database**: ×§×•×œ×§×¦×™×” ×©×œ schemas\n",
    "- **Schema**: ×§×•×œ×§×¦×™×” ×©×œ ×˜×‘×œ××•×ª\n",
    "- **Role**: ×”×¨×©××•×ª ×’×™×©×”\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reading-from-snowflake\"></a>\n",
    "## ğŸ“– ×§×¨×™××” ×-Snowflake\n",
    "\n",
    "### ×©×™×˜×” 1: ADBC (××•××œ×¥ - ×”×›×™ ××”×™×¨)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×©××™×œ×ª×”\n",
    "query = f'SELECT * FROM {table}'\n",
    "\n",
    "# ×‘× ×™×™×ª URI\n",
    "uri = (\n",
    "    f'snowflake://{username}:{password}@{account}/{database}/{schema}'\n",
    "    f'?warehouse={warehouse}&role={role}'\n",
    ")\n",
    "\n",
    "# ×§×¨×™××” ×¢× ADBC\n",
    "df = pl.read_database_uri(\n",
    "    query,\n",
    "    uri,\n",
    "    engine='adbc'  # Arrow Database Connectivity - ××”×™×¨ ×××•×“!\n",
    ")\n",
    "\n",
    "print(\"âœ… × ×ª×•× ×™× × ×§×¨××• ×-Snowflake!\")\n",
    "print(f\"ğŸ“Š {df.shape[0]:,} ×©×•×¨×•×ª Ã— {df.shape[1]} ×¢××•×“×•×ª\")\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš¡ ×œ××” ADBC?\n",
    "\n",
    "**ADBC (Arrow Database Connectivity)** ×”×•× ×ª×§×Ÿ ×—×“×©:\n",
    "\n",
    "- ğŸš€ **××”×™×¨**: 10-100x ××”×™×¨ ×ODBC/JDBC\n",
    "- ğŸ’¾ **×™×¢×™×œ**: ×”×¢×‘×¨×ª × ×ª×•× ×™× ×œ×œ× ×”××¨×•×ª\n",
    "- ğŸ¯ **××•×“×¨× ×™**: ×ª×•×›× ×Ÿ ×‘××™×•×—×“ ×¢×‘×•×¨ × ×ª×•× ×™× ×‘×¡×§××œ×” ×’×“×•×œ×”\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"alternative-snowflake-methods\"></a>\n",
    "### ×©×™×˜×” 2: Snowflake Connector"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import snowflake.connector\n",
    "\n",
    "# ×™×¦×™×¨×ª ×—×™×‘×•×¨\n",
    "conn = snowflake.connector.connect(\n",
    "    user=username,\n",
    "    password=password,\n",
    "    account=account,\n",
    "    warehouse=warehouse,\n",
    "    database=database,\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "# ×§×¨×™××” ×“×¨×š Arrow\n",
    "df = pl.from_arrow(\n",
    "    conn.cursor()\n",
    "    .execute(query)\n",
    "    .fetch_arrow_all()\n",
    ")\n",
    "\n",
    "print(\"âœ… × ×ª×•× ×™× × ×§×¨××• ×“×¨×š Snowflake Connector!\")\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ×©×™×˜×” 3: read_database"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×©×™××•×© ×™×©×™×¨ ×‘×—×™×‘×•×¨\n",
    "df = pl.read_database(query, connection=conn)\n",
    "\n",
    "print(\"âœ… ×§×¨×™××” ×¢× read_database!\")\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š ×”×©×•×•××ª ×©×™×˜×•×ª ×§×¨×™××”:\n",
    "\n",
    "| ×©×™×˜×” | ××”×™×¨×•×ª | ×§×œ×•×ª ×©×™××•×© | ××•××œ×¥ ×œ |\n",
    "|------|--------|------------|--------|\n",
    "| ADBC | âš¡âš¡âš¡ | â­â­â­ | × ×ª×•× ×™× ×’×“×•×œ×™× |\n",
    "| Connector + Arrow | âš¡âš¡ | â­â­ | ×©×™××•×© ×›×œ×œ×™ |\n",
    "| read_database | âš¡ | â­â­â­ | ×¤×©×˜×•×ª |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ ×˜×™×¤×™× ×œ×¢×‘×•×“×” ×¢× Snowflake:\n",
    "\n",
    "### 1. × ×™×”×•×œ Warehouses:\n",
    "```sql\n",
    "-- ×”×©×”×” warehouse ×›×©×œ× ×‘×©×™××•×©\n",
    "ALTER WAREHOUSE my_wh SUSPEND;\n",
    "\n",
    "-- ×©× ×” ×’×•×“×œ ×œ×¤×™ ×¦×•×¨×š\n",
    "ALTER WAREHOUSE my_wh SET WAREHOUSE_SIZE = 'LARGE';\n",
    "```\n",
    "\n",
    "### 2. ××•×¤×˜×™××™×–×¦×™×”:\n",
    "- ×”×©×ª××©×• ×‘-**Clustering Keys** ×œ×˜×‘×œ××•×ª ×’×“×•×œ×•×ª\n",
    "- × ×¦×œ×• **Result Caching** - ×ª×•×¦××•×ª × ×©××¨×•×ª ×œ-24 ×©×¢×•×ª\n",
    "- ×”×©×ª××©×• ×‘-**Materialized Views** ×œ×—×™×©×•×‘×™× ×—×•×–×¨×™×\n",
    "\n",
    "### 3. ××‘×˜×—×”:\n",
    "```python\n",
    "# ××œ ×ª×©××¨×• ×¡×™×¡×××•×ª ×‘×§×•×“!\n",
    "import os\n",
    "password = os.environ.get('SNOWFLAKE_PASSWORD')\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š ×ª×¨×’×™×œ 5: Snowflake\n",
    "\n",
    "### ğŸ¯ ××©×™××”:\n",
    "1. ×”×ª×—×‘×¨ ×œ-Snowflake\n",
    "2. ×—×©×‘ ×××•×¦×¢ ×’×™×œ ×œ×¤×™ × ××œ ×¢×œ×™×™×” (Embarked)\n",
    "3. ×¡× ×Ÿ ×¨×§ × ××œ×™× ×¢× 100+ × ×•×¡×¢×™×"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×›×ª×•×‘ ×©××™×œ×ª×” ×‘-SQL\n",
    "port_analysis_query = \"\"\"\n",
    "SELECT \n",
    "    ...\n",
    "-- ×”×©×œ× ××ª ×”×©××™×œ×ª×”\n",
    "\"\"\"\n",
    "\n",
    "# ×”×¨×¥ ×‘-Snowflake\n",
    "# ...\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"exercises\"></a>\n",
    "# ğŸ“Š ×ª×¨×’×™×œ×™× ××ª×§×“××™×\n",
    "\n",
    "## ×ª×¨×’×™×œ 6: ×”×©×•×•××” ×‘×™×Ÿ ×¤×œ×˜×¤×•×¨××•×ª\n",
    "\n",
    "### ğŸ¯ ××©×™××”:\n",
    "×§×¨× ××ª ××•×ª×• dataset ××›×œ ×”×¤×œ×˜×¤×•×¨××•×ª (S3, Azure, GCS) ×•×”×©×•×•×” ×‘×™×¦×•×¢×™×."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "def benchmark_read(platform_name, read_function):\n",
    "    \"\"\"××•×“×“ ×–××Ÿ ×§×¨×™××”\"\"\"\n",
    "    start = time.time()\n",
    "    df = read_function()\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"{platform_name}: {elapsed:.2f} ×©× ×™×•×ª - {df.shape[0]:,} ×©×•×¨×•×ª\")\n",
    "    return elapsed\n",
    "\n",
    "# ×”×©×œ× ××ª ×”×¤×•× ×§×¦×™×•×ª\n",
    "# benchmark_read(\"S3\", lambda: pl.read_csv(...))\n",
    "# benchmark_read(\"Azure\", lambda: ...)\n",
    "# benchmark_read(\"GCS\", lambda: ...)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ×ª×¨×’×™×œ 7: Pipeline ××œ×\n",
    "\n",
    "### ğŸ¯ ××©×™××”:\n",
    "×¦×•×¨ pipeline ×©××¢×‘×™×¨ × ×ª×•× ×™× ×‘×™×Ÿ ×¤×œ×˜×¤×•×¨××•×ª:\n",
    "\n",
    "1. ×§×¨× ×-S3\n",
    "2. × ×§×” ×•×¢×‘×“ × ×ª×•× ×™× ×‘-Polars\n",
    "3. ×©××•×¨ ×œ-BigQuery\n",
    "4. ×‘×“×•×§ ××ª ×”×ª×•×¦××”"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def data_pipeline():\n",
    "    \"\"\"Pipeline ××œ× ×œ×”×¢×‘×¨×ª × ×ª×•× ×™×\"\"\"\n",
    "    \n",
    "    # ×©×œ×‘ 1: ×§×¨×™××” ×-S3\n",
    "    print(\"ğŸ“¥ ×§×•×¨× ×-S3...\")\n",
    "    # df = ...\n",
    "    \n",
    "    # ×©×œ×‘ 2: ×¢×™×‘×•×“\n",
    "    print(\"âš™ï¸ ××¢×‘×“ × ×ª×•× ×™×...\")\n",
    "    # df_processed = df.filter(...).select(...)\n",
    "    \n",
    "    # ×©×œ×‘ 3: ×›×ª×™×‘×” ×œ-BigQuery\n",
    "    print(\"ğŸ“¤ ×©×•××¨ ×œ-BigQuery...\")\n",
    "    # ...\n",
    "    \n",
    "    # ×©×œ×‘ 4: ××™××•×ª\n",
    "    print(\"âœ… ××××ª...\")\n",
    "    # verification = ...\n",
    "    \n",
    "    print(\"ğŸ‰ Pipeline ×”×•×©×œ×!\")\n",
    "    return verification\n",
    "\n",
    "# ×”×¨×¥ pipeline\n",
    "# result = data_pipeline()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"summary\"></a>\n",
    "# ğŸ“ ×¡×™×›×•× ×•××©××‘×™×\n",
    "\n",
    "## ğŸ“ ×¡×™×›×•× ×”×¤×¨×§\n",
    "\n",
    "### ××” ×œ××“× ×•:\n",
    "\n",
    "âœ… **Amazon S3**:\n",
    "- ×§×¨×™××” ×•×›×ª×™×‘×” ×¤×©×•×˜×” ×¢× `s3://`\n",
    "- ××™××•×ª ×¢× AWS credentials\n",
    "- ×©×™××•×© ×‘-s3fs ×•-PyArrow\n",
    "\n",
    "âœ… **Azure Blob Storage**:\n",
    "- ×¢×‘×•×“×” ×¢× `az://` protocol\n",
    "- ×©×™××•×© ×‘-adlfs\n",
    "- ×”×‘×“×œ×™× ×‘×™×Ÿ Blob ×œ-ADLS\n",
    "\n",
    "âœ… **Google Cloud Storage**:\n",
    "- ×§×¨×™××” ×¢× `gs://`\n",
    "- ××™××•×ª ×¢× Service Account\n",
    "- ×©×™××•×© ×‘-gcsfs\n",
    "\n",
    "âœ… **BigQuery**:\n",
    "- ×©××™×œ×ª×•×ª SQL\n",
    "- ×§×¨×™××” ×¢× ConnectorX/Arrow\n",
    "- ×”×¢×œ××” ×™×©×™×¨×”\n",
    "\n",
    "âœ… **Snowflake**:\n",
    "- ×—×™×‘×•×¨ ×¢× ADBC\n",
    "- ×©×™×˜×•×ª ×§×¨×™××” ×©×•× ×•×ª\n",
    "- ××•×¤×˜×™××™×–×¦×™×” ×•×—×™×¡×›×•×Ÿ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ××ª×™ ×œ×”×©×ª××© ×‘××”?\n",
    "\n",
    "### ×¤×œ×˜×¤×•×¨××ª ××—×¡×•×Ÿ:\n",
    "\n",
    "| ×¤×œ×˜×¤×•×¨××” | ××©×ª××© ×‘ | ×™×ª×¨×•× ×•×ª | ×—×¡×¨×•× ×•×ª |\n",
    "|-----------|---------|----------|----------|\n",
    "| **S3** | AWS | ××‘×•×’×¨, ×™×¦×™×‘ | ××•×¨×›×‘ ×œ× ×™×”×•×œ |\n",
    "| **Azure Blob** | Microsoft | ××™× ×˜×’×¨×¦×™×” | ××—×™×¨ |\n",
    "| **GCS** | Google | ××”×™×¨, ×¤×©×•×˜ | ×¤×—×•×ª ××•×¤×¦×™×•×ª |\n",
    "\n",
    "### ××¡×“ × ×ª×•× ×™×:\n",
    "\n",
    "| ××¡×“ × ×ª×•× ×™× | ××©×ª××© ×‘ | ×™×ª×¨×•× ×•×ª | ×—×¡×¨×•× ×•×ª |\n",
    "|------------|---------|----------|----------|\n",
    "| **BigQuery** | Google | ××”×™×¨ ×××•×“ | ××—×™×¨ ×œ×©××™×œ×ª×•×ª |\n",
    "| **Snowflake** | Multi-cloud | ×’××™×© | ×™×§×¨ |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ Best Practices\n",
    "\n",
    "### 1. ××‘×˜×—×”:\n",
    "```python\n",
    "# âœ… ×˜×•×‘ - ××©×ª× ×™ ×¡×‘×™×‘×”\n",
    "import os\n",
    "api_key = os.environ.get('API_KEY')\n",
    "\n",
    "# âŒ ×¨×¢ - ×‘×§×•×“\n",
    "api_key = 'my-secret-key-12345'\n",
    "```\n",
    "\n",
    "### 2. ×‘×™×¦×•×¢×™×:\n",
    "- ×”×©×ª××©×• ×‘-`scan_*` ×‘××§×•× `read_*` ×œ× ×ª×•× ×™× ×’×“×•×œ×™×\n",
    "- ×¡× × ×• ××•×§×“× ×›×›×œ ×”××¤×©×¨\n",
    "- ×”×©×ª××©×• ×‘-Parquet ×‘××§×•× CSV\n",
    "\n",
    "### 3. ×¢×œ×•×™×•×ª:\n",
    "- ×§×¨××• ×¨×§ ××” ×©×¦×¨×™×š\n",
    "- ×”×©×ª××©×• ×‘-lifecycle policies\n",
    "- ×¡×’×¨×• warehouses/connections ×›×©×œ× ×‘×©×™××•×©\n",
    "\n",
    "### 4. ×ª×—×–×•×§×”:\n",
    "```python\n",
    "# ×”×©×ª××©×• ×‘-context managers\n",
    "with fs.open(path, 'wb') as f:\n",
    "    df.write_parquet(f)\n",
    "# ×”×§×•×‘×¥ × ×¡×’×¨ ××•×˜×•××˜×™×ª\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ××©××‘×™× × ×•×¡×¤×™×\n",
    "\n",
    "### ×ª×™×¢×•×“ ×¨×©××™:\n",
    "- [Polars Docs - Cloud Storage](https://pola-rs.github.io/polars/)\n",
    "- [AWS S3 Documentation](https://docs.aws.amazon.com/s3/)\n",
    "- [Azure Blob Docs](https://docs.microsoft.com/azure/storage/blobs/)\n",
    "- [Google Cloud Storage](https://cloud.google.com/storage/docs)\n",
    "- [BigQuery Docs](https://cloud.google.com/bigquery/docs)\n",
    "- [Snowflake Docs](https://docs.snowflake.com/)\n",
    "\n",
    "### ×›×œ×™×:\n",
    "- [s3fs](https://s3fs.readthedocs.io/)\n",
    "- [adlfs](https://github.com/fsspec/adlfs)\n",
    "- [gcsfs](https://gcsfs.readthedocs.io/)\n",
    "- [PyArrow](https://arrow.apache.org/docs/python/)\n",
    "- [ConnectorX](https://github.com/sfu-db/connector-x)\n",
    "\n",
    "### ×§×”×™×œ×”:\n",
    "- [Polars Discord](https://discord.gg/4UfP5cfBE7)\n",
    "- [Stack Overflow - Polars](https://stackoverflow.com/questions/tagged/python-polars)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ ×¡×™×•×\n",
    "\n",
    "**×›×œ ×”×›×‘×•×“!** ğŸŠ ×¡×™×™××ª ××ª ×”××“×¨×™×š ×”××§×™×£ ×œ×¢×‘×•×“×” ×¢× ××§×•×¨×•×ª × ×ª×•× ×™× ×‘×¢× ×Ÿ.\n",
    "\n",
    "### ××” ×”×œ××”?\n",
    "\n",
    "1. ğŸ’ª **×ª×¨×’×•×œ**: × ×¡×” ××ª ×”×§×•×“ ×¢× ×”× ×ª×•× ×™× ×©×œ×š\n",
    "2. ğŸ”¨ **×¤×¨×•×™×§×˜**: ×‘× ×” pipeline ×××™×ª×™\n",
    "3. ğŸ“– **×œ××“ ×¢×•×“**: ×—×§×•×¨ × ×•×©××™× ××ª×§×“××™×\n",
    "4. ğŸ¤ **×©×ª×£**: ×¢×–×•×¨ ×œ××—×¨×™× ×‘×§×”×™×œ×”\n",
    "\n",
    "### ×–×•×›×¨:\n",
    "- âš¡ **×”×ª×—×œ ×§×˜×Ÿ** - ××œ ×ª× ×¡×” ×”×›×œ ×‘×‘×ª ××—×ª\n",
    "- ğŸ” **× ×¡×”** - × ×™×¡×•×™ ×•×˜×¢×™×™×” ×”× ×—×œ×§ ××”×œ××™×“×”\n",
    "- ğŸ’¬ **×©××œ** - ×”×§×”×™×œ×” ×›××Ÿ ×œ×¢×–×•×¨\n",
    "- ğŸ¯ **×”×ª××§×“** - ×‘×—×¨ ×¤×œ×˜×¤×•×¨××” ××—×ª ×œ×œ××•×“ ×œ×¢×•××§\n",
    "\n",
    "**×”××“×¨×™×š ×”××œ× ×©×œ×š ×œ-Polars ×•×œ×¢× ×Ÿ ××•×›×Ÿ!** ğŸš€\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: center; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 10px; color: white;\">\n",
    "    <h2>ğŸ»â€â„ï¸ ×”××©×š ×œ×œ××•×“ ×•×œ×”×ª×¤×ª×—! ğŸ»â€â„ï¸</h2>\n",
    "    <p>×¢× Polars, ×”×¢× ×Ÿ ×”×•× ×”×’×‘×•×œ!</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
