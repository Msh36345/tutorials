# ğŸš€ ××“×¨×™×š ××”×™×¨ - Polars ×¢× ××§×•×¨×•×ª Cloud

> **Quick Reference Guide** ×œ×¢×‘×•×“×” ×¢× Polars ×•×¤×œ×˜×¤×•×¨××•×ª ×¢× ×Ÿ  
> ×’×¨×¡×”: 2024 | ×ª×•×× ×œ-Polars 0.20+

---

## ğŸ“‹ ×ª×•×›×Ÿ ×¢× ×™×™× ×™×

- [×”×ª×§× ×” ××”×™×¨×”](#×”×ª×§× ×”)
- [Amazon S3](#s3)
- [Azure Blob Storage](#azure)
- [Google Cloud Storage](#gcs)
- [BigQuery](#bigquery)
- [Snowflake](#snowflake)
- [×˜×™×¤×™× ×•×“×™×‘××’](#tips)
- [×˜×‘×œ×ª ×”×©×•×•××”](#comparison)

---

## âš¡ ×”×ª×§× ×” ××”×™×¨×” {#×”×ª×§× ×”}

```bash
# ×—×‘×™×œ×•×ª ×‘×¡×™×¡
pip install polars pyarrow

# ×¢×‘×•×¨ S3
pip install s3fs

# ×¢×‘×•×¨ Azure
pip install adlfs

# ×¢×‘×•×¨ GCS
pip install gcsfs google-cloud-bigquery

# ×¢×‘×•×¨ Snowflake
pip install snowflake-connector-python adbc-driver-snowflake

# ×¢×‘×•×¨ ×§×¨×™××” ×××¡×“×™ × ×ª×•× ×™×
pip install connectorx
```

---

## ğŸª£ Amazon S3 {#s3}

### ×§×¨×™××” (×§×•×‘×¥ ×¤×•××‘×™)
```python
import polars as pl

# CSV
df = pl.read_csv('s3://bucket-name/file.csv')

# Parquet
df = pl.read_parquet('s3://bucket-name/file.parquet')

# Lazy (××•××œ×¥ ×œ×§×‘×¦×™× ×’×“×•×œ×™×)
lf = pl.scan_parquet('s3://bucket-name/file.parquet')
```

### ×§×¨×™××” ×¢× ××™××•×ª
```python
storage_options = {
    'aws_access_key_id': 'YOUR_KEY',
    'aws_secret_access_key': 'YOUR_SECRET',
    'aws_region': 'us-east-1'
}

df = pl.read_csv('s3://bucket/file.csv', 
                 storage_options=storage_options)
```

### ×›×ª×™×‘×”
```python
import s3fs

fs = s3fs.S3FileSystem(
    key='YOUR_KEY',
    secret='YOUR_SECRET'
)

with fs.open('s3://bucket/output.parquet', 'wb') as f:
    df.write_parquet(f)
```

### ×§×¨×™××” ×¢× ×¡×™× ×•×Ÿ (PyArrow)
```python
import pyarrow.dataset as ds

dataset = ds.dataset('s3://bucket/file.parquet', 
                     format='parquet', filesystem=fs)

df = (
    pl.scan_pyarrow_dataset(dataset)
    .filter(pl.col('age') > 30)
    .collect()
)
```

**ğŸ“Œ ×¤×•×¨××˜ × ×ª×™×‘:** `s3://bucket-name/path/to/file.ext`

---

## â˜ï¸ Azure Blob Storage {#azure}

### ×§×¨×™××”
```python
storage_options = {
    'account_name': 'YOUR_ACCOUNT',
    'access_key': 'YOUR_KEY'
}

df = pl.read_csv('az://container/file.csv', 
                 storage_options=storage_options)
```

### ×›×ª×™×‘×”
```python
import adlfs

fs = adlfs.AzureBlobFileSystem(
    account_name='YOUR_ACCOUNT',
    account_key='YOUR_KEY'
)

with fs.open('az://container/output.parquet', 'wb') as f:
    df.write_parquet(f)
```

### ××©×ª× ×™ ×¡×‘×™×‘×”
```python
import os

os.environ['AZURE_STORAGE_ACCOUNT_NAME'] = 'YOUR_ACCOUNT'
os.environ['AZURE_STORAGE_ACCOUNT_KEY'] = 'YOUR_KEY'

# ×¢×›×©×™×• ×™×¢×‘×•×“ ×‘×œ×™ storage_options
df = pl.read_csv('az://container/file.csv')
```

### Azure Data Lake (ADLS)
```python
# ××•×ª×• API - ×¨×§ ×©× ×” ××ª account_name
storage_options['account_name'] = 'YOUR_ADLS_ACCOUNT'
```

**ğŸ“Œ ×¤×•×¨××˜ × ×ª×™×‘:** `az://container-name/path/to/file.ext`

---

## ğŸŒ Google Cloud Storage (GCS) {#gcs}

### ×§×¨×™××” (×¤×•××‘×™)
```python
df = pl.read_csv('gs://bucket-name/file.csv')
```

### ×§×¨×™××” ×¢× ××™××•×ª
```python
# ×©×™×˜×” 1: × ×ª×™×‘ ×œ×§×•×‘×¥ JSON
storage_options = {'token': 'path/to/credentials.json'}

df = pl.read_csv('gs://bucket/file.csv', 
                 storage_options=storage_options)

# ×©×™×˜×” 2: ×˜×¢×™× ×ª JSON
import json

with open('credentials.json') as f:
    storage_options = json.load(f)

df = pl.read_csv('gs://bucket/file.csv', 
                 storage_options=storage_options)
```

### ×›×ª×™×‘×”
```python
import gcsfs

fs = gcsfs.GCSFileSystem(token='credentials.json')

with fs.open('gs://bucket/output.parquet', 'wb') as f:
    df.write_parquet(f)
```

**ğŸ“Œ ×¤×•×¨××˜ × ×ª×™×‘:** `gs://bucket-name/path/to/file.ext`

---

## ğŸ“Š BigQuery {#bigquery}

### ×§×¨×™××” - ConnectorX (××”×™×¨)
```python
project = 'my-project'
dataset = 'my_dataset'
table = 'my_table'

query = f"SELECT * FROM `{project}.{dataset}.{table}` LIMIT 1000"
uri = f'bigquery://path/to/credentials.json'

df = pl.read_database_uri(query, uri, engine='connectorx')
```

### ×§×¨×™××” - BigQuery Client
```python
from google.cloud import bigquery

client = bigquery.Client.from_service_account_json('creds.json')
query_job = client.query(query)
rows = query_job.result()

df = pl.from_arrow(rows.to_arrow())
```

### ×§×¨×™××” - read_database
```python
df = pl.read_database(query, connection=client)
```

### ×›×ª×™×‘×”
```python
import io

with io.BytesIO() as stream:
    df.write_csv(stream)
    stream.seek(0)
    
    job = client.load_table_from_file(
        stream,
        destination=f'{project}.{dataset}.new_table',
        project=project,
        job_config=bigquery.LoadJobConfig(
            autodetect=True,
            source_format=bigquery.SourceFormat.CSV
        )
    )
    job.result()
```

### ğŸ’¡ ×˜×™×¤×™× ×œ×—×™×¡×›×•×Ÿ
```python
# âœ… ×˜×•×‘ - ×‘×—×¨ ×¢××•×“×•×ª ×¡×¤×¦×™×¤×™×•×ª
query = "SELECT name, age FROM table LIMIT 1000"

# âŒ ×¨×¢ - ×©×•×œ×£ ×”×›×œ
query = "SELECT * FROM huge_table"

# âœ… ×˜×•×‘ - ×¡× ×Ÿ ×‘×©××™×œ×ª×”
query = "SELECT * FROM table WHERE date >= '2024-01-01'"
```

---

## â„ï¸ Snowflake {#snowflake}

### ×”×’×“×¨×ª ×¤×¨××˜×¨×™×
```python
config = {
    'username': 'YOUR_USER',
    'password': 'YOUR_PASS',
    'account': 'abc12345.region',
    'database': 'MY_DB',
    'schema': 'PUBLIC',
    'warehouse': 'COMPUTE_WH',
    'role': 'ACCOUNTADMIN'
}
```

### ×§×¨×™××” - ADBC (××•××œ×¥ - ×”×›×™ ××”×™×¨)
```python
uri = (
    f"snowflake://{config['username']}:{config['password']}"
    f"@{config['account']}/{config['database']}/{config['schema']}"
    f"?warehouse={config['warehouse']}&role={config['role']}"
)

query = "SELECT * FROM my_table"
df = pl.read_database_uri(query, uri, engine='adbc')
```

### ×§×¨×™××” - Snowflake Connector
```python
import snowflake.connector

conn = snowflake.connector.connect(
    user=config['username'],
    password=config['password'],
    account=config['account'],
    warehouse=config['warehouse'],
    database=config['database'],
    schema=config['schema']
)

# ×“×¨×š Arrow (××”×™×¨)
df = pl.from_arrow(
    conn.cursor().execute(query).fetch_arrow_all()
)

# ××•
df = pl.read_database(query, connection=conn)
```

### ğŸ’¡ ×˜×™×¤×™×
```python
# ×”×©×”×” warehouse ××—×¨×™ ×©×™××•×©
conn.cursor().execute("ALTER WAREHOUSE my_wh SUSPEND")

# ×©× ×” ×’×•×“×œ ×œ×¤×™ ×¦×•×¨×š
conn.cursor().execute(
    "ALTER WAREHOUSE my_wh SET WAREHOUSE_SIZE = 'LARGE'"
)
```

---

## ğŸ”§ ×˜×™×¤×™× ×›×œ×œ×™×™× ×•×“×™×‘××’ {#tips}

### 1. ×‘×—×™×¨×ª ×¤×•×¨××˜ ×§×•×‘×¥

| ×¤×•×¨××˜ | ×§×¨×™××” | ×›×ª×™×‘×” | ×’×•×“×œ | ××”×™×¨×•×ª | ××ª×™ ×œ×”×©×ª××© |
|-------|-------|-------|------|--------|------------|
| CSV | âœ… | âœ… | ğŸ˜ | ğŸ˜ | ×§×‘×¦×™× ×§×˜× ×™×, ×©×™×ª×•×£ ×× ×•×©×™ |
| Parquet | âœ… | âœ… | âœ… | âœ… | **××•××œ×¥ ×œ×›×œ ×“×‘×¨!** |
| JSON | âœ… | âœ… | âŒ | âŒ | APIs, ×”×’×“×¨×•×ª |

```python
# âœ… ××•××œ×¥ - Parquet
df.write_parquet('output.parquet')

# ğŸ˜ ×œ×©×™×ª×•×£ - CSV
df.write_csv('output.csv')
```

### 2. Lazy vs Eager

```python
# Eager - ×˜×•×¢×Ÿ ××™×“ (×§×‘×¦×™× ×§×˜× ×™×)
df = pl.read_csv('file.csv')

# Lazy - ××—×©×‘ ×¨×§ ×›×©×¦×¨×™×š (×§×‘×¦×™× ×’×“×•×œ×™×)
lf = pl.scan_csv('file.csv')
result = (
    lf
    .filter(pl.col('age') > 30)
    .select(['name', 'age'])
    .collect()  # ×¨×§ ×›××Ÿ ×–×” ×¨×¥ ×‘×××ª!
)
```

### 3. ××‘×˜×—×ª Credentials

```python
# âŒ ×œ×¢×•×œ× ×œ×!
api_key = "my-secret-key-123"

# âœ… ××©×ª× ×™ ×¡×‘×™×‘×”
import os
api_key = os.environ.get('API_KEY')

# âœ… ×§×‘×¦×™ config ××—×•×¥ ×œ-git
with open('../secrets/config.json') as f:
    config = json.load(f)

# âœ… .gitignore
"""
secrets/
*.json
credentials*
"""
```

### 4. ×“×™×‘××’ ×©×’×™××•×ª × ×¤×•×¦×•×ª

#### ×©×’×™××”: "No module named 's3fs'"
```bash
pip install s3fs
```

#### ×©×’×™××”: "Access Denied" ×‘-S3
```python
# ×‘×“×•×§ credentials
import boto3
s3 = boto3.client('s3')
s3.list_buckets()  # ×××•×¨ ×œ×¢×‘×•×“
```

#### ×©×’×™××”: "File not found" ×‘-GCS
```python
# ×•×•×“× ×©×”× ×ª×™×‘ × ×›×•×Ÿ
import gcsfs
fs = gcsfs.GCSFileSystem()
fs.ls('gs://bucket-name/')  # ×¨×©×™××ª ×§×‘×¦×™×
```

#### ×©×’×™××”: ×—×™×‘×•×¨ ×œ-Snowflake × ×›×©×œ
```python
# ×‘×“×•×§ ××ª ×›×œ ×”×¤×¨××˜×¨×™×
import snowflake.connector
try:
    conn = snowflake.connector.connect(
        user='USER',
        password='PASS',
        account='ACCOUNT'  # ×œ×œ× .snowflakecomputing.com
    )
    print("âœ… ×—×™×‘×•×¨ ×”×¦×œ×™×—!")
except Exception as e:
    print(f"âŒ ×©×’×™××”: {e}")
```

### 5. ××•×¤×˜×™××™×–×¦×™×”

```python
# âœ… ×¡× ×Ÿ ××•×§×“×
df = (
    pl.scan_parquet('huge_file.parquet')
    .filter(pl.col('year') == 2024)  # ×¡×™× ×•×Ÿ ×œ×¤× ×™ ×˜×¢×™× ×”
    .select(['id', 'name'])           # ×¨×§ ×¢××•×“×•×ª × ×“×¨×©×•×ª
    .collect()
)

# âŒ ×œ× ×™×¢×™×œ
df = pl.read_parquet('huge_file.parquet')  # ×˜×•×¢×Ÿ ×”×›×œ
df = df.filter(pl.col('year') == 2024)     # ××¡× ×Ÿ ××—×¨×™

# âœ… ×”×©×ª××© ×‘-predicate pushdown
import pyarrow.dataset as ds
dataset = ds.dataset('s3://bucket/data.parquet', filesystem=fs)
df = pl.scan_pyarrow_dataset(dataset).filter(...).collect()
```

### 6. × ×™×”×•×œ ×–×™×›×¨×•×Ÿ

```python
# ×‘×“×™×§×ª ×’×•×“×œ DataFrame
df.estimated_size('mb')  # ××’×”-×‘×™×™×˜×™×

# streaming ×œ×§×‘×¦×™× ×¢× ×§×™×™×
for batch in pl.read_csv_batched('huge.csv', batch_size=10000):
    process(batch)

# ×©×—×¨×•×¨ ×–×™×›×¨×•×Ÿ
del df
import gc
gc.collect()
```

---

## ğŸ“Š ×˜×‘×œ×ª ×”×©×•×•××” - ×¤×œ×˜×¤×•×¨××•×ª {#comparison}

### ××—×¡×•×Ÿ (Storage)

| ×ª×›×•× ×” | S3 | Azure Blob | GCS |
|-------|----|-----------|----|
| **××—×™×¨** (×œTB/×—×•×“×©) | ~$23 | ~$18 | ~$20 |
| **××”×™×¨×•×ª ×§×¨×™××”** | âš¡âš¡âš¡ | âš¡âš¡ | âš¡âš¡âš¡ |
| **×–××™× ×•×ª** | 99.99% | 99.9% | 99.95% |
| **××™× ×˜×’×¨×¦×™×” AWS** | âœ…âœ…âœ… | âŒ | âŒ |
| **××™× ×˜×’×¨×¦×™×” Azure** | âŒ | âœ…âœ…âœ… | âŒ |
| **××™× ×˜×’×¨×¦×™×” GCP** | âŒ | âŒ | âœ…âœ…âœ… |
| **×¤×©×˜×•×ª API** | âš¡âš¡ | âš¡âš¡ | âš¡âš¡âš¡ |
| **×’××™×©×•×ª ××–×•×¨×™×** | âœ…âœ…âœ… | âœ…âœ… | âœ…âœ… |

### ××¡×“×™ × ×ª×•× ×™× (Databases)

| ×ª×›×•× ×” | BigQuery | Snowflake |
|-------|----------|-----------|
| **××—×™×¨ ×œTB ×¡×¨×•×§** | $5 | $40 (compute) |
| **××”×™×¨×•×ª ×©××™×œ×ª×•×ª** | âš¡âš¡âš¡ | âš¡âš¡âš¡ |
| **Serverless** | âœ… | âœ… |
| **SQL Standard** | âœ… | âœ… |
| **×œ××™×“×”** | ×§×œ | ×‘×™× ×•× ×™ |
| **Data Sharing** | âŒ | âœ…âœ…âœ… |
| **Multi-cloud** | âŒ | âœ… |
| **××™×“×™××œ×™ ×œ** | GCP users | Enterprise |

### ××ª×™ ×œ×”×©×ª××© ×‘××”?

#### S3 - ×”×©×ª××© ×›××©×¨:
- âœ… ××ª×” ×›×‘×¨ ×‘-AWS
- âœ… ×¦×¨×™×š ××™× ×˜×’×¨×¦×™×” ×¢× EC2/Lambda
- âœ… ×¦×¨×™×š versioning ××ª×§×“×
- âœ… ×™×© ×œ×š ×¦×•×•×ª DevOps ×—×–×§

#### Azure Blob - ×”×©×ª××© ×›××©×¨:
- âœ… ××ª×” ×‘-Microsoft ecosystem
- âœ… ×™×© ×œ×š Active Directory
- âœ… ×¦×¨×™×š tier pricing (Hot/Cool/Archive)
- âœ… ×¦×•×•×ª ××•×›×¨ ×¢× Azure

#### GCS - ×”×©×ª××© ×›××©×¨:
- âœ… ××ª×” ×‘-GCP
- âœ… ××©×ª××© ×‘-BigQuery/Dataflow
- âœ… ×¨×•×¦×” API ×¤×©×•×˜
- âœ… ×¦×¨×™×š ×‘×™×¦×•×¢×™× ×’×‘×•×”×™×

#### BigQuery - ×”×©×ª××© ×›××©×¨:
- âœ… × ×™×ª×•×— × ×ª×•× ×™× ×’×“×•×œ×™× (TB+)
- âœ… ×©××™×œ×ª×•×ª ad-hoc ×ª×›×•×¤×•×ª
- âœ… ×œ× ×¨×•×¦×” ×œ× ×”×œ ×ª×©×ª×™×ª
- âœ… ×¦×¨×™×š auto-scaling

#### Snowflake - ×”×©×ª××© ×›××©×¨:
- âœ… ×¦×¨×™×š multi-cloud
- âœ… Data sharing ×—×©×•×‘
- âœ… ×¦×¨×™×š ×’××™×©×•×ª compute/storage
- âœ… ×™×© ×ª×§×¦×™×‘ ×’×“×•×œ

---

## ğŸ¯ ×“×•×’×××•×ª ×§×•×“ ××”×™×¨×•×ª

### Example 1: ×§×¨×™××” ×-S3 â† ×¢×™×‘×•×“ â† ×©××™×¨×” ×œ-GCS
```python
import polars as pl
import s3fs
import gcsfs

# ×§×¨×™××” ×-S3
s3_fs = s3fs.S3FileSystem()
df = pl.read_parquet('s3://my-bucket/input.parquet')

# ×¢×™×‘×•×“
df_processed = (
    df
    .filter(pl.col('status') == 'active')
    .select(['id', 'name', 'value'])
    .with_columns(pl.col('value') * 2)
)

# ×©××™×¨×” ×œ-GCS
gcs_fs = gcsfs.GCSFileSystem(token='creds.json')
with gcs_fs.open('gs://my-bucket/output.parquet', 'wb') as f:
    df_processed.write_parquet(f)
```

### Example 2: ×©××™×œ×ª×” ×‘-BigQuery â† ×©××™×¨×” ×œ-S3
```python
from google.cloud import bigquery
import s3fs

# ×§×¨×™××” ×-BigQuery
client = bigquery.Client.from_service_account_json('creds.json')
query = "SELECT * FROM `project.dataset.table` WHERE date >= '2024-01-01'"
df = pl.from_arrow(client.query(query).result().to_arrow())

# ×©××™×¨×” ×œ-S3
s3_fs = s3fs.S3FileSystem(key='KEY', secret='SECRET')
with s3_fs.open('s3://bucket/output.parquet', 'wb') as f:
    df.write_parquet(f)
```

### Example 3: Snowflake â† ×¢×™×‘×•×“ â† Azure
```python
import snowflake.connector
import adlfs

# ×§×¨×™××” ×-Snowflake
conn = snowflake.connector.connect(
    user='USER', password='PASS', account='ACCOUNT',
    database='DB', schema='SCHEMA', warehouse='WH'
)
df = pl.from_arrow(
    conn.cursor().execute("SELECT * FROM table").fetch_arrow_all()
)

# ×¢×™×‘×•×“
df_agg = df.group_by('category').agg(pl.col('amount').sum())

# ×©××™×¨×” ×œ-Azure
azure_fs = adlfs.AzureBlobFileSystem(
    account_name='ACCOUNT', account_key='KEY'
)
with azure_fs.open('az://container/output.parquet', 'wb') as f:
    df_agg.write_parquet(f)
```

---

## ğŸ†˜ ×¤×ª×¨×•×Ÿ ×‘×¢×™×•×ª × ×¤×•×¦×•×ª

### ×‘×¢×™×”: "ImportError: cannot import name 'polars'"
```bash
# ×¤×ª×¨×•×Ÿ
pip install --upgrade polars
```

### ×‘×¢×™×”: "PermissionDenied" ×‘-S3
```python
# ×‘×“×•×§ ×”×¨×©××•×ª IAM
import boto3
sts = boto3.client('sts')
print(sts.get_caller_identity())  # ××™ ××ª×”?

# ×‘×“×•×§ bucket permissions
s3 = boto3.client('s3')
s3.get_bucket_policy(Bucket='my-bucket')
```

### ×‘×¢×™×”: "AuthenticationFailed" ×‘-Azure
```python
# ×‘×“×•×§ connection string
from azure.storage.blob import BlobServiceClient
try:
    blob_service = BlobServiceClient(
        account_url=f"https://{account_name}.blob.core.windows.net",
        credential=account_key
    )
    blob_service.list_containers()
    print("âœ… ××™××•×ª ×”×¦×œ×™×—!")
except Exception as e:
    print(f"âŒ {e}")
```

### ×‘×¢×™×”: "Quota exceeded" ×‘-BigQuery
```python
# ×”×§×˜×Ÿ ××ª ×”×©××™×œ×ª×”
query = """
SELECT * FROM table
WHERE date >= CURRENT_DATE() - 7  -- ×¨×§ ×©×‘×•×¢ ××—×¨×•×Ÿ
LIMIT 10000                        -- ×”×’×‘×œ ×©×•×¨×•×ª
"""

# ××• ×”×©×ª××© ×‘-sampling
query = """
SELECT * FROM table
TABLESAMPLE SYSTEM (10 PERCENT)  -- 10% ××”× ×ª×•× ×™×
"""
```

### ×‘×¢×™×”: DataFrame ×’×“×•×œ ××“×™ ×œ×–×™×›×¨×•×Ÿ
```python
# ×¤×ª×¨×•×Ÿ 1: Lazy evaluation
lf = pl.scan_parquet('huge.parquet')
result = (
    lf
    .filter(pl.col('year') == 2024)
    .select(['id', 'value'])
    .collect()
)

# ×¤×ª×¨×•×Ÿ 2: Streaming
for batch in pl.read_parquet_batched('huge.parquet', batch_size=10000):
    process_batch(batch)

# ×¤×ª×¨×•×Ÿ 3: ×”×©×ª××© ×‘Cloud compute
# ×”×¢×‘×¨ ××ª ×”×¢×™×‘×•×“ ×œ-BigQuery/Snowflake
```

---

## ğŸ“Œ ×¡×™×›×•× - ×”×›×™ ×—×©×•×‘ ×œ×–×›×•×¨

### âœ… DO (×ª×¢×©×”)
1. **×”×©×ª××© ×‘-Parquet** - ×ª××™×“ ×¢×“×™×£ ×¢×œ CSV
2. **Lazy evaluation** - ×œ×§×‘×¦×™× ×’×“×•×œ×™×
3. **×¡× ×Ÿ ××•×§×“×** - ×œ×¤× ×™ ×§×¨×™××ª ×”× ×ª×•× ×™×
4. **××©×ª× ×™ ×¡×‘×™×‘×”** - ×œsensitive data
5. **Context managers** - `with` ×œ× ×™×”×•×œ ×§×‘×¦×™×
6. **×˜×™×¤×•×¡×™ × ×ª×•× ×™×** - ×©××•×¨ ×¢×œ consistency

### âŒ DON'T (××œ ×ª×¢×©×”)
1. **××œ ×ª×©××•×¨ credentials ×‘×§×•×“** - NEVER!
2. **××œ ×ª×§×¨× SELECT \*** - ×‘×—×¨ ×¢××•×“×•×ª
3. **××œ ×ª×©×›×— LIMIT** - ×‘×©××™×œ×ª×•×ª ××‘×—×Ÿ
4. **××œ ×ª×˜×¢×Ÿ ×”×›×œ ×œ×–×™×›×¨×•×Ÿ** - streaming!
5. **××œ ×ª×©×›×— ×œ×¡×’×•×¨ connections** - memory leaks
6. **××œ ×ª×¢×‘×•×“ ×™×©×™×¨×•×ª ×¢×œ production** - backup!

### ğŸ”¥ One-liners ×©×™××•×©×™×™×

```python
# ×§×¨×™××” ××”×™×¨×” ×S3
df = pl.read_parquet('s3://bucket/file.parquet')

# ×§×¨×™××” ×¢× ×¡×™× ×•×Ÿ
df = pl.scan_parquet('s3://bucket/huge.parquet').filter(pl.col('year')==2024).collect()

# ×©××™×¨×” ××”×™×¨×”
import s3fs; fs = s3fs.S3FileSystem()
with fs.open('s3://bucket/out.parquet','wb') as f: df.write_parquet(f)

# BigQuery ×‘×©×•×¨×” ××—×ª
df = pl.from_arrow(bigquery.Client().query("SELECT * FROM table LIMIT 100").result().to_arrow())

# ×‘×“×™×§×ª ×’×•×“×œ
print(f"{df.estimated_size('mb'):.2f} MB")
```

---

## ğŸ“š ××©××‘×™× ××”×™×¨×™×

### ×œ×™× ×§×™× ××”×™×¨×™×
- [Polars Docs](https://pola-rs.github.io/polars/)
- [AWS CLI Config](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html)
- [Azure Storage Explorer](https://azure.microsoft.com/features/storage-explorer/)
- [GCS Console](https://console.cloud.google.com/storage)
- [BigQuery Sandbox](https://console.cloud.google.com/bigquery) (×—×™× ×!)

### Cheat Sheets
- [Polars Cheat Sheet](https://franzdiebold.github.io/polars-cheat-sheet/Polars_cheat_sheet.pdf)
- [AWS S3 CLI](https://docs.aws.amazon.com/cli/latest/reference/s3/)
- [SQL for BigQuery](https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax)

---

<div style="background: #f0f0f0; padding: 20px; border-radius: 10px; text-align: center;">
<h2>ğŸ‰ ×–×”×•! ××ª× ××•×›× ×™× ×œ×›×‘×•×© ××ª ×”×¢× ×Ÿ! â˜ï¸</h2>
<p><strong>×©××¨×• ××ª ×”××“×¨×™×š ×”×–×” - ×ª×¦×˜×¨×›×• ××•×ª×•! ğŸ“Œ</strong></p>
<p>× ×•×¦×¨ ×¢× â¤ï¸ ×¢×‘×•×¨ ×§×”×™×œ×ª Polars ×‘×™×©×¨××œ ğŸ‡®ğŸ‡±</p>
</div>

---

**×’×¨×¡×”:** 1.0 | **×¢×•×“×›×Ÿ ×œ××—×¨×•× ×”:** 2024 | **×¨×™×©×™×•×Ÿ:** MIT
