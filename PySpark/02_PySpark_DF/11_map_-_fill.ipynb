{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    body, * {\n",
    "        direction: rtl !important;\n",
    "        text-align: right !important;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "#  专 拽祝 -PySpark - 专住驻专爪转 驻注转\n",
    "\n",
    "专转  住 转 砖 :\n",
    "1. **map()** - 专住驻专爪 住转 - [map](https://sparkbyexamples.com/pyspark/pyspark-map-transformation/)\n",
    "2. **flatMap()** - 专住驻专爪 砖转 - [flatMap](https://sparkbyexamples.com/pyspark/pyspark-flatmap-transformation/)\n",
    "3. **foreach()** - 专爪 注  - [foreach](https://sparkbyexamples.com/pyspark/pyspark-foreach-usage-with-examples/)\n",
    "4. **sampling()** -  拽专转 - [sampling](https://sparkbyexamples.com/pyspark/pyspark-sampling-example/)\n",
    "5. **fillna() & fill()** - 驻转 注专 NULL - [fillna() & fill()](https://sparkbyexamples.com/pyspark/pyspark-fillna-fill-replace-null-values/)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 专转 专砖转\n",
    "\n",
    "专砖转,  转 住驻专转 专砖转 爪专 SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 爪专转 SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. map() - 专住驻专爪转 驻 猴\n",
    "\n",
    "##  map()?\n",
    "\n",
    "驻拽爪 `map()` -PySpark  专住驻专爪 砖砖转 转 驻拽爪/ 注   -RDD (Resilient Distributed Dataset) 专 RDD 砖  转 转爪转.\n",
    "\n",
    "###  拽转 驻转:\n",
    "\n",
    "- DataFrame  转 专住驻专爪 map(),  砖 专 转 -RDD 转\n",
    "-  砖  转 , 砖转砖 -mapPartitions() 拽 map(),  砖转  转爪注 专拽 驻注 转  爪 拽  专砖\n",
    "\n",
    "## 转专 (Syntax)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 转专\n",
    "# map(f, preservesPartitioning=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1: 爪专转 RDD 专砖"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 爪专转 转 \n",
    "data = [\"Project\", \"Gutenberg's\", \"Alice's\", \"Adventures\",\n",
    "        \"in\", \"Wonderland\", \"Project\", \"Gutenberg's\", \"Adventures\",\n",
    "        \"in\", \"Wonderland\", \"Project\", \"Gutenberg's\"]\n",
    "\n",
    "# 爪专转 RDD\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# 驻住转 \n",
    "print(rdd.collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2: 砖砖 -map() 注 RDD\n",
    "\n",
    "  专住驻专爪 map() 注 RDD.  , 转  驻爪转  专砖 驻 专 -RDD 住祝 砖转 转. 转爪 砖 -RDD 专转  转  专砖."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 转 map() - 爪专转 tuples 注 注专 1\n",
    "rdd2 = rdd.map(lambda x: (x, 1))\n",
    "\n",
    "# 驻住转 转爪转\n",
    "print(rdd2.collect())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3: 砖砖 -map() 注 DataFrame\n",
    "\n",
    "专 爪注专, PySpark DataFrame  转 专住驻专爪 map() 转 驻拽爪 , 砖专 专爪  专住驻专爪 转转 砖转, 砖 专 转 -DataFrame -RDD  转 专住驻专爪 map()."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 爪专转 DataFrame \n",
    "data = [('James', 'Smith', 'M', 30),\n",
    "        ('Anna', 'Rose', 'F', 41),\n",
    "        ('Robert', 'Williams', 'M', 62)]\n",
    "\n",
    "columns = [\"firstname\", \"lastname\", \"gender\", \"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 砖砖 -map() - 转住转 注转 驻 拽住"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 专 -RDD 转 map\n",
    "rdd2 = df.rdd.map(lambda x: (x[0] + \",\" + x[1], x[2], x[3] * 2))\n",
    "\n",
    "df2 = rdd2.toDF([\"name\", \"gender\", \"new_salary\"])\n",
    "df2.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 砖砖 -map() - 转住转 注转 驻 砖"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 转住转 注转 驻 砖\n",
    "rdd2 = df.rdd.map(lambda x: \n",
    "    (x[\"firstname\"] + \",\" + x[\"lastname\"], x[\"gender\"], x[\"salary\"] * 2)\n",
    ")\n",
    "\n",
    "df2 = rdd2.toDF([\"name\", \"gender\", \"new_salary\"])\n",
    "df2.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 砖砖 驻拽爪 转转 砖转 注 map()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 专转 驻拽爪 转转\n",
    "def func1(x):\n",
    "    firstName = x.firstname\n",
    "    lastName = x.lastname\n",
    "    name = firstName + \",\" + lastName\n",
    "    gender = x.gender.lower()\n",
    "    salary = x.salary * 2\n",
    "    return (name, gender, salary)\n",
    "\n",
    "# 转 驻拽爪\n",
    "rdd2 = df.rdd.map(lambda x: func1(x))\n",
    "df2 = rdd2.toDF([\"name\", \"gender\", \"salary\"])\n",
    "df2.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. flatMap() - 专住驻专爪转 驻 砖 \n",
    "\n",
    "##  flatMap()?\n",
    "\n",
    "-`flatMap()` 砖 PySpark  专住驻专爪 砖砖转 转 -RDD/DataFrame (注专/驻转 注转 DataFrame) 专 转 驻拽爪 注   专 RDD/DataFrame 砖 砖 PySpark.\n",
    "\n",
    "## 转专"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 转专\n",
    "# flatMap(f, preservesPartitioning=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## : 爪专转 RDD 转 flatMap()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data = [\"Project Gutenberg's\",\n",
    "        \"Alice's Adventures in Wonderland\",\n",
    "        \"Project Gutenberg's\",\n",
    "        \"Adventures in Wonderland\",\n",
    "        \"Project Gutenberg's\"]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "print(rdd.collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转 flatMap() 注 驻爪"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 驻爪  专砖 驻 专\n",
    "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "print(rdd2.collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 砖砖 -flatMap() 注 DataFrame\n",
    "\n",
    "专 爪注专, PySpark DataFrame  转 专住驻专爪 flatMap(),  砖  驻拽爪转 SQL `explode()` 砖砖砖转 砖 转 注."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "arrayData = [\n",
    "    ('James', ['Java', 'Scala'], {'hair': 'black', 'eye': 'brown'}),\n",
    "    ('Michael', ['Spark', 'Java', None], {'hair': 'brown', 'eye': None}),\n",
    "    ('Robert', ['CSharp', ''], {'hair': 'red', 'eye': ''}),\n",
    "    ('Washington', None, None),\n",
    "    ('Jefferson', ['1', '2'], {})\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=arrayData, schema=['name', 'knownLanguages', 'properties'])\n",
    "df2 = df.select(df.name, explode(df.knownLanguages))\n",
    "df2.printSchema()\n",
    "df2.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. foreach() -  注  \n",
    "\n",
    "##  foreach()?\n",
    "\n",
    "-`foreach()` 砖 PySpark  驻注转 action  -RDD, DataFrame 注专/ 注   -DataFrame.   -for 注 拽住驻 转拽.  砖 驻注转 专转  砖驻拽爪 foreach()  专 注专, 拽 转  爪注转 转 驻拽爪转 拽 注   砖 RDD, DataFrame.\n",
    "\n",
    "## 转专"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 转专\n",
    "# DataFrame.foreach(f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1: foreach() 注 DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 爪专转 DataFrame \n",
    "columns = [\"Seqno\", \"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "        (\"2\", \"tracey smith\"),\n",
    "        (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.show()\n",
    "\n",
    "#  foreach()\n",
    "def f(df):\n",
    "    print(df.Seqno)\n",
    "\n",
    "df.foreach(f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 砖砖 -foreach() 注 accumulator"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#  foreach() 注 accumulator\n",
    "accum = spark.sparkContext.accumulator(0)\n",
    "df.foreach(lambda x: accum.add(int(x.Seqno)))\n",
    "print(accum.value)  # 砖 注  专专"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2: foreach() 注 RDD"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#  foreach() 注 RDD\n",
    "accum = spark.sparkContext.accumulator(0)\n",
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "rdd.foreach(lambda x: accum.add(x))\n",
    "print(accum.value)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. sample() -  拽专转 \n",
    "\n",
    "##  sample()?\n",
    "\n",
    "PySpark 住驻拽 驻拽爪转  拽专转:\n",
    "- `pyspark.sql.DataFrame.sample()`\n",
    "- `pyspark.sql.DataFrame.sampleBy()`\n",
    "- `RDD.sample()`\n",
    "- `RDD.takeSample()`\n",
    "\n",
    "## 转专"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 转专\n",
    "# sample(withReplacement, fraction, seed=None)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 砖砖 -fraction 拽转  拽专转"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = spark.range(100)\n",
    "print(df.sample(0.06).collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 砖砖 -seed 砖专 转 转"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(df.sample(0.1, 123).collect())\n",
    "print(df.sample(0.1, 123).collect())\n",
    "print(df.sample(0.1, 456).collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3  注 withReplacement"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(df.sample(True, 0.3, 123).collect())  # 注 驻转\n",
    "print(df.sample(0.3, 123).collect())  #  驻转"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4  专转 (Stratified Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df2 = df.select((df.id % 3).alias(\"key\"))\n",
    "print(df2.sampleBy(\"key\", {0: 0.1, 1: 0.2}, 0).collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 砖砖 -sample() 注 RDD"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "rdd = spark.sparkContext.range(0, 100)\n",
    "print(rdd.sample(False, 0.1, 0).collect())\n",
    "print(rdd.sample(True, 0.3, 123).collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 砖砖 -takeSample() 注 RDD"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(rdd.takeSample(False, 10, 0))\n",
    "print(rdd.takeSample(True, 30, 123))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. fillna() & fill() -  注专 住专 \n",
    "\n",
    "##   fillna() -fill()?\n",
    "\n",
    "-PySpark, `fillna()` 拽 DataFrame  `fill()` -DataFrameNaFunctions 砖砖 驻转 注专 NULL/None.\n",
    "\n",
    "## 转专"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# fillna(value, subset=None)\n",
    "# fill(value, subset=None)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爪专转 DataFrame  注 注专 住专"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "    Row(id=1, zipcode=704, type=\"STANDARD\", city=None, state=\"PR\", population=None),\n",
    "    Row(id=2, zipcode=704, type=None, city=\"PASEO COSTA DEL SUR\", state=\"PR\", population=None),\n",
    "    Row(id=3, zipcode=709, type=None, city=\"BDA SAN LUIS\", state=\"PR\", population=13700),\n",
    "    Row(id=4, zipcode=76166, type=\"UNIQUE\", city=\"CINGULAR WIRELESS\", state=\"TX\", population=84000),\n",
    "    Row(id=5, zipcode=76177, type=\"STANDARD\", city=None, state=\"TX\", population=None)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 驻转 注专 NULL 注 驻住 (0)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 祝 0 注专 null 注专  注转 住驻专 砖\n",
    "df.na.fill(value=0).show()\n",
    "\n",
    "# 祝 0 注专 null 专拽 注转 population\n",
    "df.na.fill(value=0, subset=[\"population\"]).show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 驻转 NULL 注 专转 专拽"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.na.fill(\"\").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 驻转 nulls 注转 住驻爪驻转"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.na.fill(\"unknown\", [\"city\"]) \\\n",
    "    .na.fill(\"\", [\"type\"]).show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 砖砖 -dictionary  注专 砖"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.na.fill({\"city\": \"unknown\", \"type\": \"\"}).show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  住\n",
    "\n",
    "专  :\n",
    "\n",
    "1. **map()** - 专住驻专爪 转 驻拽爪 注   -RDD\n",
    "2. **flatMap()** - 专住驻专爪 砖砖 转 转爪转 专砖 转\n",
    "3. **foreach()** - action 爪注 驻注 注   ( 专转 注专)\n",
    "4. **sample()** - 拽转  拽专转 转\n",
    "5. **fillna()** -  注专 住专 (NULL/None)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Learning! **\n",
    "\n",
    "---\n",
    "\n",
    "##  拽专转\n",
    "\n",
    "- [Apache Spark Documentation](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [SparkByExamples.com](https://sparkbyexamples.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
