{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    body, * {\n",
    "        direction: rtl !important;\n",
    "        text-align: right !important;\n",
    "    }\n",
    "</style>\n",
    "# ğŸ“Š ××“×¨×™×š PySpark ××§×™×£ - Pivot, PartitionBy ×•-MapType\n",
    "\n",
    "## ğŸ“š ×¡×§×™×¨×” ×›×œ×œ×™×ª\n",
    "\n",
    "××—×‘×¨×ª ×–×• ××›×¡×” ×©×œ×•×©×” × ×•×©××™× ×—×©×•×‘×™× ×‘-PySpark:\n",
    "\n",
    "1. **ğŸ”„ Pivot & Unpivot** - ×¡×™×‘×•×‘ × ×ª×•× ×™× ××¢××•×“×•×ª ×œ×©×•×¨×•×ª ×•×œ×”×™×¤×š - [Pivot & Unpivot](https://sparkbyexamples.com/pyspark/pyspark-pivot-and-unpivot-dataframe/)\n",
    "2. **ğŸ’¾ PartitionBy** - ×—×œ×•×§×ª × ×ª×•× ×™× ×œ×§×‘×¦×™× ××¨×•×‘×™× ×¢×œ ×”×“×™×¡×§ - [PartitionBy](https://sparkbyexamples.com/pyspark/pyspark-partitionby-example/)\n",
    "3. **ğŸ—ºï¸ MapType** - ×¢×‘×•×“×” ×¢× ××‘× ×™ × ×ª×•× ×™× ××¡×•×’ Dictionary/Map - [MapType](https://sparkbyexamples.com/pyspark/pyspark-maptype-dict-examples/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ ×—×œ×§ 1: Pivot ×•-Unpivot ×‘-DataFrame\n",
    "\n",
    "### ××”×• Pivot?\n",
    "\n",
    "×”×¤×•× ×§×¦×™×” `pivot()` ××©××©×ª ×œ×¡×™×‘×•×‘/×˜×¨× ×¡×¤×•×–×™×¦×™×” ×©×œ × ×ª×•× ×™× ××¢××•×“×” ××—×ª ×œ××¡×¤×¨ ×¢××•×“×•×ª DataFrame. Pivot ×”×•× ××’×¨×’×¦×™×” ×©×‘×” ××—×“ ××¢×¨×›×™ ×¢××•×“×•×ª ×”×§×™×‘×•×¥ ××•×¢×‘×¨ ×œ×¢××•×“×•×ª ×‘×•×“×“×•×ª ×¢× × ×ª×•× ×™× ××•×‘×—× ×™×.\n",
    "\n",
    "### ×ª×—×‘×™×¨ (Syntax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "pivot_df = original_df.groupBy(\"grouping_column\").pivot(\"pivot_column\").agg({\"agg_column\": \"agg_function\"})\n",
    "```\n",
    "\n",
    "- **grouping_column**: ×”×¢××•×“×” ×”××©××©×ª ×œ×§×™×‘×•×¥\n",
    "- **pivot_column**: ×”×¢××•×“×” ×©×¢×¨×›×™×” ×”××•×‘×—× ×™× ×™×”×¤×›×• ×œ×¢××•×“×•×ª ×—×“×©×•×ª\n",
    "- **agg_column**: ×”×¢××•×“×” ×©×¢×œ×™×” ××‘×•×¦×¢×ª ×”××’×¨×’×¦×™×” (×œ×“×•×’××”, ×©×™××•×© ×‘×¤×•× ×§×¦×™×•×ª ×›××• `sum`, `avg` ×•×›×•')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ×™×™×‘×•× ×¡×¤×¨×™×•×ª ×•×”×’×“×¨×ª Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×™×‘×•× ×¡×¤×¨×™×•×ª\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# ×™×¦×™×¨×ª Spark Session\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ×™×¦×™×¨×ª DataFrame ×œ×“×•×’××”"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×¦×™×¨×ª × ×ª×•× ×™× ×œ×“×•×’××”\n",
    "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \n",
    "        (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \n",
    "        (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \n",
    "        (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n",
    "\n",
    "columns= [\"Product\",\"Amount\",\"Country\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”„ ×‘×™×¦×•×¢ Pivot ×¢×œ DataFrame\n",
    "\n",
    "PySpark SQL ××¡×¤×§ ×¤×•× ×§×¦×™×™×ª `pivot()` ×œ×¡×™×‘×•×‘ ×”× ×ª×•× ×™× ××¢××•×“×” ××—×ª ×œ××¡×¤×¨ ×¢××•×“×•×ª. ×–×•×”×™ ××’×¨×’×¦×™×” ×©×‘×” ××—×“ ××¢×¨×›×™ ×¢××•×“×•×ª ×”×§×™×‘×•×¥ ××•×¢×‘×¨ ×œ×¢××•×“×•×ª ×‘×•×“×“×•×ª ×¢× × ×ª×•× ×™× ××•×‘×—× ×™×.\n",
    "\n",
    "×›×“×™ ×œ×§×‘×œ ××ª ×”×¡×›×•× ×”×›×•×œ×œ ×”××™×•×¦× ×œ×›×œ ××“×™× ×” ×¢×‘×•×¨ ×›×œ ××•×¦×¨, × ×‘×¦×¢ ×§×™×‘×•×¥ ×œ×¤×™ `Product`, pivot ×œ×¤×™ `Country`, ×•×¡×›×•× ×©×œ `Amount`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×‘×™×¦×•×¢ pivot\n",
    "pivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
    "pivotDF.printSchema()\n",
    "pivotDF.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×”×¤×¢×•×œ×” ×ª×¢×‘×™×¨ ××ª ×”××“×™× ×•×ª ××©×•×¨×•×ª DataFrame ×œ×¢××•×“×•×ª ×•×ª×™×™×¦×¨ ×¤×œ×˜ ×©×‘×• ×›×œ × ×ª×•×Ÿ ×©××™× ×• ×§×™×™× ××™×•×¦×’ ×›-`null` ×›×‘×¨×™×¨×ª ××—×“×œ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš¡ ×©×™×¤×•×¨ ×‘×™×¦×•×¢×™× ×‘-PySpark 2.0\n",
    "\n",
    "×’×¨×¡×” 2.0 ×•××¢×œ×” ×©×™×¤×¨×” ××ª ×”×‘×™×¦×•×¢×™× ×©×œ Pivot. ×¢× ×–××ª, ×× ××ª×” ××©×ª××© ×‘×’×¨×¡×” × ××•×›×” ×™×•×ª×¨, ×©×™× ×œ×‘ ×©-pivot ×”×™× ×¤×¢×•×œ×” ×™×§×¨×” ×××•×“, ×œ×›×Ÿ ××•××œ×¥ ×œ×¡×¤×§ × ×ª×•× ×™ ×¢××•×“×•×ª (×× ×™×“×•×¢×™×) ×›××¨×’×•×× ×˜ ×œ×¤×•× ×§×¦×™×” ×›×¤×™ ×©××•×¦×’ ×œ×”×œ×Ÿ.\n",
    "\n",
    "#### ×’×™×©×” 1: ×©×™××•×© ×‘×¤×•× ×§×¦×™×” ×¢× × ×ª×•× ×™ ×¢××•×“×•×ª"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pivot ×¢× ×¦×™×•×Ÿ ××“×™× ×•×ª ××¨××©\n",
    "countries = [\"USA\",\"China\",\"Canada\",\"Mexico\"]\n",
    "pivotDF = df.groupBy(\"Product\").pivot(\"Country\", countries).sum(\"Amount\")\n",
    "pivotDF.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ×’×™×©×” 2: ××’×¨×’×¦×™×” ×“×•-×©×œ×‘×™×ª\n",
    "\n",
    "×’×™×©×” × ×•×¡×¤×ª ×”×™× ×œ×‘×¦×¢ ××’×¨×’×¦×™×” ×“×•-×©×œ×‘×™×ª. PySpark 2.0 ××©×ª××© ×‘×™×™×©×•× ×–×” ×›×“×™ ×œ×©×¤×¨ ××ª ×”×‘×™×¦×•×¢×™× (Spark-13749)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ××’×¨×’×¦×™×” ×“×•-×©×œ×‘×™×ª\n",
    "pivotDF = df.groupBy(\"Product\",\"Country\") \\\n",
    "      .sum(\"Amount\") \\\n",
    "      .groupBy(\"Product\") \\\n",
    "      .pivot(\"Country\") \\\n",
    "      .sum(\"sum(Amount)\")\n",
    "pivotDF.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×©×ª×™ ×”×“×•×’×××•×ª ×œ×¢×™×œ ××—×–×™×¨×•×ª ××ª ××•×ª×• ×¤×œ×˜ ××š ×¢× ×‘×™×¦×•×¢×™× ×˜×•×‘×™× ×™×•×ª×¨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”™ Unpivot ×©×œ PySpark DataFrame\n",
    "\n",
    "Unpivot ×”×•× ×¤×¢×•×œ×” ×”×¤×•×›×”. × ×™×ª×Ÿ ×œ×”×©×™×’ ×–××ª ×¢×œ ×™×“×™ ×¡×™×‘×•×‘ ×¢×¨×›×™ ×¢××•×“×•×ª ×œ×¢×¨×›×™ ×©×•×¨×•×ª. PySpark SQL ××™× ×• ××›×™×œ ×¤×•× ×§×¦×™×™×ª unpivot, ×œ×›×Ÿ × ×©×ª××© ×‘×¤×•× ×§×¦×™×” `stack()` ×‘××§×•×.\n",
    "\n",
    "×”×§×•×“ ×”×‘× ×××™×¨ ×¢××•×“×ª pivot \"country\" ×œ×©×•×¨×•×ª."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Unpivot - ×”××¨×ª ×¢××•×“×•×ª ×œ××“×™× ×•×ª ×‘×—×–×¨×” ×œ×©×•×¨×•×ª\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "unpivotExpr = \"stack(3, 'Canada', Canada, 'China', China, 'Mexico', Mexico) as (Country,Total)\"\n",
    "unPivotDF = pivotDF.select(\"Product\", expr(unpivotExpr)) \\\n",
    "    .where(\"Total is not null\")\n",
    "unPivotDF.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×”×¤×¢×•×œ×” ×××™×¨×” ×¢××•×“×ª pivot \"country\" ×œ×©×•×¨×•×ª."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â“ ×©××œ×•×ª × ×¤×•×¦×•×ª ×¢×œ pivot() ×•-unpivot()\n",
    "\n",
    "#### ×”×× × ×™×ª×Ÿ ×œ×‘×¦×¢ transpose ××• pivot ×©×œ PySpark DataFrame ×œ×œ× ××’×¨×’×¦×™×”?\n",
    "\n",
    "×›××•×‘×Ÿ ×©×›×Ÿ, ××š ×œ××¨×‘×” ×”×¦×¢×¨, ××™ ××¤×©×¨ ×œ×”×©×™×’ ×–××ª ×‘×××¦×¢×•×ª ×¤×•× ×§×¦×™×™×ª Pivot. ×¢× ×–××ª, pivoting ××• transposing ××‘× ×” DataFrame ×œ×œ× ××’×¨×’×¦×™×” ××©×•×¨×•×ª ×œ×¢××•×“×•×ª ×•××¢××•×“×•×ª ×œ×©×•×¨×•×ª × ×™×ª×Ÿ ×œ×‘×™×¦×•×¢ ×‘×§×œ×•×ª ×‘×××¦×¢×•×ª PySpark ×•-Scala hack. ×× × ×¢×™×™×Ÿ ×‘×“×•×’××” ×‘-stackoverflow.\n",
    "\n",
    "#### ××” ×§×•×¨×” ×× ×™×© ×¢×¨×›×™× ×›×¤×•×œ×™× ×¢×‘×•×¨ ×¢××•×“×ª ×”-pivot?\n",
    "\n",
    "×¤×¢×•×œ×ª `pivot` ×“×•×¨×©×ª ×©×™×œ×•×‘×™× ×™×™×—×•×“×™×™× ×©×œ ×¢××•×“×•×ª ×§×™×‘×•×¥ ×•-pivot. ×× ×™×© ×¢×¨×›×™× ×›×¤×•×œ×™×, ×™×™×ª×›×Ÿ ×©×ª×¦×˜×¨×š ×œ×‘×¦×¢ ××’×¨×’×¦×™×” (×œ××©×œ, ×©×™××•×© ×‘×¤×¨××˜×¨ `agg`) ×›×“×™ ×œ×¤×ª×•×¨ ××ª ×”×§×•× ×¤×œ×™×§×˜.\n",
    "\n",
    "#### ×›×™×¦×“ ×¤×¢×•×œ×ª pivot ××˜×¤×œ×ª ×‘×¢×¨×›×™× ×—×¡×¨×™×?\n",
    "\n",
    "×¤×¢×•×œ×ª `pivot` ×××œ××ª ×¢×¨×›×™× ×—×¡×¨×™× ×¢× null. ×× ××ª×” ×¦×¨×™×š ×œ×˜×¤×œ ×‘×¢×¨×›×™× ×—×¡×¨×™× ×‘××•×¤×Ÿ ×©×•× ×”, ××ª×” ×™×›×•×œ ×œ×”×©×ª××© ×‘×©×™×˜×•×ª ×›××• `fillna` ××• `na.drop` ×¢×œ ×”-DataFrame ×”-pivoted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ×¡×™×›×•× - Pivot & Unpivot\n",
    "\n",
    "×¨××™× ×• ×›×™×¦×“ ×œ-Pivot DataFrame ×¢× PySpark ×œ×“×•×’××” ×•×œ-Unpivot ×‘×—×–×¨×” ×‘×××¦×¢×•×ª ×¤×•× ×§×¦×™×•×ª SQL. ×•×¨××™× ×• ×’× ×›×™×¦×“ ×©×™× ×•×™×™ PySpark 2.0 ×©×™×¤×¨×• ××ª ×”×‘×™×¦×•×¢×™× ×¢×œ ×™×“×™ ×‘×™×¦×•×¢ ××’×¨×’×¦×™×” ×“×•-×©×œ×‘×™×ª.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ ×—×œ×§ 2: PySpark partitionBy() - ×›×ª×™×‘×” ×œ×“×™×¡×§\n",
    "\n",
    "### ××”×• Partition ×‘-PySpark?\n",
    "\n",
    "PySpark `partitionBy()` ×”×™× ×¤×•× ×§×¦×™×” ×©×œ ××—×œ×§×” `pyspark.sql.DataFrameWriter` ×”××©××©×ª ×œ×—×œ×•×§×ª ×”× ×ª×•× ×™× ×”×’×“×•×œ×™× (DataFrame) ×œ×§×‘×¦×™× ×§×˜× ×™× ×™×•×ª×¨ ×¢×œ ×‘×¡×™×¡ ×¢××•×“×” ××—×ª ××• ×™×•×ª×¨ ×ª×•×š ×›×ª×™×‘×” ×œ×“×™×¡×§. ×‘×•××• × ×¨××” ×›×™×¦×“ ×œ×”×©×ª××© ×‘×–×” ×¢× ×“×•×’×××•×ª Python.\n",
    "\n",
    "### ×™×ª×¨×•× ×•×ª Partition\n",
    "\n",
    "×—×œ×•×§×ª ×”× ×ª×•× ×™× ×¢×œ ××¢×¨×›×ª ×”×§×‘×¦×™× ×”×™× ×“×¨×š ×œ×©×¤×¨ ××ª ×”×‘×™×¦×•×¢×™× ×©×œ ×”×©××™×œ×ª×•×ª ×›××©×¨ ××ª××•×“×“×™× ×¢× ××¢×¨×š × ×ª×•× ×™× ×’×“×•×œ, ×œ××©×œ Data lake.\n",
    "\n",
    "×›×¤×™ ×©××ª×” ××•×“×¢, PySpark ××ª×•×›× ×Ÿ ×œ×¢×‘×“ ××¢×¨×›×™ × ×ª×•× ×™× ×’×“×•×œ×™× ××”×¨ ×¤×™ 100 ××”×¢×™×‘×•×“ ×”××¡×•×¨×ª×™. ×–×” ×œ× ×”×™×” ××¤×©×¨×™ ×œ×œ× partition. ×œ×”×œ×Ÿ ××¡×¤×¨ ×™×ª×¨×•× ×•×ª ×©×œ ×©×™××•×© ×‘-partitions ×©×œ PySpark ×¢×œ ×–×™×›×¨×•×Ÿ ××• ×¢×œ ×“×™×¡×§:\n",
    "\n",
    "- **×’×™×©×” ××”×™×¨×” ×œ× ×ª×•× ×™×** âš¡\n",
    "- **×™×›×•×œ×ª ×œ×‘×¦×¢ ×¤×¢×•×œ×•×ª ×¢×œ ××¢×¨×š × ×ª×•× ×™× ×§×˜×Ÿ ×™×•×ª×¨** ğŸ“Š\n",
    "\n",
    "Partition ×‘×× ×•×—×” (×“×™×¡×§) ×”×™× ×ª×›×•× ×” ×©×œ ××¡×“×™ × ×ª×•× ×™× ×¨×‘×™× ×•××¡×’×¨×•×ª ×¢×™×‘×•×“ × ×ª×•× ×™×, ×•×”×™× ××¤×ª×— ×œ×”×¤×•×š ×¢×‘×•×“×•×ª ×œ×¢×‘×•×“ ×‘×§× ×” ××™×“×”.\n",
    "\n",
    "### ğŸ”€ PySpark ×ª×•××š ×‘-Partition ×‘×©×ª×™ ×“×¨×›×™×\n",
    "\n",
    "1. **Partition in memory (DataFrame)**: ××ª×” ×™×›×•×œ ×œ×—×œ×§ ××• ×œ×‘×¦×¢ repartition ×©×œ DataFrame ×¢×œ ×™×“×™ ×§×¨×™××” ×œ-`repartition()` ××• `coalesce()`\n",
    "2. **Partition on disk (File system)**: ×‘×–××Ÿ ×›×ª×™×‘×ª PySpark DataFrame ×‘×—×–×¨×” ×œ×“×™×¡×§, ××ª×” ×™×›×•×œ ×œ×‘×—×•×¨ ×›×™×¦×“ ×œ×—×œ×§ ××ª ×”× ×ª×•× ×™× ×¢×œ ×‘×¡×™×¡ ×¢××•×“×•×ª ×‘×××¦×¢×•×ª `partitionBy()` ×©×œ `pyspark.sql.DataFrameWriter`. ×–×” ×“×•××” ×œ-Hives partitions scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“‚ ×™×¦×™×¨×ª DataFrame ×œ×“×•×’××”\n",
    "\n",
    "×‘×•××• × ×™×¦×•×¨ DataFrame ×¢×œ ×™×“×™ ×§×¨×™××ª ×§×•×‘×¥ CSV. ×¢×§×•×‘ ××—×¨ Github zipcodes.csv ×›×“×™ ×œ×”×•×¨×™×“ ××ª ×§×•×‘×¥ ×”-CSV."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×¦×™×¨×ª DataFrame ××§×•×‘×¥ CSV\n",
    "df = spark.read.option(\"header\",True) \\\n",
    "        .csv(\"../data/simple-zipcodes.csv\")\n",
    "df.printSchema()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "××”-DataFrame ×œ×¢×™×œ, ××©×ª××© ×‘-`state` ×›××¤×ª×— partition ×œ×“×•×’×××•×ª ×©×œ× ×• ×œ×”×œ×Ÿ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¾ PySpark partitionBy()\n",
    "\n",
    "×‘-PySpark, ×˜×¨× ×¡×¤×•×¨××¦×™×™×ª `partitionBy()` ××©××©×ª ×œ×—×œ×•×§×ª × ×ª×•× ×™× ×‘-RDD ××• DataFrame ×¢×œ ×‘×¡×™×¡ ×”××—×™×¦×” ×©×¦×•×™× ×”. ×”×™× ××™×•×©××ª ×‘×“×¨×š ×›×œ×œ ×œ××—×¨ ×¤×¢×•×œ×•×ª ××¡×•×™××•×ª ×›××• `groupBy()` ××• `join()` ×›×“×™ ×œ×©×œ×•×˜ ×‘×”×¤×¦×” ×©×œ ×”× ×ª×•× ×™× ×¢×œ ×¤× ×™ ××—×™×¦×•×ª.\n",
    "\n",
    "×”-`partitionBy()` ×–××™×Ÿ ×‘××—×œ×§×” `DataFrameWriter`, ×•×œ×›×Ÿ ××©×ª××©×™× ×‘×” ×›×“×™ ×œ×›×ª×•×‘ ××ª × ×ª×•× ×™ ×”-partition ×œ×“×™×¡×§.\n",
    "\n",
    "#### ×ª×—×‘×™×¨ (Syntax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "partitionBy(self, *cols)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×›××©×¨ ××ª×” ×›×•×ª×‘ PySpark DataFrame ×œ×“×™×¡×§ ×¢×œ ×™×“×™ ×§×¨×™××” ×œ-`partitionBy()`, PySpark ××¤×¦×œ ××ª ×”×¨×©×•××•×ª ×¢×œ ×‘×¡×™×¡ ×¢××•×“×ª ×”-partition ×•×©×•××¨ ×›×œ × ×ª×•× ×™ partition ×‘×ª×ª-×¡×¤×¨×™×™×”.\n",
    "\n",
    "#### ×“×•×’××”: Partition ×¢× ×¢××•×“×” ××—×ª"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×“×•×’××” ×œ-partitionBy()\n",
    "df.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"state\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(\"tmp/zipcodes-state\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×”× ×ª×•× ×™× ×©×œ×™ ×›×•×œ×œ×™× ×©×© ××“×™× ×•×ª, ×œ×›×Ÿ × ×•×¦×¨×• ×©×© ×¡×¤×¨×™×•×ª ×¢×œ ×”×“×™×¡×§. ×©× ×ª×ª-×”×¡×¤×¨×™×™×” ×™×”×™×” ×¢××•×“×ª ×”-partition ×•×”×¢×¨×š ×©×œ×” (partition column=value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "× ×™×ª×Ÿ ×œ×××ª ×–××ª ×¢×œ ×™×“×™ ×”×¦×’×ª ×”×§×‘×¦×™× ×‘×¡×¤×¨×™×™×”:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”×¦×’×ª ××‘× ×” ×”×§×‘×¦×™× (×‘×©×•×¨×ª ×”×¤×§×•×“×”)\n",
    "# !ls -lrt tmp/zipcodes-state"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**×”×¢×¨×” ×—×©×•×‘×”:** ×‘×–××Ÿ ×›×ª×™×‘×ª ×”× ×ª×•× ×™× ×›-partitions, PySpark ××‘×˜×œ ××ª ×¢××•×“×ª ×”-partition ×‘×§×•×‘×¥ ×”× ×ª×•× ×™× ×•××•×¡×™×£ ××ª ×¢××•×“×ª ×”-partition ×•×”×¢×¨×š ×œ×©× ×”×ª×™×§×™×™×”, ×•×—×•×¡×š ××§×•× ××—×¡×•×Ÿ.\n",
    "\n",
    "×‘×›×œ ×¡×¤×¨×™×™×”, ×™×™×ª×›× ×• ×—×œ×§ ×§×•×‘×¥ ××—×“ ××• ×™×•×ª×¨ (×©×›×Ÿ ××¢×¨×š ×”× ×ª×•× ×™× ×©×œ× ×• ×§×˜×Ÿ, ×›×œ ×”×¨×©×•××•×ª ×œ×›×œ `state` × ×©××¨×•×ª ×‘×—×œ×§ ×§×•×‘×¥ ××—×“). × ×™×ª×Ÿ ×œ×©× ×•×ª ×”×ª× ×”×’×•×ª ×–×• ×¢×œ ×™×“×™ ×¦×™×•×Ÿ ××¡×¤×¨ ×”-partitions (×—×œ×§×™ ×§×‘×¦×™×) ×©×ª×¨×¦×” ×œ×›×œ `state` ×›××¨×’×•×× ×˜ ×œ×©×™×˜×ª `repartition()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“š PySpark partitionBy() - ×¢××•×“×•×ª ××¨×•×‘×•×ª\n",
    "\n",
    "××ª×” ×™×›×•×œ ×’× ×œ×™×¦×•×¨ partitions ×¢×œ ×¢××•×“×•×ª ××¨×•×‘×•×ª ×‘×××¦×¢×•×ª PySpark `partitionBy()`. ×¤×©×•×˜ ×”×¢×‘×¨ ××ª ×”×¢××•×“×•×ª ×©×‘×¨×¦×•× ×š ×œ×—×œ×§ ×›××¨×’×•×× ×˜×™× ×œ×©×™×˜×” ×–×•."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Partition ×¢× ×¢××•×“×•×ª ××¨×•×‘×•×ª\n",
    "df.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"state\",\"city\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(\"tmp/zipcodes-state-city\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×–×” ×™×•×¦×¨ ×”×™×¨×¨×›×™×™×ª ×ª×™×§×™×•×ª ×œ×›×œ partition; ×¦×™×™× ×• ××ª ×”-partition ×”×¨××©×•×Ÿ ×›-`state` ×•××—×¨×™×• `city`, ×œ×›×Ÿ ×”×™× ×™×•×¦×¨×ª ×ª×™×§×™×™×ª `city` ×‘×ª×•×š ×ª×™×§×™×™×ª `state` (×ª×™×§×™×™×” ××—×ª ×¢×‘×•×¨ ×›×œ `city` ×‘-`state`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "× ×™×ª×Ÿ ×œ×××ª ×–××ª ×¢×œ ×™×“×™ ×”×¦×’×ª ×”××‘× ×”:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”×¦×’×ª ××‘× ×” ×”×™×¨×¨×›×™ ×©×œ ×”×§×‘×¦×™×\n",
    "!ls -lrt tmp/zipcodes-state-city/state=AL"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”„ ×©×™××•×© ×‘-repartition() ×™×—×“ ×¢× partitionBy()\n",
    "\n",
    "`repartition()` ××©××© ×œ×”×’×“×œ×” ××• ×”×§×˜× ×” ×©×œ ××¡×¤×¨ ×”-partitions ×‘×–×™×›×¨×•×Ÿ. ×›××©×¨ ××©×ª××©×™× ×‘-`repartition()` ×¢× `partitionBy()`, ×”×•× ××¤×¦×œ ×¢×•×“ ×™×•×ª×¨ ×œ××¡×¤×¨ partitions ××¨×•×‘×™× ×¢×œ ×‘×¡×™×¡ × ×ª×•× ×™ ×¢××•×“×•×ª."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×©×™××•×© ×‘-repartition() ×•-partitionBy() ×™×—×“\n",
    "df.repartition(2) \\\n",
    "        .write.option(\"header\",True) \\\n",
    "        .partitionBy(\"state\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(\"tmp/zipcodes-state-more\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×œ××—×¨ ×‘×“×™×§×ª ×”×ª×™×§×™×™×” ×©×‘×” ×”× ×ª×•× ×™× × ×›×ª×‘×™×, ×ª×‘×—×™×Ÿ ×©×™×© ×¨×§ ×©× ×™ ×§×‘×¦×™ partition ×¢×‘×•×¨ ×›×œ ××“×™× ×”. ×œ××—×¨ ×©××¢×¨×š ×”× ×ª×•× ×™× ×›×•×œ×œ ×©×© ××“×™× ×•×ª ×™×™×—×•×“×™×•×ª ×•×™×© ×©× ×™ partitions ×‘×–×™×›×¨×•×Ÿ ×”××•×§×¦×™× ×œ×›×œ ××“×™× ×”, ×”×§×•×“ ×™×•×¦×¨ ××§×¡×™××•× ×©×œ 12 ×§×‘×¦×™ partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ Data Skew - ×©×œ×™×˜×” ×‘××¡×¤×¨ ×”×¨×©×•××•×ª ×œ×›×œ Partition\n",
    "\n",
    "×”×©×ª××© ×‘××¤×©×¨×•×ª `maxRecordsPerFile` ×× ×ª×¨×¦×” ×œ×©×œ×•×˜ ×‘××¡×¤×¨ ×”×¨×©×•××•×ª ×¢×‘×•×¨ ×›×œ partition. ×–×” ×©×™××•×©×™ ×‘××™×•×—×“ ×›××©×¨ ×”× ×ª×•× ×™× ×©×œ×š skewed (×›×œ×•××¨, ×™×© partitions ×¢× ××¡×¤×¨ ×¨×©×•××•×ª × ××•×š ×××•×“ ×•-partitions ××—×¨×™× ×¢× ××¡×¤×¨ ×¨×©×•××•×ª ×’×‘×•×”)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×©×œ×™×˜×” ×‘××¡×¤×¨ ×¨×©×•××•×ª ×œ×›×œ ×§×•×‘×¥\n",
    "df.write.option(\"header\",True) \\\n",
    "        .option(\"maxRecordsPerFile\", 2) \\\n",
    "        .partitionBy(\"state\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(\"tmp/zipcodes-state\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×”×“×•×’××” ×œ×¢×™×œ ×™×•×¦×¨×ª ×§×‘×¦×™ part ××¨×•×‘×™× ×¢×‘×•×¨ ×›×œ `state`, ×•×›×œ ×§×•×‘×¥ part ××›×™×œ ×¨×§ 2 ×¨×©×•××•×ª."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“– ×§×¨×™××ª Partition ×¡×¤×¦×™×¤×™\n",
    "\n",
    "×§×¨×™××•×ª ××”×™×¨×•×ª ×™×•×ª×¨ ×‘×”×¨×‘×” ×¢×œ × ×ª×•× ×™× ×©-partitioned. ×§×˜×¢ ×”×§×•×“ ×”×–×” ×××—×–×¨ ××ª ×”× ×ª×•× ×™× ×-partition ×¡×¤×¦×™×¤×™ `state=AL` ×•-`city=SPRINGVILLE`. ×›××Ÿ, ×”×•× ×¤×©×•×˜ ×§×•×¨× ××ª ×”× ×ª×•× ×™× ×××•×ª×” ×ª×™×§×™×™×” ×¡×¤×¦×™×¤×™×ª ×‘××§×•× ×œ×¡×¨×•×§ ×§×•×‘×¥ ×©×œ× (×›×©×œ× partitioned)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×§×¨×™××” ×× ×ª×•× ×™ partition ×¡×¤×¦×™×¤×™×™×\n",
    "dfSinglePart = spark.read.option(\"header\",True) \\\n",
    "            .csv(\"tmp/zipcodes-state-city/state=AZ/city=MESA\")\n",
    "dfSinglePart.printSchema()\n",
    "dfSinglePart.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×‘×¢×ª ×§×¨×™××ª × ×ª×•× ×™ Partition ×¡×¤×¦×™×¤×™×™× ×œ-DataFrame, ×”×•× ×œ× ×©×•××¨ ××ª ×¢××•×“×•×ª ×”-partition ×¢×œ DataFrame, ×•×œ×›×Ÿ ×‘-DataFrame ×—×¡×¨×•×ª ×¢××•×“×•×ª `state` ×•-`city`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” PySpark SQL - ×§×¨×™××ª × ×ª×•× ×™ Partition\n",
    "\n",
    "×–×• ×“×•×’××” ×œ×›×ª×™×‘×ª Spark DataFrame ×¢×œ ×™×“×™ ×©××™×¨×ª ×¢××•×“×•×ª ×”-partition ×¢×œ DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×§×¨×™××” ×× ×ª×•× ×™ partition ×ª×•×š ×©×™××•×© ×‘-SQL\n",
    "parqDF = spark.read.option(\"header\",True).csv(\"tmp/zipcodes-state\")\n",
    "parqDF.createOrReplaceTempView(\"ZIPCODE\")\n",
    "spark.sql(\"select * from ZIPCODE  where state='AL' and city = 'SPRINGVILLE'\") \\\n",
    "    .show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×‘×™×¦×•×¢ ×”×©××™×œ×ª×” ×”×–×” ×’× ××”×™×¨ ××©××¢×•×ª×™×ª ××”×©××™×œ×ª×” ×œ×œ× partition. ×”×•× ××¡× ×Ÿ ××ª ×”× ×ª×•× ×™× ×ª×—×™×œ×” ×‘-`state`, ×•××– ××™×™×©× ××¡× × ×™× ×œ×¢××•×“×” `city` ××‘×œ×™ ×œ×¡×¨×•×§ ××ª ××¢×¨×š ×”× ×ª×•× ×™× ×”×©×œ×."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤” ×›×™×¦×“ ×œ×‘×—×•×¨ ×¢××•×“×ª Partition ×‘×¢×ª ×›×ª×™×‘×” ×œ××¢×¨×›×ª ×”×§×‘×¦×™×?\n",
    "\n",
    "×‘×¢×ª ×™×¦×™×¨×ª partitions ×¢×œ×™×š ×œ×”×™×•×ª ×–×”×™×¨ ×××•×“ ×¢× ××¡×¤×¨ ×”-partitions ×©×ª×™×¦×•×¨, ×©×›×Ÿ ×™×¦×™×¨×ª ×™×•×ª×¨ ××“×™ partitions ×™×•×¦×¨×ª ×™×•×ª×¨ ××“×™ ×ª×ª-×¡×¤×¨×™×•×ª ×‘-HDFS, ××” ×©××‘×™× ×œ×¢×•××¡ ×•××™× ×”×œ ××™×•×ª×¨×™× ×œ-NameNode (×× ××ª×” ××©×ª××© ×‘-Hadoop), ×©×›×Ÿ ×”×•× ×—×™×™×‘ ×œ×©××•×¨ ××ª ×›×œ ×”××˜×-×“××˜×” ×©×œ ××¢×¨×›×ª ×”×§×‘×¦×™× ×‘×–×™×›×¨×•×Ÿ.\n",
    "\n",
    "### ×“×•×’××” ×œ××¦×‘ ×‘×¢×™×™×ª×™:\n",
    "\n",
    "×§×—×• ×‘×—×©×‘×•×Ÿ ×ª×¨×—×™×© ×©×‘×• ×™×© ×œ×›× ×˜×‘×œ×ª ××•×›×œ×•×¡×™×Ÿ ×××¨×™×§××™×ª ×¢× ××™×§×•×“, ×¢×¨×™×, ××“×™× ×•×ª ×•×¢××•×“×•×ª × ×•×¡×¤×•×ª. ×©×™××•×© ×‘-state ×›××¤×ª×— partition ××—×œ×§ ××ª ×”×˜×‘×œ×” ×œ×›-50 partitions. ×›×ª×•×¦××” ××›×š, ×›×©××—×¤×©×™× ××™×§×•×“ ×‘×ª×•×š ××“×™× ×” ×¡×¤×¦×™×¤×™×ª (×œ××©×œ, state='CA' ×•-zipCode='92704'), ×”×ª×”×œ×™×š ××•××¥ ××›×™×•×•×Ÿ ×©×”×•× ×›×•×œ×œ ×¨×§ ×¡×¨×™×§×” ×‘×¡×¤×¨×™×™×” ×”××§×•×©×¨×ª ×œ-partition state='CA'.\n",
    "\n",
    "### ×”×’×“×¨×” ×”×‘×¢×™×™×ª×™×ª:\n",
    "\n",
    "**Partition ×¢×œ zipcode ××™× ×• ××¤×©×¨×•×ª ×˜×•×‘×” ××›×™×•×•×Ÿ ×©×™×™×ª×›×Ÿ ×©×ª×¡×™×™× ×¢× ×™×•×ª×¨ ××“×™ partitions.**\n",
    "\n",
    "### ×“×•×’××” ×˜×•×‘×”:\n",
    "\n",
    "**×“×•×’××” ×˜×•×‘×” ××—×¨×ª ×©×œ partition ×”×™× ×‘×¢××•×“×ª ×”×ª××¨×™×š. ×‘××•×¤×Ÿ ××™×“×™××œ×™, ×¢×œ×™×š ×œ×‘×¦×¢ partition ×œ×¤×™ ×©× ×”/×—×•×“×© ××š ×œ× ×œ×¤×™ ×ª××¨×™×š.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â“ ×©××œ×•×ª × ×¤×•×¦×•×ª ×¢×œ partitionBy()\n",
    "\n",
    "#### 1. ××” ×”×”×‘×“×œ ×‘×™×Ÿ partitionBy() ×œ-groupBy() ×‘-PySpark?\n",
    "\n",
    "**`partitionBy()`** ××©××© ×œ××¨×’×•×Ÿ ×¤×™×–×™ ×©×œ ×”× ×ª×•× ×™× ×¢×œ ×”×“×™×¡×§ ×‘×¢×ª ×›×ª×™×‘×” ×œ××¢×¨×›×ª ×§×‘×¦×™×, ×‘×¢×•×“ **`groupBy()`** ××©××© ×œ×§×™×‘×•×¥ ×œ×•×’×™ ×©×œ × ×ª×•× ×™× ×‘×ª×•×š DataFrame.\n",
    "\n",
    "#### 2. ×”×× × ×™×ª×Ÿ ×œ×”×©×ª××© ×‘×¢××•×“×•×ª ××¨×•×‘×•×ª ×¢× partitionBy()?\n",
    "\n",
    "×›×Ÿ, × ×™×ª×Ÿ ×œ×¦×™×™×Ÿ ×¢××•×“×•×ª ××¨×•×‘×•×ª ×‘×¤×•× ×§×¦×™×” `partitionBy()` ×›×“×™ ×œ×™×¦×•×¨ ××‘× ×” ×ª×™×§×™×•×ª ×”×™×¨×¨×›×™. ×œ×“×•×’××”:\n",
    "\n",
    "```python\n",
    "df.write.partitionBy(\"column1\", \"column2\").parquet(\"/path/to/output\")\n",
    "```\n",
    "\n",
    "#### 3. ×›×™×¦×“ partitioning ××©×¤×™×¢ ×¢×œ ×‘×™×¦×•×¢×™ ×©××™×œ×ª×•×ª?\n",
    "\n",
    "Partitioning ×™×›×•×œ ×œ×©×¤×¨ ××©××¢×•×ª×™×ª ××ª ×‘×™×¦×•×¢×™ ×”×©××™×œ×ª×”, ×‘××™×•×—×“ ×›××©×¨ ×©×•××œ×™× ×ª×ª×™ ×§×‘×•×¦×•×ª ×¡×¤×¦×™×¤×™×•×ª ×©×œ × ×ª×•× ×™×. ×–×” ×¢×•×–×¨ ×œ×“×œ×’ ×¢×œ × ×ª×•× ×™× ×œ× ×¨×œ×•×•× ×˜×™×™× ×‘×¢×ª ×§×¨×™××”, ×•××§×˜×™×Ÿ ××ª ×›××•×ª ×”× ×ª×•× ×™× ×©×¦×¨×™×š ×œ×¢×‘×“."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ×¡×™×›×•× - PartitionBy\n",
    "\n",
    "×‘×–××Ÿ ×©××ª×” ×™×•×¦×¨ Data Lake ×-Azure, HDFS ××• AWS, ××ª×” ×¦×¨×™×š ×œ×”×‘×™×Ÿ ×›×™×¦×“ ×œ×—×œ×§ ××ª ×”× ×ª×•× ×™× ×©×œ×š ×‘×× ×•×—×” (××¢×¨×›×ª ×§×‘×¦×™×/×“×™×¡×§). PySpark `partitionBy()` ×•-`repartition()` ×¢×•×–×¨×™× ×œ×š ×œ×—×œ×§ ××ª ×”× ×ª×•× ×™× ×•×œ×‘×˜×œ ××ª ×”-Data Skew ×¢×œ ××¢×¨×›×™ ×”× ×ª×•× ×™× ×”×’×“×•×œ×™× ×©×œ×š.\n",
    "\n",
    "×× ×™ ××§×•×•×” ×©×–×” × ×•×ª×Ÿ ×œ×š ×¨×¢×™×•×Ÿ ×˜×•×‘ ×™×•×ª×¨ ×¢×œ partitions ×‘-PySpark.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ºï¸ ×—×œ×§ 3: PySpark MapType (Dict) - ×©×™××•×© ×•×“×•×’×××•×ª\n",
    "\n",
    "### ××”×• PySpark MapType?\n",
    "\n",
    "PySpark `MapType` (× ×§×¨× ×’× ×¡×•×’ map) ×”×•× ×¡×•×’ × ×ª×•× ×™× ×œ×™×™×¦×•×’ Python Dictionary (`dict`) ×œ××—×¡×•×Ÿ ×–×•×’ key-value. ××•×‘×™×™×§×˜ MapType ×›×•×œ×œ ×©×œ×•×©×” ×©×“×•×ª:\n",
    "\n",
    "- `keyType` - DataType\n",
    "- `valueType` - DataType  \n",
    "- `valueContainsNull` - Boolean (×‘×¨×™×¨×ª ××—×“×œ True)\n",
    "\n",
    "### ×”×¨×—×‘×ª ××—×œ×§×ª DataType\n",
    "\n",
    "PySpark MapType ××©××© ×œ×™×™×¦×•×’ ×–×•×’ key-value ×“×•××” ×œ-Python Dictionary (Dict). ×”×•× ××¨×—×™×‘ ××ª ××—×œ×§×ª DataType ×©×”×™× superclass ×©×œ ×›×œ ×”×¡×•×’×™× ×‘-PySpark ×•×œ×•×§×— ×©× ×™ ××¨×’×•×× ×˜×™× ×—×•×‘×”:\n",
    "\n",
    "- `keyType` - ×¡×•×’ ×”××¤×ª×—\n",
    "- `valueType` - ×¡×•×’ ×”×¢×¨×š\n",
    "\n",
    "×•××¨×’×•×× ×˜ ×‘×•×œ×™×× ×™ ××•×¤×¦×™×•× ×œ×™ ××—×“:\n",
    "\n",
    "- `valueContainsNull` (×‘×¨×™×¨×ª ××—×“×œ True)\n",
    "\n",
    "`keyType` ×•-`valueType` ×™×›×•×œ×™× ×œ×”×™×•×ª ××›×œ ×¡×•×’ ×©××¨×—×™×‘ ××ª DataType. ×œ×“×•×’××”: `StringType`, `IntegerType`, `ArrayType`, `MapType`, `StructType` (struct) ×•×›×•'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš™ï¸ ×™×¦×™×¨×ª PySpark MapType\n",
    "\n",
    "×›×“×™ ×œ×”×©×ª××© ×‘×¡×•×’ × ×ª×•× ×™× MapType ×ª×—×™×œ×”, ×¢×œ×™×š ×œ×™×™×‘× ××•×ª×• ×-`pyspark.sql.types.MapType` ×•×œ×”×©×ª××© ×‘×‘× ××™ `MapType()` ×œ×™×¦×™×¨×ª ××•×‘×™×™×§×˜ map.\n",
    "\n",
    "#### × ×§×•×“×•×ª ××¤×ª×—:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ×”×¤×¨××˜×¨ ×”×¨××©×•×Ÿ `keyType` ××©××© ×œ×¦×™×•×Ÿ ×¡×•×’ ×”××¤×ª×— ×‘××¤×”\n",
    "- ×”×¤×¨××˜×¨ ×”×©× ×™ `valueType` ××©××© ×œ×¦×™×•×Ÿ ×¡×•×’ ×”×¢×¨×š ×‘××¤×”\n",
    "- ×”×¤×¨××˜×¨ ×”×©×œ×™×©×™ `valueContainsNull` ×”×•× ×¡×•×’ ×‘×•×œ×™×× ×™ ××•×¤×¦×™×•× ×œ×™ ×”××©××© ×œ×¦×™×•×Ÿ ×× ×”×¢×¨×š ×©×œ ×”×¤×¨××˜×¨ ×”×©× ×™ ×™×›×•×œ ×œ×§×‘×œ ×¢×¨×›×™ `Null/None`\n",
    "- ×”××¤×ª×— ×©×œ ×”××¤×” ×œ× ×™×§×‘×œ ×¢×¨×›×™ `None/Null`\n",
    "- PySpark ××¡×¤×§ ××¡×¤×¨ ×¤×•× ×§×¦×™×•×ª SQL ×œ×¢×‘×•×“×” ×¢× `MapType`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ×“×•×’××”: ×™×¦×™×¨×ª MapType"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×™×‘×•× ×”×¡×¤×¨×™×•×ª ×”× ×“×¨×©×•×ª\n",
    "from pyspark.sql.types import StringType, MapType\n",
    "\n",
    "# ×™×¦×™×¨×ª MapType\n",
    "mapCol = MapType(StringType(), StringType(), False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ—ï¸ ×™×¦×™×¨×ª MapType ×-StructType\n",
    "\n",
    "×‘×•××• × ×¨××” ×›×™×¦×“ ×œ×™×¦×•×¨ MapType ×‘×××¦×¢×•×ª PySpark StructType ×•-StructField. `StructType()` constructor ×œ×•×§×— list ×©×œ StructField. StructField ×œ×•×§×— fieldname ×•×¡×•×’ ×”×¢×¨×š."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×¦×™×¨×ª MapType ×-StructType\n",
    "from pyspark.sql.types import StructField, StructType, StringType, MapType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('properties', MapType(StringType(), StringType()), True)\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×¢×›×©×™×• ×‘×•××• × ×™×¦×•×¨ DataFrame ×‘×××¦×¢×•×ª StructType schema ×œ×¢×™×œ."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×¦×™×¨×ª DataFrame ×¢× MapType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "dataDictionary = [\n",
    "        ('James', {'hair':'black', 'eye':'brown'}),\n",
    "        ('Michael', {'hair':'brown', 'eye':None}),\n",
    "        ('Robert', {'hair':'red', 'eye':'black'}),\n",
    "        ('Washington', {'hair':'grey', 'eye':'grey'}),\n",
    "        ('Jefferson', {'hair':'brown', 'eye':''})  \n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=dataDictionary, schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” ×’×™×©×” ×œ××œ×× ×˜×™× ×©×œ PySpark MapType\n",
    "\n",
    "×‘×•××• × ×¨××” ×›×™×¦×“ ×œ×—×œ×¥ ××ª ×”××¤×ª×— ×•×”×¢×¨×›×™× ××¢××•×“×ª Dictionary ×‘-PySpark DataFrame. ×›××Ÿ ×”×©×ª××©×ª×™ ×‘×˜×¨× ×¡×¤×•×¨××¦×™×” ×©×œ PySpark map ×›×“×™ ×œ×§×¨×•× ××ª ×”×¢×¨×›×™× ×©×œ `properties` (×¢××•×“×ª MapType)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×’×™×©×” ×œ××œ×× ×˜×™× ×©×œ Map\n",
    "df3 = df.rdd.map(lambda x: \\\n",
    "    (x.name, x.properties[\"hair\"], x.properties[\"eye\"])) \\\n",
    "    .toDF([\"name\", \"hair\", \"eye\"])\n",
    "df3.printSchema()\n",
    "df3.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×‘×•××• × ×©×ª××© ×‘×“×¨×š × ×•×¡×¤×ª ×œ×§×‘×œ ××ª ×”×¢×¨×š ×©×œ ××¤×ª×— ×-Map ×‘×××¦×¢×•×ª `getItem()` ×©×œ ×¢××•×“×” Column. ×©×™×˜×” ×–×• ×œ×•×§×—×ª ××¤×ª×— ×›××¨×’×•×× ×˜ ×•××—×–×™×¨×” ×¢×¨×š."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×©×™××•×© ×‘-getItem() ×œ×’×™×©×” ×œ×¢×¨×›×™×\n",
    "df.withColumn(\"hair\", df.properties.getItem(\"hair\")) \\\n",
    "  .withColumn(\"eye\", df.properties.getItem(\"eye\")) \\\n",
    "  .drop(\"properties\") \\\n",
    "  .show()\n",
    "\n",
    "df.withColumn(\"hair\", df.properties[\"hair\"]) \\\n",
    "  .withColumn(\"eye\", df.properties[\"eye\"]) \\\n",
    "  .drop(\"properties\") \\\n",
    "  .show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”§ ×¤×•× ×§×¦×™×•×ª MapType\n",
    "\n",
    "×œ×”×œ×Ÿ ×›××” ××¤×•× ×§×¦×™×•×ª MapType ×¢× ×“×•×’×××•×ª.\n",
    "\n",
    "#### ğŸ¯ explode() - ×”×¨×—×‘×ª Map ×œ×©×•×¨×•×ª\n",
    "\n",
    "×‘×•××• × ×—×™×œ ××ª ×¤×•× ×§×¦×™×” `explode()` ×¢×œ ×¢××•×“×ª map ×‘-DataFrame ×›×“×™ ×œ×”×¨×—×™×‘ ×›×œ ×–×•×’ key-value ×œ×©×•×¨×•×ª × ×¤×¨×“×•×ª."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×©×™××•×© ×‘-explode ×œ×”×¨×—×‘×ª Map\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df.select(df.name, explode(df.properties)).show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ”‘ map_keys() - ×§×‘×œ×ª ×›×œ ××¤×ª×—×•×ª ×”-Map\n",
    "\n",
    "×¤×•× ×§×¦×™×™×ª `map_keys()` ××—×–×™×¨×” ××ª ×›×œ ×”××¤×ª×—×•×ª ××¢××•×“×ª MapType ×›×¨×©×™××”."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×§×‘×œ×ª ×›×œ ××¤×ª×—×•×ª ×”-Map\n",
    "from pyspark.sql.functions import map_keys\n",
    "\n",
    "df.select(df.name, map_keys(df.properties)).show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**××–×”×¨×”:** ×–×” ×¤×•×¢×œ ×œ××˜ ×××•×“.\n",
    "\n",
    "×‘××§×¨×” ×©×ª×¨×¦×” ×œ×§×‘×œ ××ª ×›×œ ××¤×ª×—×•×ª ×”-map ×›×¨×©×™××ª Python:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×§×‘×œ×ª ××¤×ª×—×•×ª ×›×¨×©×™××ª Python\n",
    "from pyspark.sql.functions import explode, map_keys\n",
    "\n",
    "keysDF = df.select(explode(map_keys(df.properties))).distinct()\n",
    "keysList = keysDF.rdd.map(lambda x: x[0]).collect()\n",
    "print(keysList)\n",
    "# ['eye', 'hair']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ’ map_values() - ×§×‘×œ×ª ×›×œ ×¢×¨×›×™ ×”-Map"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×§×‘×œ×ª ×›×œ ×¢×¨×›×™ ×”-Map\n",
    "from pyspark.sql.functions import map_values\n",
    "\n",
    "df.select(df.name, map_values(df.properties)).show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ×¡×™×›×•× - MapType\n",
    "\n",
    "MapType ×”×•× ××‘× ×” × ×ª×•× ×™× ×©×œ map ×”××©××© ×œ××—×¡×•×Ÿ ×–×•×’ key-value ×“×•××” ×œ-Python Dictionary (Dict). ××¤×ª×—×•×ª ×•×¢×¨×›×™× ×©×œ ×¡×•×’ map ×¦×¨×™×›×™× ×œ×”×™×•×ª ××¡×•×’ ×©××¨×—×™×‘ DataType. ××¤×ª×— ×©×œ ×”-map ×œ× ×™×§×‘×œ ×¢×¨×›×™ `null/None`, ×‘×¢×•×“ ×©××¤×” ×©×œ ×”××¤×ª×— ×™×›×•×œ×” ×œ×§×‘×œ ×¢×¨×›×™ `None/Null`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ×¡×™×›×•× ×›×œ×œ×™\n",
    "\n",
    "×‘××“×¨×™×š ×–×” ×œ××“× ×• ×©×œ×•×©×” × ×•×©××™× ×—×©×•×‘×™× ×‘-PySpark:\n",
    "\n",
    "### 1. ğŸ”„ Pivot & Unpivot\n",
    "- ×¡×™×‘×•×‘ × ×ª×•× ×™× ××¢××•×“×•×ª ×œ×©×•×¨×•×ª ×•×œ×”×™×¤×š\n",
    "- ×©×™×¤×•×¨ ×‘×™×¦×•×¢×™× ×¢× ××’×¨×’×¦×™×” ×“×•-×©×œ×‘×™×ª\n",
    "- ×©×™××•×© ×‘-`stack()` ×œ-unpivot\n",
    "\n",
    "### 2. ğŸ’¾ PartitionBy\n",
    "- ×—×œ×•×§×ª × ×ª×•× ×™× ×’×“×•×œ×™× ×œ×§×‘×¦×™× ×§×˜× ×™× ×™×•×ª×¨\n",
    "- ×©×™×¤×•×¨ ×‘×™×¦×•×¢×™ ×©××™×œ×ª×•×ª\n",
    "- ×‘×—×™×¨×” × ×›×•× ×” ×©×œ ×¢××•×“×•×ª partition\n",
    "- ×˜×™×¤×•×œ ×‘-data skew\n",
    "\n",
    "### 3. ğŸ—ºï¸ MapType\n",
    "- ×™×™×¦×•×’ ××‘× ×™ Dictionary ×‘-PySpark\n",
    "- ×’×™×©×” ×œ×¢×¨×›×™× ×•m×¤×ª×—×•×ª\n",
    "- ×¤×•× ×§×¦×™×•×ª ×¢×–×¨ ×›××• `explode()`, `map_keys()`, `map_values()`\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ××©××‘×™× × ×•×¡×¤×™×\n",
    "\n",
    "- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [SparkByExamples.com](https://sparkbyexamples.com)\n",
    "- [Apache Spark GitHub](https://github.com/apache/spark)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ ×˜×™×¤×™× ×œ×¢×‘×•×“×” ×¢× PySpark\n",
    "\n",
    "1. **×ª×›× ×Ÿ ××¨××©** - ×—×©×•×‘ ×¢×œ ××‘× ×” ×”× ×ª×•× ×™× ×•×”×©××™×œ×ª×•×ª ×œ×¤× ×™ ×™×¦×™×¨×ª partitions\n",
    "2. **××“×•×“ ×‘×™×¦×•×¢×™×** - ×”×©×ª××© ×‘-`explain()` ×•-Spark UI ×œ×”×‘× ×ª ×‘×™×¦×•×¢×™ ×”×©××™×œ×ª×•×ª\n",
    "3. **× ×”×œ ××©××‘×™×** - ×©×™× ×œ×‘ ×œ××¡×¤×¨ ×”-partitions ×•×”×’×•×“×œ ×©×œ×”×\n",
    "4. **×‘×“×•×§ ××ª ×”× ×ª×•× ×™×** - ×”×©×ª××© ×‘-`show()`, `printSchema()` ×•-`describe()` ×œ×‘×“×™×§×ª ×”× ×ª×•× ×™×\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Learning! ğŸ‰**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
