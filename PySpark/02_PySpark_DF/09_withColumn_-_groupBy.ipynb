{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    body, * {\n",
    "        direction: rtl !important;\n",
    "        text-align: right !important;\n",
    "    }\n",
    "</style>\n",
    "\n",
    " # ğŸ“š ××“×¨×™×š PySpark ××§×™×£ - ×¢×™×‘×•×“ × ×ª×•× ×™× ×‘-DataFrame\n",
    "\n",
    "××—×‘×¨×ª ×–×• ×›×•×œ×œ×ª ××ª ×”× ×•×©××™× ×”×‘××™×:\n",
    "1. âœï¸ **withColumn** - ×”×•×¡×¤×” ×•×©×™× ×•×™ ×¢××•×“×•×ª -  [withColumn](https://sparkbyexamples.com/pyspark/pyspark-withcolumn/)\n",
    "\n",
    "2. ğŸ·ï¸ **withColumnRenamed** - ×©×™× ×•×™ ×©××•×ª ×¢××•×“×•×ª -  [withColumnRenamed](https://sparkbyexamples.com/pyspark/pyspark-rename-dataframe-column/)\n",
    "\n",
    "3. ğŸ” **filter/where** - ×¡×™× ×•×Ÿ × ×ª×•× ×™× -  [filter/where](https://sparkbyexamples.com/pyspark/pyspark-where-filter/)\n",
    "\n",
    "4. ğŸ—‘ï¸ **distinct** - ×”×¡×¨×ª ×›×¤×™×œ×•×™×•×ª -  [distinct](https://sparkbyexamples.com/pyspark/pyspark-distinct-to-drop-duplicates/)\n",
    "\n",
    "5. ğŸ“Š **orderBy/sort** - ××™×•×Ÿ × ×ª×•× ×™× -  [orderBy/sort](https://sparkbyexamples.com/pyspark/pyspark-orderby-and-sort-explained/)\n",
    "\n",
    "6. ğŸ“ˆ **groupBy** - ×§×™×‘×•×¥ ×•×¦×‘×™×¨×” -  [groupBy](https://sparkbyexamples.com/pyspark/pyspark-groupby-explained-with-example/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ ×”×’×“×¨×ª ×¡×‘×™×‘×ª ×”×¢×‘×•×“×”\n",
    "\n",
    "×¨××©×™×ª, × ×™×™×‘× ××ª ×”×¡×¤×¨×™×•×ª ×”× ×“×¨×©×•×ª ×•× ×™×¦×•×¨ Spark Session:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, expr, upper, lower, concat_ws\n",
    "from pyspark.sql.functions import sum, avg, max, min, count, mean\n",
    "from pyspark.sql.functions import array_contains\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "\n",
    "# ×™×¦×™×¨×ª Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Tutorial\") \\\n",
    "    .getOrCreate()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ ×™×¦×™×¨×ª × ×ª×•× ×™ ×“×•×’××”\n",
    "\n",
    "× ×™×¦×•×¨ DataFrame ×œ×“×•×’××” ×¢× ××™×“×¢ ×¢×œ ×¢×•×‘×“×™×:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# × ×ª×•× ×™× ×¨××©×•× ×™×™×\n",
    "data = [\n",
    "    ('James', 'Sales', 'NY', 90000, 34, 10000),\n",
    "    ('Michael', 'Sales', 'NY', 86000, 56, 20000),\n",
    "    ('Robert', 'Sales', 'CA', 81000, 30, 23000),\n",
    "    ('Maria', 'Finance', 'CA', 90000, 24, 23000),\n",
    "    ('Raman', 'Finance', 'CA', 99000, 40, 24000),\n",
    "    ('Scott', 'Finance', 'NY', 83000, 36, 19000),\n",
    "    ('Jen', 'Finance', 'NY', 79000, 53, 15000),\n",
    "    ('Jeff', 'Marketing', 'CA', 80000, 25, 18000),\n",
    "    ('Kumar', 'Marketing', 'NY', 91000, 50, 21000)\n",
    "]\n",
    "\n",
    "# ×”×’×“×¨×ª ×¢××•×“×•×ª\n",
    "columns = ['employee_name', 'department', 'state', 'salary', 'age', 'bonus']\n",
    "\n",
    "# ×™×¦×™×¨×ª DataFrame\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1ï¸âƒ£ withColumn() - ×”×•×¡×¤×” ×•×©×™× ×•×™ ×¢××•×“×•×ª\n",
    "\n",
    "## ğŸ“– ××” ×–×” withColumn?\n",
    "\n",
    "`withColumn()` ×”×™× ×¤×•× ×§×¦×™×” ××¨×›×–×™×ª ×‘-PySpark ×”××©××©×ª ×œ:\n",
    "- ğŸ†• ×”×•×¡×¤×ª ×¢××•×“×” ×—×“×©×”\n",
    "- â™»ï¸ ×¢×“×›×•×Ÿ ×¢××•×“×” ×§×™×™××ª\n",
    "- ğŸ”„ ×©×™× ×•×™ ×˜×™×¤×•×¡ × ×ª×•× ×™×\n",
    "- ğŸ§® ×™×¦×™×¨×ª ×¢××•×“×•×ª ××—×•×©×‘×•×ª\n",
    "\n",
    "**×—×©×•×‘ ×œ×“×¢×ª:** ×”×¤×•× ×§×¦×™×” ××—×–×™×¨×” DataFrame ×—×“×© ×•×œ× ××©× ×” ××ª ×”××§×•×¨×™!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Syntax - ×ª×—×‘×™×¨\n",
    "\n",
    "```python\n",
    "df.withColumn(colName, col)\n",
    "```\n",
    "\n",
    "- **colName**: ×©× ×”×¢××•×“×” (×—×“×©×” ××• ×§×™×™××ª)\n",
    "- **col**: ×‘×™×˜×•×™ Column ××• ×¢×¨×š ×œ×”×•×¡×¤×”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¢ ×“×•×’××” 1: ×©×™× ×•×™ ×˜×™×¤×•×¡ × ×ª×•× ×™×\n",
    "\n",
    "× ×©× ×” ××ª ×¢××•×“×ª `salary` ×-Integer ×œ-String:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”××¨×ª salary ×œ-String\n",
    "df2 = df.withColumn(\"salary\", col(\"salary\").cast(\"String\"))\n",
    "df2.printSchema()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â• ×“×•×’××” 2: ×¢×“×›×•×Ÿ ×¢×¨×›×™ ×¢××•×“×”\n",
    "\n",
    "× ×›×¤×™×œ ××ª ×”××©×›×•×¨×ª ×‘-100:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df3 = df.withColumn(\"salary\", col(\"salary\") * 100)\n",
    "df3.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ†• ×“×•×’××” 3: ×™×¦×™×¨×ª ×¢××•×“×” ×—×“×©×” ××¢××•×“×” ×§×™×™××ª\n",
    "\n",
    "× ×™×¦×•×¨ ×¢××•×“×” ×—×“×©×” ×‘×©× `CopiedColumn`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df4 = df.withColumn(\"CopiedColumn\", col(\"salary\") * -1)\n",
    "df4.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¤ ×“×•×’××” 4: ×©×™××•×© ×‘×¤×•× ×§×¦×™×•×ª ××•×‘× ×•×ª\n",
    "\n",
    "× ×©×ª××© ×‘×¤×•× ×§×¦×™×•×ª `upper()` ×•-`lower()`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”××¨×” ×œ××•×ª×™×•×ª ×’×“×•×œ×•×ª\n",
    "df5 = df.withColumn(\"department_upper\", upper(col(\"department\")))\n",
    "df5.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”— ×“×•×’××” 5: ×”×•×¡×¤×ª ××¡×¤×¨ ×¢××•×“×•×ª\n",
    "\n",
    "××¤×©×¨ ×œ×©×¨×©×¨ ××¡×¤×¨ ×¤×¢×•×œ×•×ª `withColumn()`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df6 = df.withColumn(\"Country\", lit(\"USA\")) \\\n",
    "        .withColumn(\"anotherColumn\", lit(\"anotherValue\"))\n",
    "df6.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§® ×“×•×’××” 6: ×™×¦×™×¨×ª ×¢××•×“×” ××—×•×©×‘×ª\n",
    "\n",
    "× ×—×©×‘ ×¡×š ×”×›× ×¡×” (××©×›×•×¨×ª + ×‘×•× ×•×¡):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df7 = df.withColumn(\"total_income\", col(\"salary\") + col(\"bonus\"))\n",
    "df7.select(\"employee_name\", \"salary\", \"bonus\", \"total_income\").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2ï¸âƒ£ withColumnRenamed() - ×©×™× ×•×™ ×©××•×ª ×¢××•×“×•×ª\n",
    "\n",
    "## ğŸ·ï¸ ×¡×§×™×¨×” ×›×œ×œ×™×ª\n",
    "\n",
    "`withColumnRenamed()` ××©××©×ª ×œ×©×™× ×•×™ ×©× ×¢××•×“×” ×§×™×™××ª ×‘×¦×•×¨×” ×¤×©×•×˜×” ×•×‘×¨×•×¨×”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Syntax - ×ª×—×‘×™×¨\n",
    "\n",
    "```python\n",
    "df.withColumnRenamed(existingName, newName)\n",
    "```\n",
    "\n",
    "- **existingName**: ×©× ×”×¢××•×“×” ×”× ×•×›×—×™\n",
    "- **newName**: ×”×©× ×”×—×“×©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ ×“×•×’××” 1: ×©×™× ×•×™ ×©× ×¢××•×“×” ××—×ª"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_renamed = df.withColumnRenamed(\"employee_name\", \"name\")\n",
    "df_renamed.printSchema()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”— ×“×•×’××” 2: ×©×™× ×•×™ ×©××•×ª ××¡×¤×¨ ×¢××•×“×•×ª\n",
    "\n",
    "××¤×©×¨ ×œ×©×¨×©×¨ ××¡×¤×¨ ×©×™× ×•×™×™×:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_renamed2 = df.withColumnRenamed(\"employee_name\", \"name\") \\\n",
    "                .withColumnRenamed(\"salary\", \"salary_amount\")\n",
    "df_renamed2.printSchema()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ ×“×•×’××” 3: ×©×™××•×© ×‘-toDF() ×œ×©×™× ×•×™ ×›×œ ×”×©××•×ª"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×©×™× ×•×™ ×©××•×ª ×›×œ ×”×¢××•×“×•×ª ×‘×‘×ª ××—×ª\n",
    "newColumns = ['emp_name', 'dept', 'location', 'sal', 'emp_age', 'emp_bonus']\n",
    "df_renamed_all = df.toDF(*newColumns)\n",
    "df_renamed_all.printSchema()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3ï¸âƒ£ filter() / where() - ×¡×™× ×•×Ÿ × ×ª×•× ×™×\n",
    "\n",
    "## ğŸ” ×¡×§×™×¨×” ×›×œ×œ×™×ª\n",
    "\n",
    "×”×¤×•× ×§×¦×™×•×ª `filter()` ×•-`where()` ××©××©×•×ª ×œ×¡×™× ×•×Ÿ ×©×•×¨×•×ª ×‘-DataFrame ×¢×œ ×¤×™ ×ª× ××™× ××¡×•×™××™×.\n",
    "\n",
    "**×©×ª×™ ×”×¤×•× ×§×¦×™×•×ª ×–×”×•×ª ×œ×—×œ×•×˜×™×Ÿ** - ×”×©×ª××© ×‘×–×• ×©× ×•×—×” ×œ×š ×™×•×ª×¨!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Syntax - ×ª×—×‘×™×¨\n",
    "\n",
    "```python\n",
    "df.filter(condition)\n",
    "df.where(condition)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ×™×¦×™×¨×ª DataFrame ×œ×“×•×’×××•×ª ×”×¡×™× ×•×Ÿ\n",
    "\n",
    "× ×™×¦×•×¨ DataFrame ×¢× ××‘× ×” ××•×¨×›×‘ ×™×•×ª×¨:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”×’×“×¨×ª Schema\n",
    "schema = StructType([\n",
    "    StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "        StructField('lastname', StringType(), True)\n",
    "    ])),\n",
    "    StructField('languages', ArrayType(StringType()), True),\n",
    "    StructField('state', StringType(), True),\n",
    "    StructField('gender', StringType(), True)\n",
    "])\n",
    "\n",
    "# × ×ª×•× ×™×\n",
    "data = [\n",
    "    (('James', '', 'Smith'), ['Java', 'Scala', 'C++'], 'OH', 'M'),\n",
    "    (('Anna', 'Rose', ''), ['Spark', 'Java', 'C++'], 'NY', 'F'),\n",
    "    (('Julia', '', 'Williams'), ['CSharp', 'VB'], 'OH', 'F'),\n",
    "    (('Maria', 'Anne', 'Jones'), ['CSharp', 'VB'], 'NY', 'M'),\n",
    "    (('Jen', 'Mary', 'Brown'), ['CSharp', 'VB'], 'NY', 'M'),\n",
    "    (('Mike', 'Mary', 'Williams'), ['Python', 'VB'], 'OH', 'M')\n",
    "]\n",
    "\n",
    "df_filter = spark.createDataFrame(data=data, schema=schema)\n",
    "df_filter.printSchema()\n",
    "df_filter.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŸ° ×“×•×’××” 1: ×¡×™× ×•×Ÿ ×¢× ×ª× ××™ ×©×•×•×™×•×Ÿ\n",
    "\n",
    "× ×¡× ×Ÿ ×¨×§ ×¢×•×‘×“×™× ×××“×™× ×ª OH:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×©×™××•×© ×‘-filter\n",
    "df_filter.filter(df_filter.state == \"OH\").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â‰  ×“×•×’××” 2: ×¡×™× ×•×Ÿ ×¢× ×ª× ××™ ×©×•× ×” ×-\n",
    "\n",
    "× ×¡× ×Ÿ ××ª ×›×œ ××™ ×©××™× × ×××“×™× ×ª OH:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ××•×¤×Ÿ 1: ×©×™××•×© ×‘-!=\n",
    "df_filter.filter(df_filter.state != \"OH\").show(truncate=False)\n",
    "\n",
    "# ××•×¤×Ÿ 2: ×©×™××•×© ×‘-~\n",
    "df_filter.filter(~(df_filter.state == \"OH\")).show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¢ ×“×•×’××” 3: ×©×™××•×© ×‘-col()\n",
    "\n",
    "××¤×©×¨ ×œ×”×©×ª××© ×‘×¤×•× ×§×¦×™×” `col()` ×œ×”×ª×™×™×—×¡×•×ª ×œ×¢××•×“×•×ª:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_filter.filter(col(\"state\") == \"OH\").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“œ ×“×•×’××” 4: ×©×™××•×© ×‘×‘×™×˜×•×™ SQL\n",
    "\n",
    "××¤×©×¨ ×’× ×œ×”×©×ª××© ×‘×ª×—×‘×™×¨ SQL:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×¡×™× ×•×Ÿ ×œ×¤×™ ××™×Ÿ\n",
    "df_filter.filter(\"gender == 'M'\").show()\n",
    "\n",
    "# ××™ ×©×•×•×™×•×Ÿ\n",
    "df_filter.filter(\"gender != 'M'\").show()\n",
    "df_filter.filter(\"gender <> 'M'\").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”€ ×“×•×’××” 5: ××¡×¤×¨ ×ª× ××™× (AND)\n",
    "\n",
    "×¡×™× ×•×Ÿ ×œ×¤×™ ××¡×¤×¨ ×ª× ××™× ×¢× ××•×¤×¨×˜×•×¨ AND (`&`):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×ª× ××™× ×¢× &\n",
    "df_filter.filter((df_filter.state == \"OH\") & (df_filter.gender == \"M\")).show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” ×“×•×’××” 6: ×ª× ××™ OR\n",
    "\n",
    "×œ×”×©×ª××© ×‘××•×¤×¨×˜×•×¨ OR (`|`):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_filter.filter((df_filter.state == \"OH\") | (df_filter.gender == \"M\")).show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ ×“×•×’××” 7: ×¡×™× ×•×Ÿ ×œ×¤×™ ×¨×©×™××ª ×¢×¨×›×™× (isin)\n",
    "\n",
    "×‘×“×™×§×” ×× ×¢×¨×š × ××¦× ×‘×¨×©×™××”:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×¡×™× ×•×Ÿ ××“×™× ×•×ª OH, CA, DE\n",
    "states_list = ['OH', 'CA', 'DE']\n",
    "df_filter.filter(df_filter.state.isin(states_list)).show()\n",
    "\n",
    "# ×©×œ×™×œ×” - NOT IN\n",
    "df_filter.filter(~df_filter.state.isin(states_list)).show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¤ ×“×•×’××” 8: ×¡×™× ×•×Ÿ ×¢× startsWith, endsWith, contains"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×©××•×ª ×©××ª×—×™×œ×™× ×‘-M\n",
    "df_filter.filter(df_filter['name.firstname'].startswith(\"M\")).show()\n",
    "\n",
    "# ×©××•×ª ×©××¡×ª×™×™××™× ×‘-s\n",
    "df_filter.filter(df_filter['name.lastname'].endswith(\"s\")).show()\n",
    "\n",
    "# ××›×™×œ ××ª ×”××•×ª 'o'\n",
    "df_filter.filter(df_filter['name.firstname'].contains(\"o\")).show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¨ ×“×•×’××” 9: ×¡×™× ×•×Ÿ ×¢× Regular Expression (like, rlike)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×©××•×ª ×©××›×™×œ×™× 'Rose'\n",
    "df_filter.filter(df_filter['name.firstname'].like(\"%rose%\")).show()\n",
    "\n",
    "# rlike ×¢× regex\n",
    "df_filter.filter(df_filter['name.firstname'].rlike(\"(?i)^*rose$\")).show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ×“×•×’××” 10: ×¡×™× ×•×Ÿ ×¢××•×“×ª Array\n",
    "\n",
    "×©×™××•×© ×‘-`array_contains()` ×œ×‘×“×™×§×ª ×§×™×•× ×¢×¨×š ×‘××¢×¨×š:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "df_filter.filter(array_contains(df_filter.languages, \"Java\")).show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒ³ ×“×•×’××” 11: ×¡×™× ×•×Ÿ ×¢××•×“×•×ª ××§×•× × ×•×ª (Nested)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_filter.filter(df_filter['name.lastname'] == \"Williams\").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4ï¸âƒ£ distinct() - ×”×¡×¨×ª ×›×¤×™×œ×•×™×•×ª\n",
    "\n",
    "## ğŸ—‘ï¸ ×¡×§×™×¨×” ×›×œ×œ×™×ª\n",
    "\n",
    "`distinct()` ××©××©×ª ×œ×”×¡×¨×ª ×©×•×¨×•×ª ×›×¤×•×œ×•×ª ×-DataFrame.\n",
    "\n",
    "**×—×œ×•×¤×”:** `dropDuplicates()` - ×××¤×©×¨×ª ×’××™×©×•×ª ×¨×‘×” ×™×•×ª×¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ×™×¦×™×¨×ª DataFrame ×¢× ×›×¤×™×œ×•×™×•×ª"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# × ×ª×•× ×™× ×¢× ×›×¤×™×œ×•×™×•×ª\n",
    "data_dup = [\n",
    "    ('James', 'Sales', 3000),\n",
    "    ('Michael', 'Sales', 4600),\n",
    "    ('Robert', 'Sales', 4100),\n",
    "    ('Maria', 'Finance', 3000),\n",
    "    ('James', 'Sales', 3000),\n",
    "    ('Scott', 'Finance', 3300),\n",
    "    ('Jen', 'Finance', 3900),\n",
    "    ('Jeff', 'Marketing', 3000),\n",
    "    ('Kumar', 'Marketing', 2000),\n",
    "    ('Saif', 'Sales', 4100)\n",
    "]\n",
    "\n",
    "columns = ['employee_name', 'department', 'salary']\n",
    "df_dup = spark.createDataFrame(data=data_dup, schema=columns)\n",
    "df_dup.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¹ ×“×•×’××” 1: ×”×¡×¨×ª ×©×•×¨×•×ª ×›×¤×•×œ×•×ª (×›×œ ×”×¢××•×“×•×ª)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "distinctDF = df_dup.distinct()\n",
    "print(f\"Distinct count: {distinctDF.count()}\")\n",
    "distinctDF.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ×“×•×’××” 2: ×”×¡×¨×ª ×›×¤×™×œ×•×™×•×ª ×‘×¢××•×“×•×ª × ×‘×—×¨×•×ª\n",
    "\n",
    "×©×™××•×© ×‘-`dropDuplicates()` ×¢× ×©××•×ª ×¢××•×“×•×ª:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”×¡×¨×ª ×›×¤×™×œ×•×™×•×ª ×œ×¤×™ department ×•-salary\n",
    "dropDisDF = df_dup.dropDuplicates(['department', 'salary'])\n",
    "print(f\"Distinct count of department & salary: {dropDisDF.count()}\")\n",
    "dropDisDF.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5ï¸âƒ£ orderBy() / sort() - ××™×•×Ÿ × ×ª×•× ×™×\n",
    "\n",
    "## ğŸ“Š ×¡×§×™×¨×” ×›×œ×œ×™×ª\n",
    "\n",
    "×”×¤×•× ×§×¦×™×•×ª `orderBy()` ×•-`sort()` ××©××©×•×ª ×œ××™×•×Ÿ ×©×•×¨×•×ª ×‘-DataFrame.\n",
    "\n",
    "**×©×ª×™ ×”×¤×•× ×§×¦×™×•×ª ×–×”×•×ª!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Syntax - ×ª×—×‘×™×¨\n",
    "\n",
    "```python\n",
    "df.sort(*cols, **kwargs)\n",
    "df.orderBy(*cols, **kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â¬†ï¸ ×“×•×’××” 1: ××™×•×Ÿ ×‘×¡×“×¨ ×¢×•×œ×” (ASC)\n",
    "\n",
    "××™×•×Ÿ ×œ×¤×™ department:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×©×œ×•×© ×“×¨×›×™× ×–×”×•×ª:\n",
    "df.sort(\"department\", \"state\").show(truncate=False)\n",
    "df.sort(col(\"department\"), col(\"state\")).show(truncate=False)\n",
    "df.orderBy(\"department\", \"state\").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â¬‡ï¸ ×“×•×’××” 2: ××™×•×Ÿ ×‘×¡×“×¨ ×™×•×¨×“ (DESC)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import asc, desc\n",
    "\n",
    "df.sort(df.department.asc(), df.state.desc()).show(truncate=False)\n",
    "df.sort(col(\"department\").asc(), col(\"state\").desc()).show(truncate=False)\n",
    "df.orderBy(col(\"department\").asc(), col(\"state\").desc()).show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ ×“×•×’××” 3: ××™×•×Ÿ ×¢× SQL\n",
    "\n",
    "×©×™××•×© ×‘-Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.createOrReplaceTempView(\"EMP\")\n",
    "spark.sql(\"SELECT employee_name, department, state, salary, age, bonus FROM EMP ORDER BY department ASC\").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6ï¸âƒ£ groupBy() - ×§×™×‘×•×¥ ×•×¦×‘×™×¨×”\n",
    "\n",
    "## ğŸ“ˆ ×¡×§×™×¨×” ×›×œ×œ×™×ª\n",
    "\n",
    "`groupBy()` ×”×™× ×¤×•× ×§×¦×™×” ××¨×›×–×™×ª ×œ×‘×™×¦×•×¢ ×¤×¢×•×œ×•×ª ×¦×‘×™×¨×” (aggregation) ×¢×œ × ×ª×•× ×™×.\n",
    "\n",
    "×“×•××” ×œ-`GROUP BY` ×‘-SQL!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Syntax - ×ª×—×‘×™×¨\n",
    "\n",
    "```python\n",
    "df.groupBy(*cols)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š ×¤×•× ×§×¦×™×•×ª ×¦×‘×™×¨×” × ×¤×•×¦×•×ª\n",
    "\n",
    "| ×¤×•× ×§×¦×™×” | ×ª×™××•×¨ |\n",
    "|---------|-------|\n",
    "| `count()` | ×¡×¤×™×¨×ª ×©×•×¨×•×ª |\n",
    "| `sum()` | ×¡×›×•× |\n",
    "| `avg()` / `mean()` | ×××•×¦×¢ |\n",
    "| `min()` | ××™× ×™××•× |\n",
    "| `max()` | ××§×¡×™××•× |\n",
    "| `agg()` | ××¡×¤×¨ ×¦×‘×™×¨×•×ª ×‘×•-×–×× ×™×ª |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â• ×“×•×’××” 1: ×¡×›×•× ×œ×¤×™ ×¢××•×“×”\n",
    "\n",
    "×—×™×©×•×‘ ×¡×›×•× ××©×›×•×¨×•×ª ×œ×¤×™ department:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.groupBy(\"department\").sum(\"salary\").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¢ ×“×•×’××” 2: ×¡×¤×™×¨×ª ×¢×•×‘×“×™× ×œ×¤×™ ××—×œ×§×”"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.groupBy(\"department\").count().show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‰ ×“×•×’××” 3: ××™× ×™××•× ××©×›×•×¨×ª"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.groupBy(\"department\").min(\"salary\").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ ×“×•×’××” 4: ××§×¡×™××•× ××©×›×•×¨×ª"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.groupBy(\"department\").max(\"salary\").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š ×“×•×’××” 5: ×××•×¦×¢"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.groupBy(\"department\").avg(\"salary\").show()\n",
    "df.groupBy(\"department\").mean(\"salary\").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”— ×“×•×’××” 6: ×§×™×‘×•×¥ ×œ×¤×™ ××¡×¤×¨ ×¢××•×“×•×ª"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.groupBy(\"department\", \"state\") \\\n",
    "  .sum(\"salary\", \"bonus\") \\\n",
    "  .show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ×“×•×’××” 7: ×©×™××•×© ×‘-agg() ×œ××¡×¤×¨ ×¦×‘×™×¨×•×ª\n",
    "\n",
    "`agg()` ×××¤×©×¨×ª ×‘×™×¦×•×¢ ××¡×¤×¨ ×¤×¢×•×œ×•×ª ×¦×‘×™×¨×” ×‘×•-×–×× ×™×ª:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.groupBy(\"department\") \\\n",
    "  .agg(\n",
    "      sum(\"salary\").alias(\"sum_salary\"),\n",
    "      avg(\"salary\").alias(\"avg_salary\"),\n",
    "      sum(\"bonus\").alias(\"sum_bonus\"),\n",
    "      max(\"bonus\").alias(\"max_bonus\")\n",
    "  ) \\\n",
    "  .show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” ×“×•×’××” 8: ×¡×™× ×•×Ÿ ××—×¨×™ ×§×™×‘×•×¥ (HAVING)\n",
    "\n",
    "×©×™××•×© ×‘-`where()` ××—×¨×™ `groupBy()` ×“×•××” ×œ-HAVING ×‘-SQL:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.groupBy(\"department\") \\\n",
    "  .agg(\n",
    "      sum(\"salary\").alias(\"sum_salary\"),\n",
    "      avg(\"salary\").alias(\"avg_salary\"),\n",
    "      sum(\"bonus\").alias(\"sum_bonus\"),\n",
    "      max(\"bonus\").alias(\"max_bonus\")\n",
    "  ) \\\n",
    "  .where(col(\"sum_bonus\") >= 50000) \\\n",
    "  .show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ ×“×•×’××” 9: ×©×™××•×© ×‘-SQL\n",
    "\n",
    "×©×™××•×© ×‘-Spark SQL ×¢× GROUP BY:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "sql_string = \"\"\"\n",
    "SELECT \n",
    "    department,\n",
    "    SUM(salary) AS sum_salary,\n",
    "    AVG(salary) AS avg_salary,\n",
    "    SUM(bonus) AS sum_bonus,\n",
    "    MAX(bonus) AS max_bonus\n",
    "FROM employees\n",
    "GROUP BY department\n",
    "HAVING SUM(bonus) >= 50000\n",
    "\"\"\"\n",
    "\n",
    "df2 = spark.sql(sql_string)\n",
    "df2.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ×¡×™×›×•×\n",
    "\n",
    "×‘××“×¨×™×š ×–×” ×œ××“× ×•:\n",
    "\n",
    "1. âœï¸ **withColumn()** - ××™×š ×œ×”×•×¡×™×£, ×œ×©× ×•×ª ×•×œ×¢×“×›×Ÿ ×¢××•×“×•×ª\n",
    "2. ğŸ·ï¸ **withColumnRenamed()** - ×©×™× ×•×™ ×©××•×ª ×¢××•×“×•×ª ×‘×¦×•×¨×” ×¤×©×•×˜×”\n",
    "3. ğŸ” **filter()/where()** - ×¡×™× ×•×Ÿ × ×ª×•× ×™× ×¢× ×ª× ××™× ××’×•×•× ×™×\n",
    "4. ğŸ—‘ï¸ **distinct()** - ×”×¡×¨×ª ×›×¤×™×œ×•×™×•×ª\n",
    "5. ğŸ“Š **orderBy()/sort()** - ××™×•×Ÿ × ×ª×•× ×™×\n",
    "6. ğŸ“ˆ **groupBy()** - ×§×™×‘×•×¥ × ×ª×•× ×™× ×•×‘×™×¦×•×¢ ×¤×¢×•×œ×•×ª ×¦×‘×™×¨×”\n",
    "\n",
    "### ğŸ“š ××©××‘×™× × ×•×¡×¤×™×\n",
    "\n",
    "- [PySpark Official Documentation](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [SparkByExamples.com](https://sparkbyexamples.com/pyspark-tutorial/)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Learning! ğŸš€**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¹ ×¡×’×™×¨×ª Spark Session\n",
    "\n",
    "××œ ×ª×©×›×— ×œ×¡×’×•×¨ ××ª ×”-Spark Session ×‘×¡×•×£ ×”×¢×‘×•×“×”:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# spark.stop()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
