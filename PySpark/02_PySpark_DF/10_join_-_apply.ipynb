{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    body, * {\n",
    "        direction: rtl !important;\n",
    "        text-align: right !important;\n",
    "    }\n",
    "</style>\n",
    "# ğŸ“š ××“×¨×™×š ××§×™×£ ×œ-PySpark: ×¢×‘×•×“×” ×¢× DataFrames\n",
    "\n",
    "××—×‘×¨×ª ×–×• ××›×¡×” ××ª ×”× ×•×©××™× ×”×‘××™×:\n",
    "1. âœ… ×”×—×œ×ª ×¤×•× ×§×¦×™×•×ª ×¢×œ ×¢××•×“×•×ª (Apply Functions) -  [Apply Functions](https://sparkbyexamples.com/pyspark/pyspark-apply-function-to-column/)\n",
    "\n",
    "2. ğŸ”„ ×©×™××•×© ×‘-Transform -  [Transform](https://sparkbyexamples.com/pyspark/pyspark-transform-function/)\n",
    "\n",
    "3. ğŸ¯ ×™×¦×™×¨×ª UDF (User Defined Functions) -  [User Defined Functions](https://sparkbyexamples.com/pyspark/pyspark-udf-user-defined-function/)\n",
    "\n",
    "4. ğŸ”— ××™×—×•×“ DataFrames (Union/UnionByName) -  [Union/Union](https://sparkbyexamples.com/pyspark/pyspark-union-and-unionall/)\n",
    ",  [UnionByName](https://sparkbyexamples.com/pyspark/pyspark-unionbyname/)\n",
    "\n",
    "5. ğŸ¤ ×—×™×‘×•×¨ DataFrames (Joins) -  [Joins](https://sparkbyexamples.com/pyspark/pyspark-join-explained-with-examples/)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ ×”×›× ×” ×•×”×’×“×¨×ª SparkSession\n",
    "\n",
    "× ×ª×—×™×œ ×¢× ×™×¦×™×¨×ª SparkSession ×•-DataFrame ×œ×“×•×’××”:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T15:09:22.403637Z",
     "start_time": "2025-11-09T15:09:22.202880Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ×™×¦×™×¨×ª SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()\n",
    "\n",
    "# ×™×¦×™×¨×ª DataFrame ×œ×“×•×’××”\n",
    "columns = [\"Seqno\", \"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "        (\"2\", \"tracey smith\"),\n",
    "        (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.show(truncate=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1ï¸âƒ£ ×”×—×œ×ª ×¤×•× ×§×¦×™×•×ª ×¢×œ ×¢××•×“×•×ª\n",
    "\n",
    "### ğŸ”¹ ×©×™××•×© ×‘-withColumn()\n",
    "\n",
    "`withColumn()` ×”×™× ×¤×•× ×§×¦×™×™×ª ×˜×¨× ×¡×¤×•×¨××¦×™×” ×”×××¤×©×¨×ª ×œ×”×—×™×œ ×¤×•× ×§×¦×™×” ××•×‘× ×™×ª ××• ××•×ª×××ª ××™×©×™×ª ×¢×œ ×¢××•×“×”."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T15:09:26.660107Z",
     "start_time": "2025-11-09T15:09:25.663264Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import upper\n",
    "\n",
    "# ×”×—×œ×ª ×¤×•× ×§×¦×™×™×ª upper ×¢×œ ×¢××•×“×ª Name\n",
    "df.withColumn(\"Upper_Name\", upper(df.Name)).show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+\n",
      "|Seqno|        Name|  Upper_Name|\n",
      "+-----+------------+------------+\n",
      "|    1|  john jones|  JOHN JONES|\n",
      "|    2|tracey smith|TRACEY SMITH|\n",
      "|    3| amy sanders| AMY SANDERS|\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¹ ×©×™××•×© ×‘-select()\n",
    "\n",
    "`select()` ××©××© ×œ×‘×—×™×¨×ª ×¢××•×“×•×ª, ×•× ×™×ª×Ÿ ×’× ×œ×”×—×™×œ ×¢×œ×™×”×Ÿ ×¤×•× ×§×¦×™×•×ª:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T15:09:27.169198Z",
     "start_time": "2025-11-09T15:09:26.995843Z"
    }
   },
   "source": [
    "# ×‘×—×™×¨×ª ×¢××•×“×•×ª ×•×”×—×œ×ª ×¤×•× ×§×¦×™×”\n",
    "df.select(\"Seqno\", \"Name\", upper(df.Name)).show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+\n",
      "|Seqno|        Name| upper(Name)|\n",
      "+-----+------------+------------+\n",
      "|    1|  john jones|  JOHN JONES|\n",
      "|    2|tracey smith|TRACEY SMITH|\n",
      "|    3| amy sanders| AMY SANDERS|\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¹ ×©×™××•×© ×‘-SQL\n",
    "\n",
    "× ×™×ª×Ÿ ×’× ×œ×”×©×ª××© ×‘-SQL ×›×“×™ ×œ×”×—×™×œ ×¤×•× ×§×¦×™×•×ª:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T15:09:28.196714Z",
     "start_time": "2025-11-09T15:09:27.913559Z"
    }
   },
   "source": [
    "# ×™×¦×™×¨×ª temporary view\n",
    "df.createOrReplaceTempView(\"TAB\")\n",
    "\n",
    "# ×©××™×œ×ª×ª SQL\n",
    "spark.sql(\"select Seqno, Name, UPPER(Name) from TAB\").show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+\n",
      "|Seqno|        Name| upper(Name)|\n",
      "+-----+------------+------------+\n",
      "|    1|  john jones|  JOHN JONES|\n",
      "|    2|tracey smith|TRACEY SMITH|\n",
      "|    3| amy sanders| AMY SANDERS|\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2ï¸âƒ£ ×™×¦×™×¨×ª UDF ××•×ª×× ××™×©×™×ª\n",
    "\n",
    "### ğŸ“ ×©×œ×‘ 1: ×™×¦×™×¨×ª ×¤×•× ×§×¦×™×™×ª Python"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T15:09:28.563667Z",
     "start_time": "2025-11-09T15:09:28.560215Z"
    }
   },
   "source": [
    "# ×¤×•× ×§×¦×™×” ×©×××™×¨×” ××—×¨×•×–×ª ×œ××•×ª×™×•×ª ×’×“×•×œ×•×ª\n",
    "def upperCase(str):\n",
    "    return str.upper()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ×©×œ×‘ 2: ×”××¨×ª ×”×¤×•× ×§×¦×™×” ×œ-UDF"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T15:09:29.619983Z",
     "start_time": "2025-11-09T15:09:29.612647Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# ×™×¦×™×¨×ª UDF\n",
    "upperCaseUDF = udf(lambda z: upperCase(z), StringType())"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ×©×œ×‘ 3: ×©×™××•×© ×‘-UDF\n",
    "\n",
    "#### ×¢× withColumn():"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T15:09:31.357162Z",
     "start_time": "2025-11-09T15:09:30.553932Z"
    }
   },
   "source": [
    "df.withColumn(\"Cureated_Name\", upperCaseUDF(col(\"Name\"))).show(truncate=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------------+\n",
      "|Seqno|Name        |Cureated_Name|\n",
      "+-----+------------+-------------+\n",
      "|1    |john jones  |JOHN JONES   |\n",
      "|2    |tracey smith|TRACEY SMITH |\n",
      "|3    |amy sanders |AMY SANDERS  |\n",
      "+-----+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ×¢× select():"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T15:09:31.681032Z",
     "start_time": "2025-11-09T15:09:31.509941Z"
    }
   },
   "source": [
    "df.select(col(\"Seqno\"), upperCaseUDF(col(\"Name\")).alias(\"Name\")).show(truncate=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |JOHN JONES  |\n",
      "|2    |TRACEY SMITH|\n",
      "|3    |AMY SANDERS |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ×¢× SQL:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T15:09:34.238021Z",
     "start_time": "2025-11-09T15:09:34.016397Z"
    }
   },
   "source": [
    "# ×¨×™×©×•× UDF ×œ×©×™××•×© ×‘-SQL\n",
    "spark.udf.register(\"upperCaseUDF\", upperCaseUDF)\n",
    "\n",
    "df.createOrReplaceTempView(\"TAB\")\n",
    "spark.sql(\"select Seqno, Name, upperCaseUDF(Name) from TAB\").show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------------+\n",
      "|Seqno|        Name|upperCaseUDF(Name)|\n",
      "+-----+------------+------------------+\n",
      "|    1|  john jones|        JOHN JONES|\n",
      "|    2|tracey smith|      TRACEY SMITH|\n",
      "|    3| amy sanders|       AMY SANDERS|\n",
      "+-----+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš ï¸ ×× ×•×˜×¦×™×•×ª - ×“×¨×š ×§×¦×¨×” ×œ×™×¦×™×¨×ª UDF"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T15:09:36.227649Z",
     "start_time": "2025-11-09T15:09:35.988857Z"
    }
   },
   "source": [
    "@udf(returnType=StringType())\n",
    "def upperCase(str):\n",
    "    return str.upper()\n",
    "\n",
    "df.withColumn(\"Cureated_Name\", upperCase(col(\"Name\"))).show(truncate=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------------+\n",
      "|Seqno|Name        |Cureated_Name|\n",
      "+-----+------------+-------------+\n",
      "|1    |john jones  |JOHN JONES   |\n",
      "|2    |tracey smith|TRACEY SMITH |\n",
      "|3    |amy sanders |AMY SANDERS  |\n",
      "+-----+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” ×˜×™×¤×•×œ ×‘-Null Values\n",
    "\n",
    "**×—×©×•×‘!** UDF ×¢×œ×•×œ ×œ×”×—×–×™×¨ ×©×’×™××•×ª ×× ×”×¢××•×“×” ××›×™×œ×” ×¢×¨×›×™ null. ×”× ×” ×“×¨×š ×œ×˜×¤×œ ×‘×›×š:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T15:09:37.192998Z",
     "start_time": "2025-11-09T15:09:37.078529Z"
    }
   },
   "source": [
    "# DataFrame ×¢× null\n",
    "columns = [\"Seqno\", \"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "        (\"2\", \"tracey smith\"),\n",
    "        (\"3\", \"amy sanders\"),\n",
    "        (\"4\", None)]\n",
    "\n",
    "df2 = spark.createDataFrame(data=data, schema=columns)\n",
    "df2.show(truncate=False)\n",
    "df2.createOrReplaceTempView(\"NAME_TABLE2\")\n",
    "\n",
    "# ×“×¨×š 1: ×‘×“×™×§×” ×¤× ×™××™×ª ×‘-UDF\n",
    "# spark.udf.register(\"_nullsafeUDF\", upperCase, StringType())\n",
    "# spark.sql(\"select _nullsafeUDF(Name) from NAME_TABLE2\").show(truncate=False)\n",
    "\n",
    "# ×“×¨×š 2: ×©×™××•×© ×‘-WHERE ×‘-SQL\n",
    "# spark.sql(\"select Seqno, _nullsafeUDF(Name) as Name from NAME_TABLE2 \" +\n",
    "#           \"where Name is not null and _nullsafeUDF(Name) like '%John%'\").show(truncate=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "|4    |NULL        |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš¡ ×‘×™×¦×•×¢×™× - UDF vs ×¤×•× ×§×¦×™×•×ª ××•×‘× ×•×ª\n",
    "\n",
    "**×—×©×•×‘ ×œ×“×¢×ª:** UDF ×”×•× ×ª×”×œ×™×š ×™×§×¨ ×‘×™×•×ª×¨ ××‘×—×™× ×ª ×‘×™×¦×•×¢×™×! ×›×“××™ ×ª××™×“ ×œ×”×©×ª××© ×‘×¤×•× ×§×¦×™×•×ª ×”××•×‘× ×•×ª ×©×œ PySpark SQL ×›×©××¤×©×¨.\n",
    "\n",
    "×”-UDF ×”×•× \"×§×•×¤×¡×” ×©×—×•×¨×”\" ×¢×‘×•×¨ PySpark - ×”×•× ×œ× ×™×›×•×œ ×œ×‘×¦×¢ ××•×¤×˜×™××™×–×¦×™×•×ª. ×”×©×ª××© ×‘-UDF ×¨×§ ×›××©×¨ ××™×Ÿ ×¤×•× ×§×¦×™×” ××•×‘× ×™×ª ××ª××™××”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3ï¸âƒ£ ×©×™××•×© ×‘-Pandas UDF ×¢× PySpark\n",
    "\n",
    "× ×™×ª×Ÿ ×œ×”×©×ª××© ×‘-Pandas API ××¢×œ PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T15:09:39.250161Z",
     "start_time": "2025-11-09T15:09:38.765846Z"
    }
   },
   "source": [
    "import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "ps.set_option(\"compute.fail_on_ansi_mode\", False)\n",
    "\n",
    "# ×™×¦×™×¨×ª DataFrame ×¢× Pandas ×¢×œ PySpark\n",
    "psdf = ps.DataFrame(\n",
    "    {'fee': [20000, 25000, 30000, 22000, np.nan],\n",
    "     'Discount': [1000, 1500, 1500, 1200, 3000]}\n",
    ")\n",
    "print(psdf)\n",
    "\n",
    "# ×”×’×“×¨×ª ×¤×•× ×§×¦×™×”\n",
    "def add(data):\n",
    "    return data[0] + data[1]\n",
    "\n",
    "# ×©×™××•×© ×‘×¤×•× ×§×¦×™×”\n",
    "addDF = psdf.apply(add, axis=1)\n",
    "print(addDF)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mosheshulman/tutorials/PySpark/.venv/lib/python3.13/site-packages/pyspark/pandas/__init__.py:43: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n",
      "/Users/mosheshulman/tutorials/PySpark/.venv/lib/python3.13/site-packages/pyspark/pandas/utils.py:1037: PandasAPIOnSparkAdviceWarning: The config 'spark.sql.ansi.enabled' is set to True. This can cause unexpected behavior from pandas API on Spark since pandas API on Spark follows the behavior of pandas, not SQL.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       fee  Discount\n",
      "0  20000.0      1000\n",
      "1  25000.0      1500\n",
      "2  30000.0      1500\n",
      "3  22000.0      1200\n",
      "4      NaN      3000\n",
      "0    21000.0\n",
      "1    26500.0\n",
      "2    31500.0\n",
      "3    23200.0\n",
      "4        NaN\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mosheshulman/tutorials/PySpark/.venv/lib/python3.13/site-packages/pyspark/pandas/utils.py:1037: PandasAPIOnSparkAdviceWarning: If the type hints is not specified for `apply`, it is expensive to infer the data type internally.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/var/folders/kb/bm6sw7g57_5_708m47q7jqzm0000gn/T/ipykernel_14041/1333334986.py:15: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return data[0] + data[1]\n",
      "/Users/mosheshulman/tutorials/PySpark/.venv/lib/python3.13/site-packages/pyspark/pandas/utils.py:1037: PandasAPIOnSparkAdviceWarning: The config 'spark.sql.ansi.enabled' is set to True. This can cause unexpected behavior from pandas API on Spark since pandas API on Spark follows the behavior of pandas, not SQL.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Transform Function\n",
    "\n",
    "PySpark ××¡×¤×§ ×©× ×™ ×¡×•×’×™× ×©×œ `transform()`:\n",
    "1. **DataFrame.transform()** - ×–××™×Ÿ ×-Spark 3.0\n",
    "2. **sql.functions.transform()** - ×¢×‘×•×“×” ×¢× ×¢××•×“×•×ª ××¡×•×’ Array\n",
    "\n",
    "### ğŸ”¹ DataFrame.transform()\n",
    "\n",
    "××©××© ×œ×©×¨×©×•×¨ ×˜×¨× ×¡×¤×•×¨××¦×™×•×ª ××•×ª×××•×ª ××™×©×™×ª:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T15:09:44.157813Z",
     "start_time": "2025-11-09T15:09:44.017364Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "# ×™×¦×™×¨×ª DataFrame\n",
    "simpleData = ((\"Java\", 4000, 5), \n",
    "              (\"Python\", 4600, 10),\n",
    "              (\"Scala\", 4100, 15), \n",
    "              (\"Scala\", 4500, 15),\n",
    "              (\"PHP\", 3000, 20))\n",
    "              \n",
    "columns = [\"CourseName\", \"fee\", \"discount\"]\n",
    "\n",
    "df = spark.createDataFrame(data=simpleData, schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CourseName: string (nullable = true)\n",
      " |-- fee: long (nullable = true)\n",
      " |-- discount: long (nullable = true)\n",
      "\n",
      "+----------+----+--------+\n",
      "|CourseName|fee |discount|\n",
      "+----------+----+--------+\n",
      "|Java      |4000|5       |\n",
      "|Python    |4600|10      |\n",
      "|Scala     |4100|15      |\n",
      "|Scala     |4500|15      |\n",
      "|PHP       |3000|20      |\n",
      "+----------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ×™×¦×™×¨×ª ×¤×•× ×§×¦×™×•×ª ×˜×¨× ×¡×¤×•×¨××¦×™×” ××•×ª×××•×ª:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T15:09:52.104003Z",
     "start_time": "2025-11-09T15:09:52.058396Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import upper\n",
    "\n",
    "# ×˜×¨× ×¡×¤×•×¨××¦×™×” 1: ×”××¨×ª ×©× ×§×•×¨×¡ ×œ××•×ª×™×•×ª ×’×“×•×œ×•×ª\n",
    "def to_upper_str_columns(df):\n",
    "    return df.withColumn(\"CourseName\", upper(df.CourseName))\n",
    "\n",
    "# ×˜×¨× ×¡×¤×•×¨××¦×™×” 2: ×”×¤×—×ª×ª ××—×•×– ×”× ×—×” ××”××—×™×¨\n",
    "def reduce_price(df, reduceBy):\n",
    "    return df.withColumn(\"new_fee\", df.fee - reduceBy)\n",
    "\n",
    "# ×˜×¨× ×¡×¤×•×¨××¦×™×” 3: ×—×™×©×•×‘ ××—×™×¨ ×œ××—×¨ ×”× ×—×”\n",
    "def apply_discount(df):\n",
    "    return df.withColumn(\"discounted_fee\",  \n",
    "                        df.new_fee - (df.new_fee * df.discount) / 100)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ×©×¨×©×•×¨ ×”×˜×¨× ×¡×¤×•×¨××¦×™×•×ª:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×©×™××•×© ×‘-transform() ×œ×©×¨×©×•×¨\n",
    "df2 = df.transform(to_upper_str_columns) \\\n",
    "        .transform(reduce_price, 1000) \\\n",
    "        .transform(apply_discount)\n",
    "\n",
    "df2.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¹ sql.functions.transform()\n",
    "\n",
    "××©××© ×œ×”×—×œ×ª ×˜×¨× ×¡×¤×•×¨××¦×™×•×ª ×¢×œ ×¢××•×“×•×ª ××¡×•×’ ArrayType:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import upper, transform\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "# ×™×¦×™×¨×ª DataFrame ×¢× Array\n",
    "data = [\n",
    "    (\"James\", \"Smith\", [\"Java\", \"Scala\", \"C++\"]),\n",
    "    (\"Michael\", \"Rose\", [\"Spark\", \"Java\", \"C++\"]),\n",
    "    (\"Robert\", \"Williams\", [\"CSharp\", \"VB\"])\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=[\"Name\", \"Languages1\", \"Languages2\"])\n",
    "df.printSchema()\n",
    "df.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ×”×—×œ×ª ×˜×¨× ×¡×¤×•×¨××¦×™×” ×¢×œ Array:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”××¨×ª ×›×œ ××™×‘×¨×™ ×”××¢×¨×š ×œ××•×ª×™×•×ª ×’×“×•×œ×•×ª\n",
    "df.select(transform(col(\"Languages2\"), lambda x: upper(x)).alias(\"languagesUP\")).show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5ï¸âƒ£ ××™×—×•×“ DataFrames - Union\n",
    "\n",
    "### ğŸ”¹ union() - ××™×—×•×“ ×œ×¤×™ ××™×§×•×\n",
    "\n",
    "`union()` ×××–×’ ×©× ×™ DataFrames **×œ×¤×™ ××™×§×•× ×”×¢××•×“×•×ª** (×œ× ×œ×¤×™ ×©×):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "# DataFrame ×¨××©×•×Ÿ\n",
    "simpleData = [\n",
    "    (\"James\", \"Sales\", \"NY\", 90000, 34, 10000),\n",
    "    (\"Michael\", \"Sales\", \"NY\", 86000, 56, 20000),\n",
    "    (\"Robert\", \"Sales\", \"CA\", 81000, 30, 23000),\n",
    "    (\"Maria\", \"Finance\", \"CA\", 90000, 24, 23000)\n",
    "]\n",
    "\n",
    "columns = [\"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "# DataFrame ×©× ×™\n",
    "simpleData2 = [\n",
    "    (\"James\", \"Sales\", \"NY\", 90000, 34, 10000),\n",
    "    (\"Maria\", \"Finance\", \"CA\", 90000, 24, 23000),\n",
    "    (\"Jen\", \"Finance\", \"NY\", 79000, 53, 15000),\n",
    "    (\"Jeff\", \"Marketing\", \"CA\", 80000, 25, 18000),\n",
    "    (\"Kumar\", \"Marketing\", \"NY\", 91000, 50, 21000)\n",
    "]\n",
    "\n",
    "columns2 = [\"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\"]\n",
    "df2 = spark.createDataFrame(data=simpleData2, schema=columns2)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ×‘×™×¦×•×¢ union:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# union ××©×œ×‘ ××ª ×›×œ ×”×©×•×¨×•×ª (×›×•×œ×œ ×›×¤×™×œ×•×™×•×ª)\n",
    "unionDF = df.union(df2)\n",
    "unionDF.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ×”×¡×¨×ª ×›×¤×™×œ×•×™×•×ª:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# distinct() ××¡×™×¨ ×©×•×¨×•×ª ×›×¤×•×œ×•×ª\n",
    "disDF = df.union(df2).distinct()\n",
    "disDF.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¹ unionByName() - ××™×—×•×“ ×œ×¤×™ ×©× ×¢××•×“×”\n",
    "\n",
    "`unionByName()` ×××–×’ DataFrames **×œ×¤×™ ×©× ×”×¢××•×“×•×ª**, ×œ× ×œ×¤×™ ×”××™×§×•×:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# DataFrames ×¢× ×¢××•×“×•×ª ×‘××•×ª× ×©××•×ª ××š ×‘×¡×“×¨ ×©×•× ×”\n",
    "df1 = spark.createDataFrame([[5, 2, 6]], [\"col0\", \"col1\", \"col2\"])\n",
    "df2 = spark.createDataFrame([[6, 7, 3]], [\"col1\", \"col2\", \"col0\"])\n",
    "\n",
    "# unionByName ××¡×“×¨ ××ª ×”×¢××•×“×•×ª ×œ×¤×™ ×©×\n",
    "df3 = df1.unionByName(df2)\n",
    "df3.printSchema()\n",
    "df3.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ×˜×™×¤×•×œ ×‘××¡×¤×¨ ×©×•× ×” ×©×œ ×¢××•×“×•×ª:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# DataFrames ×¢× ××¡×¤×¨ ×©×•× ×” ×©×œ ×¢××•×“×•×ª\n",
    "df1 = spark.createDataFrame([[5, 2, 6]], [\"col0\", \"col1\", \"col2\"])\n",
    "df2 = spark.createDataFrame([[6, 7, 3]], [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "# allowMissingColumns=True ×××œ× null ×¢×‘×•×¨ ×¢××•×“×•×ª ×—×¡×¨×•×ª\n",
    "df3 = df1.unionByName(df2, allowMissingColumns=True)\n",
    "df3.printSchema()\n",
    "df3.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6ï¸âƒ£ Joins - ×—×™×‘×•×¨ DataFrames\n",
    "\n",
    "PySpark ×ª×•××š ×‘×›×œ ×¡×•×’×™ ×”-Joins ×”××•×›×¨×™× ×-SQL:\n",
    "- **Inner Join** - ×¨×§ ×©×•×¨×•×ª ×ª×•×××•×ª\n",
    "- **Left Outer Join** - ×›×œ ×”×©×•×¨×•×ª ××”×©×××œ + ×ª×•×××•×ª ××”×™××™×Ÿ\n",
    "- **Right Outer Join** - ×›×œ ×”×©×•×¨×•×ª ××”×™××™×Ÿ + ×ª×•×××•×ª ××©×××œ\n",
    "- **Full Outer Join** - ×›×œ ×”×©×•×¨×•×ª ××©× ×™ ×”×¦×“×“×™×\n",
    "- **Left Semi Join** - ×¨×§ ×©×•×¨×•×ª ××”×©×××œ ×©×™×© ×œ×”×Ÿ ×”×ª×××”\n",
    "- **Left Anti Join** - ×¨×§ ×©×•×¨×•×ª ××”×©×××œ ×©××™×Ÿ ×œ×”×Ÿ ×”×ª×××”\n",
    "- **Self Join** - ×—×™×‘×•×¨ ×©×œ ×˜×‘×œ×” ×œ×¢×¦××”\n",
    "- **Cross Join** - ××›×¤×œ×” ×§×¨×˜×–×™×ª\n",
    "\n",
    "### ×”×›× ×ª DataFrames ×œ×“×•×’××”:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "# DataFrame ×¢×•×‘×“×™×\n",
    "emp = [\n",
    "    (1, \"Smith\", -1, \"2018\", \"10\", \"M\", 3000),\n",
    "    (2, \"Rose\", 1, \"2010\", \"20\", \"M\", 4000),\n",
    "    (3, \"Williams\", 1, \"2010\", \"10\", \"M\", 1000),\n",
    "    (4, \"Jones\", 2, \"2005\", \"10\", \"F\", 2000),\n",
    "    (5, \"Brown\", 2, \"2010\", \"40\", \"\", -1),\n",
    "    (6, \"Brown\", 2, \"2010\", \"50\", \"\", -1)\n",
    "]\n",
    "\n",
    "empColumns = [\"emp_id\", \"name\", \"superior_emp_id\", \"year_joined\", \n",
    "              \"emp_dept_id\", \"gender\", \"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema=empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)\n",
    "\n",
    "# DataFrame ××—×œ×§×•×ª\n",
    "dept = [\n",
    "    (\"Finance\", 10),\n",
    "    (\"Marketing\", 20),\n",
    "    (\"Sales\", 30),\n",
    "    (\"IT\", 40)\n",
    "]\n",
    "\n",
    "deptColumns = [\"dept_name\", \"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema=deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¹ Inner Join\n",
    "\n",
    "××—×–×™×¨ ×¨×§ ×©×•×¨×•×ª ×©×™×© ×œ×”×Ÿ ×”×ª×××” ×‘×©× ×™ ×”-DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Inner Join\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"inner\").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¹ Left Outer Join\n",
    "\n",
    "××—×–×™×¨ ××ª ×›×œ ×”×©×•×¨×•×ª ××”×©×××œ, ×•×©×•×¨×•×ª ×ª×•×××•×ª ××”×™××™×Ÿ (null ×× ××™×Ÿ ×”×ª×××”):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Left Outer Join\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"left\").show(truncate=False)\n",
    "\n",
    "# × ×™×ª×Ÿ ×’× ×œ×”×©×ª××© ×‘-\"leftouter\"\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"leftouter\").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¹ Right Outer Join\n",
    "\n",
    "××—×–×™×¨ ××ª ×›×œ ×”×©×•×¨×•×ª ××”×™××™×Ÿ, ×•×©×•×¨×•×ª ×ª×•×××•×ª ××”×©×××œ (null ×× ××™×Ÿ ×”×ª×××”):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Right Outer Join\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"right\").show(truncate=False)\n",
    "\n",
    "# × ×™×ª×Ÿ ×’× ×œ×”×©×ª××© ×‘-\"rightouter\"\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"rightouter\").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¹ Full Outer Join\n",
    "\n",
    "××—×–×™×¨ ××ª ×›×œ ×”×©×•×¨×•×ª ××©× ×™ ×”-DataFrames (null ×‘××§×•××•×ª ×©××™×Ÿ ×”×ª×××”):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Full Outer Join\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"outer\").show(truncate=False)\n",
    "\n",
    "# × ×™×ª×Ÿ ×’× ×œ×”×©×ª××© ×‘-\"full\" ××• \"fullouter\"\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"full\").show(truncate=False)\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"fullouter\").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¹ Left Semi Join\n",
    "\n",
    "××—×–×™×¨ ×¨×§ ×©×•×¨×•×ª ××”×©×××œ ×©×™×© ×œ×”×Ÿ ×”×ª×××” ×‘×™××™×Ÿ (×œ×œ× ×¢××•×“×•×ª ××”×™××™×Ÿ):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Left Semi Join\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"leftsemi\").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¹ Left Anti Join\n",
    "\n",
    "××—×–×™×¨ ×¨×§ ×©×•×¨×•×ª ××”×©×××œ ×©××™×Ÿ ×œ×”×Ÿ ×”×ª×××” ×‘×™××™×Ÿ:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Left Anti Join\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"leftanti\").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¹ Self Join\n",
    "\n",
    "×—×™×‘×•×¨ ×©×œ DataFrame ×œ×¢×¦××• (×©×™××•×©×™ ×œ××¦×•× ×§×©×¨×™× ×”×™×¨×¨×›×™×™×):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Self Join - ××¦×™××ª ×× ×”×œ ×©×œ ×›×œ ×¢×•×‘×“\n",
    "empDF.alias(\"emp1\").join(\n",
    "    empDF.alias(\"emp2\"),\n",
    "    col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"emp1.emp_id\"),\n",
    "    col(\"emp1.name\"),\n",
    "    col(\"emp2.emp_id\").alias(\"superior_emp_id\"),\n",
    "    col(\"emp2.name\").alias(\"superior_emp_name\")\n",
    ").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¹ ×©×™××•×© ×‘-SQL Expressions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×¦×™×¨×ª temporary views\n",
    "empDF.createOrReplaceTempView(\"EMP\")\n",
    "deptDF.createOrReplaceTempView(\"DEPT\")\n",
    "\n",
    "# ×©×™××•×© ×‘-SQL\n",
    "joinDF = spark.sql(\n",
    "    \"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# Inner Join ×¢× SQL\n",
    "joinDF2 = spark.sql(\n",
    "    \"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\"\n",
    ").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š ×“×•×’××” ××§×™×¤×” - ×©×™×œ×•×‘ ×›×œ ×”× ×•×©××™×"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, upper, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# ×™×¦×™×¨×ª Spark session\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()\n",
    "\n",
    "# ×™×¦×™×¨×ª DataFrames\n",
    "columns = [\"Seqno\", \"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "        (\"2\", \"tracey smith\"),\n",
    "        (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.show(truncate=False)\n",
    "\n",
    "# ×©×™××•×© ×‘×¤×•× ×§×¦×™×” ××•×‘× ×™×ª\n",
    "df.withColumn(\"Upper_Name\", upper(df.Name)).show()\n",
    "\n",
    "# ×©×™××•×© ×‘-select\n",
    "df.select(\"Seqno\", \"Name\", upper(df.Name)).show()\n",
    "\n",
    "# ×©×™××•×© ×‘-SQL\n",
    "df.createOrReplaceTempView(\"TAB\")\n",
    "spark.sql(\"select Seqno, Name, UPPER(Name) from TAB\").show()\n",
    "\n",
    "# ×™×¦×™×¨×ª ×¤×•× ×§×¦×™×” ××•×ª×××ª\n",
    "def upperCase(str):\n",
    "    return str.upper()\n",
    "\n",
    "# ×”××¨×” ×œ-UDF\n",
    "upperCaseUDF = udf(lambda z: upperCase(z), StringType())\n",
    "\n",
    "# ×©×™××•×© ×‘-UDF ×¢× withColumn\n",
    "df.withColumn(\"Cureated_Name\", upperCaseUDF(col(\"Name\"))).show(truncate=False)\n",
    "\n",
    "# ×©×™××•×© ×‘-UDF ×¢× select\n",
    "df.select(col(\"Seqno\"), upperCaseUDF(col(\"Name\")).alias(\"Name\")).show(truncate=False)\n",
    "\n",
    "# ×¨×™×©×•× UDF ×•×©×™××•×© ×‘-SQL\n",
    "spark.udf.register(\"upperCaseUDF\", upperCaseUDF)\n",
    "df.createOrReplaceTempView(\"TAB\")\n",
    "spark.sql(\"select Seqno, Name, upperCaseUDF(Name) from TAB\").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š ××©××‘×™× × ×•×¡×¤×™×\n",
    "\n",
    "### ×§×™×©×•×¨×™× ×©×™××•×©×™×™×:\n",
    "- [PySpark SQL Functions Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)\n",
    "- [PySpark UDF Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.udf.html)\n",
    "- [Spark By Examples - sparkbyexamples.com](https://sparkbyexamples.com/pyspark/)\n",
    "\n",
    "### × ×•×©××™× ×§×©×•×¨×™×:\n",
    "- PySpark Window Functions\n",
    "- PySpark Aggregate Functions\n",
    "- PySpark Date and Timestamp Functions\n",
    "- PySpark Broadcasting\n",
    "- PySpark Partitioning\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ ×˜×™×¤×™× ×—×©×•×‘×™×\n",
    "\n",
    "1. **×‘×™×¦×•×¢×™×**: ×ª××™×“ ×”×¢×“×£ ×¤×•× ×§×¦×™×•×ª ××•×‘× ×•×ª ×¢×œ ×¤× ×™ UDF\n",
    "2. **Null Handling**: ×ª××™×“ ×˜×¤×œ ×‘×¢×¨×›×™ null ×‘-UDF\n",
    "3. **Transform**: ×”×©×ª××© ×‘-transform() ×œ×©×¨×©×•×¨ ×¤×¢×•×œ×•×ª ×‘×¦×•×¨×” ×§×¨×™××”\n",
    "4. **Join Type**: ×‘×—×¨ ××ª ×¡×•×’ ×”-Join ×”××ª××™× ×œ×¦×¨×›×™× ×©×œ×š\n",
    "5. **unionByName**: ×”×©×ª××© ×‘-unionByName() ×›×©×™×© ×œ×š ×¢××•×“×•×ª ×‘××•×ª× ×©××™× ××š ×‘×¡×“×¨ ×©×•× ×”\n",
    "\n",
    "---\n",
    "\n",
    "**×”×¢×¨×”**: ××—×‘×¨×ª ×–×• × ×•×¦×¨×” ×¢×œ ×‘×¡×™×¡ ×”××“×¨×™×›×™× ×-SparkByExamples.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
