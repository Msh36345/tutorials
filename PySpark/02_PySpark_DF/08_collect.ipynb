{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<style>\n",
    "    body, * {\n",
    "        direction: rtl !important;\n",
    "        text-align: right !important;\n",
    "    }\n",
    "</style>\n",
    "××§×•×¨: [Spark By Examples - Collect() Retrieve data from DataFrame\n",
    "](https://sparkbyexamples.com/pyspark/pyspark-collect/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š PySpark Collect() - ×©×œ×™×¤×ª × ×ª×•× ×™× ×-DataFrame\n",
    "\n",
    "## ××‘×•×\n",
    "\n",
    "×‘××“×¨×™×š ×–×” × ×œ××“ ×¢×œ ×”×¤×•× ×§×¦×™×” `collect()` ×‘-PySpark, ×©×”×™× ×¤×¢×•×œ×ª ××§×©×Ÿ (action operation) ×”××©××©×ª ×œ×©×œ×™×¤×ª ×›×œ ×”×¨×›×™×‘×™× ×©×œ ××¢×¨×š ×”× ×ª×•× ×™× (××›×œ ×”-nodes) ×œ×¦×•××ª ×”×× ×”×œ (driver node).\n",
    "\n",
    "âš ï¸ **×”×¢×¨×” ×—×©×•×‘×”:** ×™×© ×œ×”×©×ª××© ×‘-`collect()` ×¢×œ ××¢×¨×›×™ × ×ª×•× ×™× ×§×˜× ×™× ×‘×œ×‘×“! ×©×œ×™×¤×ª ××¢×¨×›×™ × ×ª×•× ×™× ×’×“×•×œ×™× ×¢×œ×•×œ×” ×œ×’×¨×•× ×œ×©×’×™××ª `OutOfMemory`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ××” × ×œ××“ ×‘××“×¨×™×š?\n",
    "\n",
    "- ×©×™××•×© ×‘-`collect()` ×¢× DataFrame\n",
    "- ××ª×™ ×œ×”×©×ª××© ×‘-`collect()` ×•××ª×™ ×œ×”×™×× ×¢ ××× ×”\n",
    "- ×”×”×‘×“×œ ×‘×™×Ÿ `collect()` ×œ-`select()`\n",
    "- ×“×•×’×××•×ª ××¢×©×™×•×ª\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ ×™×‘×•× ×¡×¤×¨×™×•×ª ×•×”×§××ª ×¡×‘×™×‘×ª ×¢×‘×•×“×”\n",
    "\n",
    "× ×ª×—×™×œ ×‘×™×‘×•× ×”×¡×¤×¨×™×•×ª ×”× ×“×¨×©×•×ª ×•×™×¦×™×¨×ª SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ—ï¸ ×™×¦×™×¨×ª DataFrame ×œ×“×•×’××”\n",
    "\n",
    "× ×¦×•×¨ DataFrame ×¢× × ×ª×•× ×™ ××—×œ×§×•×ª ×•×¢×¨×›×™ ××›×™×¨×•×ª:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dept = [(\"Finance\", 10),\n",
    "        (\"Marketing\", 20),\n",
    "        (\"Sales\", 30),\n",
    "        (\"IT\", 40)\n",
    "       ]\n",
    "\n",
    "deptColumns = [\"dept_name\", \"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema=deptColumns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ‘€ ×”×¦×’×ª ×”×ª×•×›×Ÿ ×©×œ ×”-DataFrame\n",
    "\n",
    "× ×©×ª××© ×‘×¤×•× ×§×¦×™×” `show()` ×›×“×™ ×œ×”×¦×™×’ ××ª ×”× ×ª×•× ×™× ×‘×¤×•×¨××˜ ×˜×‘×œ×”:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "deptDF.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ” ×©×™××•×© ×‘×¤×•× ×§×¦×™×” collect()\n",
    "\n",
    "### ××” ×–×” collect()?\n",
    "\n",
    "`collect()` ×”×™× ×¤×¢×•×œ×ª ××§×©×Ÿ ×©××—×–×™×¨×” ××ª ×›×œ ×”×¨×›×™×‘×™× ×‘-DataFrame ×›××¢×¨×š ×©×œ ××•×‘×™×™×§×˜×™ Row. ×”×¤×•× ×§×¦×™×” ××—×–×™×¨×” ××ª ×”× ×ª×•× ×™× ×œ×¦×•××ª ×”×× ×”×œ (driver node).\n",
    "\n",
    "### ×“×•×’××” ×‘×¡×™×¡×™×ª:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataCollect = deptDF.collect()\n",
    "print(dataCollect)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ×”×‘× ×ª ×”×ª×•×¦××•×ª\n",
    "\n",
    "×”×¤×•× ×§×¦×™×” `deptDF.collect()` ××—×–×™×¨×” ××ª ×›×œ ×”×¨×›×™×‘×™× ×‘-DataFrame ×›-Array ××¡×•×’ Row. \n",
    "\n",
    "×”×“×¤×¡×ª ×”××¢×¨×š ×ª×¦×™×’ ××ª ×›×œ ×”×©×•×¨×•×ª:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”×ª×•×¦××” ×ª×”×™×”:\n",
    "# [Row(dept_name='Finance', dept_id=10),\n",
    "#  Row(dept_name='Marketing', dept_id=20),\n",
    "#  Row(dept_name='Sales', dept_id=30),\n",
    "#  Row(dept_name='IT', dept_id=40)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”„ ×œ×•×œ××” ×¢×œ ×”×ª×•×¦××•×ª\n",
    "\n",
    "××›×™×•×•×Ÿ ×©-`collect()` ××—×–×™×¨×” Array, ×× ×—× ×• ×™×›×•×œ×™× ×œ×”×©×ª××© ×‘×œ×•×œ××ª `for` ×›×“×™ ×œ×¢×‘×•×¨ ×¢×œ ×”× ×ª×•× ×™×:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for row in dataCollect:\n",
    "    print(row['dept_name'] + \",\" + str(row['dept_id']))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ ×©×œ×™×¤×ª ×©×•×¨×” ××• ×¢××•×“×” ×¡×¤×¦×™×¤×™×ª\n",
    "\n",
    "### ×©×œ×™×¤×ª ×”×©×•×¨×” ×”×¨××©×•× ×” ×•×”×¢××•×“×” ×”×¨××©×•× ×”:\n",
    "\n",
    "×× ×¨×•×¦×™× ×œ×©×œ×•×£ ××ª ×”×©×•×¨×” ×”×¨××©×•× ×” ×•×”×¢××•×“×” ×”×¨××©×•× ×” ×-DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ××—×–×™×¨ ××ª ×”×¢×¨×š ×©×œ ×”×©×•×¨×” ×”×¨××©×•× ×”, ×”×¢××•×“×” ×”×¨××©×•× ×”\n",
    "deptDF.collect()[0][0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ×”×¡×‘×¨ ×¢×œ ××™× ×“×§×¡×™×:\n",
    "\n",
    "- `deptDF.collect()` - ××—×–×™×¨ Array ××¡×•×’ Row\n",
    "- `deptDF.collect()[0]` - ××—×–×™×¨ ××ª ×”×¨×›×™×‘ ×”×¨××©×•×Ÿ ×‘××¢×¨×š (×©×•×¨×” ×¨××©×•× ×”)\n",
    "- `deptDF.collect()[0][0]` - ××—×–×™×¨ ××ª ×”×¢×¨×š ×©×œ ×”×©×•×¨×” ×”×¨××©×•× ×”, ×”×¢××•×“×” ×”×¨××©×•× ×”\n",
    "\n",
    "### ×“×•×’××” × ×•×¡×¤×ª:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×× × ×¨×™×¥ print ×¢×œ deptDF.collect()[0][0], ×–×” ×™×“×¤×™×¡ \"Finance\"\n",
    "# ×•-print(deptDF.collect()[0][1]) ×™×“×¤×™×¡ 10"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¨ ×©×™××•×© ×‘-select() ×œ×¤× ×™ collect()\n",
    "\n",
    "×‘××§×¨×™× ×©×‘×”× ×¨×•×¦×™× ×œ×”×—×–×™×¨ ×¨×§ ×¨×›×™×‘×™× ××¡×•×™××™× ××”-DataFrame, ×›×“××™ ×œ×”×©×ª××© ×‘-`select()` transformation ×œ×¤× ×™ ×§×¨×™××” ×œ-`collect()`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataCollect = deptDF.select(\"dept_name\").collect()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš ï¸ ××ª×™ ×œ×”×™×× ×¢ ×-collect()?\n",
    "\n",
    "### ×¡×™×‘×•×ª ×œ×”×™×× ×¢ ×-collect():\n",
    "\n",
    "×‘×“×¨×š ×›×œ×œ, `collect()` ××©××©×ª ×œ×©×œ×™×¤×ª ×¤×œ×˜ ×”××§×©×Ÿ ×›××©×¨ ×™×© ×œ× ×• ×ª×•×¦××” ×§×˜× ×” ×××•×“. \n",
    "\n",
    "**âš ï¸ ××–×”×¨×”:** ×§×¨×™××” ×œ-`collect()` ×¢×œ RDD/DataFrame ×¢× ××¢×¨×š × ×ª×•× ×™× ×’×“×•×œ ×’×•×¨××ª ×œ×©×’×™××ª `OutOfMemory` ××›×™×•×•×Ÿ ×©×”×™× ××—×–×™×¨×” ××ª ×›×œ ××¢×¨×š ×”× ×ª×•× ×™× (××›×œ ×”-workers) ×œ×¦×•××ª ×”×× ×”×œ.\n",
    "\n",
    "×œ×›×Ÿ, ×™×© ×œ×”×™×× ×¢ ××§×¨×™××” ×œ-`collect()` ×¢×œ ××¢×¨×›×™ × ×ª×•× ×™× ×’×“×•×œ×™×!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ†š collect() ×œ×¢×•××ª select()\n",
    "\n",
    "### ×”×”×‘×“×œ×™× ×”×¢×™×§×¨×™×™×:\n",
    "\n",
    "**`select()`:**\n",
    "- ×”×™× ×˜×¨× ×¡×¤×•×¨××¦×™×” (transformation)\n",
    "- ××—×–×™×¨×” DataFrame ×—×“×©\n",
    "- ××—×–×™×§×” ××ª ×”×¢××•×“×•×ª ×©× ×‘×—×¨×•\n",
    "\n",
    "**`collect()`:**\n",
    "- ×”×™× ××§×©×Ÿ (action)\n",
    "- ××—×–×™×¨×” ××ª ×›×œ ×”× ×ª×•× ×™× ×›-Array ×œ×¦×•××ª ×”×× ×”×œ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’» ×“×•×’××” ××œ××”\n",
    "\n",
    "×œ×”×œ×Ÿ ×”×§×•×“ ×”××œ× ×©×œ ×”×“×•×’××” ×©×œ× ×•:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()\n",
    "\n",
    "dept = [(\"Finance\", 10),\n",
    "        (\"Marketing\", 20),\n",
    "        (\"Sales\", 30),\n",
    "        (\"IT\", 40)\n",
    "       ]\n",
    "\n",
    "deptColumns = [\"dept_name\", \"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema=deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)\n",
    "\n",
    "dataCollect = deptDF.collect()\n",
    "\n",
    "print(dataCollect)\n",
    "\n",
    "dataCollect2 = deptDF.select(\"dept_name\").collect()\n",
    "print(dataCollect2)\n",
    "\n",
    "for row in dataCollect:\n",
    "    print(row['dept_name'] + \",\" + str(row['dept_id']))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Œ ×¡×™×›×•×\n",
    "\n",
    "×‘××“×¨×™×š PySpark ×–×”, ×œ××“× ×•:\n",
    "\n",
    "âœ… ×”×¤×•× ×§×¦×™×” `collect()` ×”×™× ×¤×¢×•×œ×ª ××§×©×Ÿ ×©××—×–×™×¨×” ××ª ×›×œ ×”×¨×›×™×‘×™× ×©×œ ×”-DataFrame ×œ×¦×•××ª ×”×× ×”×œ\n",
    "\n",
    "âœ… **×—×©×•×‘ ×œ×–×›×•×¨:** ×œ× ×›×“××™ ×œ×”×©×ª××© ×‘×” ×¢×œ ××¢×¨×›×™ × ×ª×•× ×™× ×’×“×•×œ×™×!\n",
    "\n",
    "âœ… ×”×”×‘×“×œ ×‘×™×Ÿ `collect()` ×œ-`select()`\n",
    "\n",
    "âœ… ×›×™×¦×“ ×œ×©×œ×•×£ ×©×•×¨×•×ª ×•×¢××•×“×•×ª ×¡×¤×¦×™×¤×™×•×ª ××”×ª×•×¦××”\n",
    "\n",
    "### ğŸ“ ×œ××™×“×” ××”× ×”!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ××××¨×™× ×§×©×•×¨×™×\n",
    "\n",
    "××•××œ×¥ ×œ×§×¨×•× ×’×:\n",
    "\n",
    "- PySpark distinct vs dropDuplicates\n",
    "- PySpark Select Distinct Rows\n",
    "- PySpark cache() Explained\n",
    "- PySpark SparkContext Explained\n",
    "- PySpark JSON Functions with Examples\n",
    "- PySpark Convert DataFrame to RDD\n",
    "- PySpark â€“ Loop/Iterate Through Rows in DataFrame\n",
    "- Extract First and last N rows from PySpark DataFrame\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— ×§×™×©×•×¨×™×\n",
    "\n",
    "- [PySpark GitHub Repository](https://github.com/spark-examples/pyspark-examples)\n",
    "- [SparkByExamples.com - ××××¨ ×”××§×•×¨](https://sparkbyexamples.com/pyspark/pyspark-collect/)\n",
    "\n",
    "---\n",
    "\n",
    "**×ª×’×™×•×ª:** `collect()`, `PySpark`, `DataFrame`, `Apache Spark`, `Big Data`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
