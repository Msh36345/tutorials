{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    body, * {\n",
    "        direction: rtl !important;\n",
    "        text-align: right !important;\n",
    "    }\n",
    "</style>\n",
    "# ğŸ“„ PySpark - ×§×¨×™××ª ×§×•×‘×¥ JSON ×œ-DataFrame\n",
    "[PySpark Read JSON file into DataFrame](https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/)\n",
    "## ××‘×•×\n",
    "\n",
    "×›×“×™ ×œ×§×¨×•× ×§×•×‘×¥ JSON ×œ-PySpark DataFrame, ××©×ª××©×™× ×™×›×•×œ×™× ×œ×”×©×ª××© ×‘×©×™×˜×ª `json()` ××”-DataFrameReader class. ×©×™×˜×” ×–×• ×× ×ª×—×ª ×§×‘×¦×™ JSON ×•××¡×™×§×” ××•×˜×•××˜×™×ª ××ª ×”×¡×›×™××”, ×™×•×¦×¨×ª DataFrame ××”× ×ª×•× ×™× ×”××•×‘× ×™× ×•×”×—×¦×™-××•×‘× ×™×.\n",
    "\n",
    "×§×¨×™××ª ×§×‘×¦×™ JSON ×œ-PySpark DataFrame ×××¤×©×¨×ª ×œ××©×ª××©×™× ×œ×‘×¦×¢ ×˜×¨× ×¡×¤×•×¨××¦×™×•×ª ×—×–×§×•×ª ×©×œ × ×ª×•× ×™×, × ×™×ª×•×—×™× ×•××©×™××•×ª machine learning ×¢×œ datasets ×’×“×•×œ×™× ×‘×¡×‘×™×‘×ª ××—×©×•×‘ ××‘×•×–×¨. ×–×” ×”×•×¤×š ××•×ª×• ×œ×›×œ×™ ×¨×‘-×¢×•×¦××” ×œ×¢×™×‘×•×“ ×¦×™× ×•×¨×•×ª × ×ª×•× ×™× (data processing pipelines) ×¢× PySpark.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ ×ª×•×›×Ÿ ×¢× ×™×™× ×™×:\n",
    "\n",
    "- ×§×¨×™××ª ×§×•×‘×¥ JSON ×‘-PySpark\n",
    "- ×§×¨×™××” ×-Multiline JSON File\n",
    "- ×§×¨×™××ª ××¡×¤×¨ ×§×‘×¦×™× ×‘×• ×–×× ×™×ª\n",
    "- ×§×¨×™××ª ×›×œ ×”×§×‘×¦×™× ×‘×ª×™×§×™×™×”\n",
    "- ×§×¨×™××ª ×§×‘×¦×™× ×¢× ×¡×›×™××” ××•×ª×××ª ××™×©×™×ª\n",
    "- ×§×¨×™××ª ×§×•×‘×¥ ×‘×××¦×¢×•×ª PySpark SQL\n",
    "- ××¤×©×¨×•×™×•×ª ×§×¨×™××ª ×§×•×‘×¥ JSON\n",
    "  - Handling Null Values\n",
    "  - Handling Date Formats\n",
    "- ×›×ª×™×‘×ª DataFrame ×œ×§×•×‘×¥ JSON\n",
    "  - Using options\n",
    "  - Saving Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. ğŸ“– ×§×¨×™××ª ×§×•×‘×¥ JSON ×‘-PySpark\n",
    "\n",
    "×›×“×™ ×œ×§×¨×•× ×§×•×‘×¥ JSON ×œ-PySpark DataFrame, ××ª×—×œ SparkSession ×•×”×©×ª××© ×‘-`spark.read.json(\"json_file.json\")`. ×”×—×œ×£ `\"json_file.json\"` ×¢× × ×ª×™×‘ ×”×§×•×‘×¥ ×‘×¤×•×¢×œ. ×©×™×˜×” ×–×• ××¡×™×§×” ××•×˜×•××˜×™×ª ××ª ×”×¡×›×™××” ×•×™×•×¦×¨×ª DataFrame ××”× ×ª×•× ×™× ×”××•×‘× ×™× ×•×—×¦×™-××•×‘× ×™×."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×™×‘×•××™×\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"JsonFile\").getOrCreate()\n",
    "\n",
    "# ×§×¨×™××ª ×§×•×‘×¥ JSON ×œ-dataframe\n",
    "df = spark.read.json(\"../data/zipcodes.json\")\n",
    "df.printSchema()\n",
    "df.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×œ×—×œ×•×¤×™×Ÿ, × ×™×ª×Ÿ ×œ×”×©×ª××© ×‘×¤×•× ×§×¦×™×™×ª `format()` ×™×—×“ ×¢× ×©×™×˜×ª `load()` ×œ×§×¨×™××ª ×§×•×‘×¥ JSON:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# -- ××• --\n",
    "df = spark.read.format(\"org.apache.spark.sql.json\") \\\n",
    "    .load(\"../data/zipcodes.json\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. ğŸ“ ×§×¨×™××” ×-Multiline JSON File\n",
    "\n",
    "×›×“×™ ×œ×§×¨×•× multiline JSON file ×œ-PySpark DataFrame, ×”×©×ª××© ×‘-`spark.read.option(\"multiline\",\"true\").json(\"path_to_json_file.json\")`.\n",
    "\n",
    "×”×’×“×¨×” ×–×• ×××¤×©×¨×ª ×§×¨×™××ª JSON objects ×”××©×ª×¨×¢×™× ×¢×œ ××¡×¤×¨ ×©×•×¨×•×ª. ×¦×™×™×Ÿ ××ª × ×ª×™×‘ ×§×•×‘×¥ ×”-JSON ×›-`\"path_to_json_file.json\"`. DataFrame ×©× ×•×¦×¨ ×™×›×™×œ ××ª ×”× ×ª×•× ×™× ××§×•×‘×¥ ×”-multiline JSON, ×”××•×›× ×™× ×œ×¢×™×‘×•×“ × ×•×¡×£."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×§×¨×™××ª multiline json file\n",
    "multiline_df = spark.read.option(\"multiline\", \"true\") \\\n",
    "    .json(\"../data/multiline-zipcode.json\")\n",
    "multiline_df.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. ğŸ“š ×§×¨×™××” ×-Multiple files at a time\n",
    "\n",
    "×›×“×™ ×œ×§×¨×•× ××¡×¤×¨ ×§×‘×¦×™ JSON ×‘×• ×–×× ×™×ª ×œ-PySpark DataFrame, ×”×¢×‘×¨ ×¨×©×™××” ×©×œ × ×ª×™×‘×™ ×§×‘×¦×™× ×”××•×¤×¨×“×™× ×‘×¤×¡×™×§×™× ×œ-read.json() method. ×œ×“×•×’××”:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×§×¨×™××ª ××¡×¤×¨ ×§×‘×¦×™×\n",
    "# df2 = spark.read.json(\n",
    "#     [\"../data/zipcodes.json\", \"../data/multiline-zipcode.json\"])\n",
    "# df2.show(3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. ğŸ“ ×§×¨×™××ª ×›×œ ×”×§×‘×¦×™× ×‘×ª×™×§×™×™×”\n",
    "\n",
    "×›×“×™ ×œ×§×¨×•× ××ª ×›×œ ×§×‘×¦×™ ×”-JSON ××ª×™×§×™×™×” ×œ-PySpark DataFrame ×‘×• ×–×× ×™×ª, ×”×©×ª××© ×‘-`spark.read.json(\"directory_path\")`, ×›××©×¨ `\"directory_path\"` ××¦×‘×™×¢ ×¢×œ ×”×ª×™×§×™×™×” ×”××›×™×œ×” ××ª ×§×‘×¦×™ ×”-JSON. PySpark ××¢×‘×“ ××•×˜×•××˜×™×ª ××ª ×›×œ ×§×‘×¦×™ ×”-JSON ×‘×ª×™×§×™×™×”, ××©×œ×‘ ××•×ª× ×œ-DataFrame ××—×“ ×œ× ×™×ª×•×— × ×•×—."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×§×¨×™××ª ×›×œ ×§×‘×¦×™ JSON ××ª×™×§×™×™×”\n",
    "# df3 = spark.read.json(\"../data/*.json\")\n",
    "# df3.show(3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, DoubleType\n",
    "# ×”×’×“×¨×ª ×¡×›×™××” ××•×ª×××ª\n",
    "schema = StructType([\n",
    "    StructField(\"RecordNumber\", IntegerType(), True),\n",
    "    StructField(\"Zipcode\", IntegerType(), True),\n",
    "    StructField(\"ZipCodeType\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"LocationType\", StringType(), True),\n",
    "    StructField(\"Lat\", DoubleType(), True),\n",
    "    StructField(\"Long\", DoubleType(), True),\n",
    "    StructField(\"Xaxis\", IntegerType(), True),\n",
    "    StructField(\"Yaxis\", DoubleType(), True),\n",
    "    StructField(\"Zaxis\", DoubleType(), True),\n",
    "    StructField(\"WorldRegion\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"LocationText\", StringType(), True),\n",
    "    StructField(\"Location\", StringType(), True),\n",
    "    StructField(\"Decommisioned\", BooleanType(), True),\n",
    "    StructField(\"TaxReturnsFiled\", StringType(), True),\n",
    "    StructField(\"EstimatedPopulation\", IntegerType(), True),\n",
    "    StructField(\"TotalWages\", IntegerType(), True),\n",
    "    StructField(\"Notes\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_with_schema = spark.read.schema(schema) \\\n",
    "    .json(\"../data/zipcodes.json\")\n",
    "df_with_schema.printSchema()\n",
    "df_with_schema.show(3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. ğŸ” ×§×¨×™××ª ×§×•×‘×¥ ×‘×××¦×¢×•×ª PySpark SQL\n",
    "\n",
    "PySpark SQL ××¡×¤×§ ×’× ×“×¨×š ×œ×§×¨×•× ×§×•×‘×¥ json ×™×©×™×¨×•×ª ××§×•×‘×¥ ×”×§×¨×™××” ×‘×××¦×¢×•×ª `spark.sqlContext.sql(\"load JSON to temporary view\")`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW zipcode USING json OPTIONS (path '../data/zipcodes.json')\")\n",
    "spark.sql(\"select * from zipcode\").show(3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. âš™ï¸ ××¤×©×¨×•×™×•×ª ×§×¨×™××ª ×§×•×‘×¥ JSON\n",
    "\n",
    "### nullValues\n",
    "\n",
    "×”××¤×©×¨×•×ª `nullValues` ×‘-PySpark ××©××©×ª ×œ×¦×™×•×Ÿ ××—×¨×•×–×•×ª ××•×ª×××•×ª ××™×©×™×ª ×©×¦×¨×™×›×•×ª ×œ×”×™×—×©×‘ ×›×¢×¨×›×™ null ×‘××”×œ×š ×ª×”×œ×™×š ×§×œ×™×˜×ª ×”× ×ª×•× ×™×. ×œ×“×•×’××”, ×× ××ª×” ×¨×•×¦×” ×œ×©×§×•×œ ×©×“×” ×¢× ×¢×¨×š `\"N/A\"` ×›-null ×‘-DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dateFormat\n",
    "\n",
    "×”××¤×©×¨×•×ª `dateFormat` ××©××©×ª ×œ×¦×™×•×Ÿ ××ª ×”×¤×•×¨××˜ ×©×œ date ××• timestamp columns ×‘× ×ª×•× ×™ ×”×§×œ×˜. ××¤×©×¨×•×ª ×–×• ×××¤×©×¨×ª ×œ-PySpark ×œ× ×ª×— × ×›×•×Ÿ date ××• timestamp strings ×œ×¡×•×’×™ ×”× ×ª×•× ×™× ×”××ª××™××™× ×©×œ×”×. ×ª×•××š ×‘×›×œ ×”×¤×•×¨××˜×™× ×©×œ `java.text.SimpleDateFormat`.\n",
    "\n",
    "**×”×¢×¨×”:** ××œ×‘×“ ×”××¤×©×¨×•×™×•×ª ×œ×¢×™×œ, PySpark CSV API ×ª×•××š ×‘××¤×©×¨×•×™×•×ª ×¨×‘×•×ª ××—×¨×•×ª × ×•×¡×¤×•×ª."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ×›×ª×™×‘×ª dataframe ×œ×§×•×‘×¥ json\n",
    "df.write.json(\"tmp/03_json/zipcodes.json\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "×”×¤×§×•×“×” ×”×–×• ×›×•×ª×‘×ª ××ª ×”-DataFrame `df2` ×œ× ×ª×™×‘ ×”×¤×œ×˜ ×”××•×’×“×¨ ×›×§×‘×¦×™ JSON. ×›×œ partition ×©×œ ×”-DataFrame × ×›×ª×‘ ×›×§×•×‘×¥ JSON × ×¤×¨×“. ×”× ×ª×•× ×™× ××¡×•×“×¨×™× ×‘×¤×•×¨××˜ JSON, ×•××©××¨×™× ××ª ×”×¡×›×™××” ×©×œ ×”-DataFrame."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 ××¤×©×¨×•×™×•×ª ×‘×–××Ÿ ×›×ª×™×‘×ª ×§×‘×¦×™ JSON\n",
    "\n",
    "×›××©×¨ ×›×•×ª×‘×™× DataFrame ×œ×§×‘×¦×™ JSON ×‘-PySpark, × ×™×ª×Ÿ ×œ×¦×™×™×Ÿ ××¤×©×¨×•×™×•×ª ×©×•× ×•×ª ×›×“×™ ×œ×”×ª××™× ××ª ×”×›×ª×™×‘×”. ×›××” ××¤×©×¨×•×™×•×ª × ×¤×•×¦×•×ª ×›×•×œ×œ×•×ª:\n",
    "\n",
    "1. **path**: ××¦×™×™×Ÿ ××ª ×”× ×ª×™×‘ ×©×‘×• ×§×‘×¦×™ ×”-JSON ×™×™×©××¨×•.\n",
    "\n",
    "2. **mode**: ××¦×™×™×Ÿ ××ª ×”×”×ª× ×”×’×•×ª ×‘×¢×ª ×›×ª×™×‘×” ×œ×ª×™×§×™×™×” ×§×™×™××ª.\n",
    "\n",
    "3. **compression**: ××¦×™×™×Ÿ ××ª codec ×”×“×—×™×¡×” ×œ×©×™××•×© ×‘×¢×ª ×›×ª×™×‘×ª ×§×‘×¦×™ ×”-JSON (×œ××©×œ, \"gzip\", \"snappy\").\n",
    "\n",
    "4. **dateFormat**: ××¦×™×™×Ÿ ××ª ×”×¤×•×¨××˜ ×œ×¢××•×“×•×ª ×ª××¨×™×š.\n",
    "\n",
    "5. **timestampFormat**: ××¦×™×™×Ÿ ××ª ×”×¤×•×¨××˜ ×œ×¢××•×“×•×ª timestamp.\n",
    "\n",
    "6. **lineSep**: ××¦×™×™×Ÿ ××ª ×¨×¦×£ ×”×ª×•×•×™× ×œ×©×™××•×© ×›××¤×¨×™×“ ×©×•×¨×•×ª ×‘×™×Ÿ JSON objects.\n",
    "\n",
    "7. **encoding**: ××¦×™×™×Ÿ ××ª ×”-encoding ×©×œ ×”×ª×•×•×™× ×œ×©×™××•×© ×‘×¢×ª ×›×ª×™×‘×ª ×§×‘×¦×™ ×”-JSON."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# df.write \\\n",
    "#     .option(\"compression\", \"gzip\") \\\n",
    "#     .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "#     .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\") \\\n",
    "#     .option(\"lineSep\", \"\\n\") \\\n",
    "#     .option(\"encoding\", \"UTF-8\") \\\n",
    "#     .json(\"tmp/03_json/zipcodes2.json\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 9. ğŸ”„ ××¦×‘×™ ×©××™×¨×” ×©×œ PySpark (PySpark Saving modes)\n",
    "\n",
    "×‘-PySpark, ×›××©×¨ ×©×•××¨×™× DataFrames ×œ××—×¡×•×Ÿ ×—×™×¦×•× ×™ ×›××• ××¢×¨×›×•×ª ×§×‘×¦×™× ××• databases, × ×™×ª×Ÿ ×œ×¦×™×™×Ÿ ××¦×‘×™ ×©××™×¨×” ×©×•× ×™× ×›×“×™ ×œ×©×œ×•×˜ ×‘×”×ª× ×”×’×•×ª ×‘××§×¨×” ×©××™×§×•× ×”×™×¢×“ ×›×‘×¨ ×§×™×™×. ××¦×‘×™ ×”×©××™×¨×” ×›×•×œ×œ×™×:\n",
    "\n",
    "1. **Append**: ××•×¡×™×£ ××ª ×”× ×ª×•× ×™× ×œ× ×ª×•× ×™× ×”×§×™×™××™× ×‘××™×§×•× ×”×™×¢×“. ×× ××™×§×•× ×”×™×¢×“ ×œ× ×§×™×™×, ×”×•× ×™×•×¦×¨ ×—×“×©.\n",
    "\n",
    "2. **Overwrite**: ×“×•×¨×¡ ××ª ×”× ×ª×•× ×™× ×‘××™×§×•× ×”×™×¢×“ ×× ×”×•× ×›×‘×¨ ×§×™×™×. ×× ××™×§×•× ×”×™×¢×“ ×œ× ×§×™×™×, ×”×•× ×™×•×¦×¨ ×—×“×©.\n",
    "\n",
    "3. **Ignore**: ××ª×¢×œ× ××”×¤×¢×•×œ×” ×•×œ× ×¢×•×©×” ×›×œ×•× ×× ××™×§×•× ×”×™×¢×“ ×›×‘×¨ ×§×™×™×. ×× ××™×§×•× ×”×™×¢×“ ×œ× ×§×™×™×, ×”×•× ×™×•×¦×¨ ×—×“×©.\n",
    "\n",
    "4. **Error ××• ErrorIfExists**: ×–×•×¨×§ error ×•× ×›×©×œ ×‘×¤×¢×•×œ×” ×× ××™×§×•× ×”×™×¢×“ ×›×‘×¨ ×§×™×™×. ×–×• ×”×”×ª× ×”×’×•×ª ×‘×¨×™×¨×ª ×”××—×“×œ ×× ×œ× ×¦×•×™×Ÿ ××¦×‘ ×©××™×¨×”.\n",
    "\n",
    "××¦×‘×™ ×©××™×¨×” ××œ×• ××¡×¤×§×™× ×’××™×©×•×ª ×•×©×œ×™×˜×” ×¢×œ ××™×š ×”× ×ª×•× ×™× × ×©××¨×™× ×•××˜×•×¤×œ×™× ×‘×ª×¨×—×™×©×™× ×©×•× ×™×, ×•××‘×˜×™×—×™× data integrity ×•×¢×§×‘×™×•×ª ×‘×–×¨××™ ×¢×‘×•×“×” ×©×œ data processing workflows."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×›×ª×™×‘×” ×¢× saveMode\n",
    "df.write.mode(\"overwrite\").json(\"tmp/03_json/zipcodes.json\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 10. ğŸ“‹ ×§×•×“ ××§×•×¨ ×œ×¢×™×•×Ÿ (Source code for reference)\n",
    "\n",
    "×“×•×’××” ×–×• ×–××™× ×” ×’× ×‘-GitHub PySpark Example Project ×œ×¢×™×•×Ÿ."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ×§×¨×™××ª ×§×•×‘×¥ JSON ×œ-dataframe\n",
    "df = spark.read.json(\"../data/zipcodes.json\")\n",
    "df.printSchema()\n",
    "df.show(3)\n",
    "\n",
    "# ×§×¨×™××ª multiline json file\n",
    "# multiline_df = spark.read.option(\"multiline\", \"true\") \\\n",
    "#     .json(\"../data/multiline-zipcode.json\")\n",
    "# multiline_df.show()\n",
    "\n",
    "# ×§×¨×™××ª ××¡×¤×¨ ×§×‘×¦×™×\n",
    "# df2 = spark.read.json(\n",
    "#     [\"../data/zipcode1.json\", \"../data/zipcode1.json\"])\n",
    "# df2.show()\n",
    "\n",
    "# ×§×¨×™××ª ×›×œ ×§×‘×¦×™ JSON ××ª×™×§×™×™×”\n",
    "# df3 = spark.read.json(\"../data/*.json\")\n",
    "# df3.show()\n",
    "\n",
    "# ×”×’×“×¨×ª ×¡×›×™××” ××•×ª×××ª\n",
    "schema = StructType([\n",
    "    StructField(\"RecordNumber\", IntegerType(), True),\n",
    "    StructField(\"Zipcode\", IntegerType(), True),\n",
    "    StructField(\"ZipCodeType\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"LocationType\", StringType(), True),\n",
    "    StructField(\"Lat\", DoubleType(), True),\n",
    "    StructField(\"Long\", DoubleType(), True),\n",
    "    StructField(\"Xaxis\", IntegerType(), True),\n",
    "    StructField(\"Yaxis\", DoubleType(), True),\n",
    "    StructField(\"Zaxis\", DoubleType(), True),\n",
    "    StructField(\"WorldRegion\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"LocationText\", StringType(), True),\n",
    "    StructField(\"Location\", StringType(), True),\n",
    "    StructField(\"Decommisioned\", BooleanType(), True),\n",
    "    StructField(\"TaxReturnsFiled\", StringType(), True),\n",
    "    StructField(\"EstimatedPopulation\", IntegerType(), True),\n",
    "    StructField(\"TotalWages\", IntegerType(), True),\n",
    "    StructField(\"Notes\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_with_schema = spark.read.schema(schema) \\\n",
    "    .json(\"../data/zipcodes.json\")\n",
    "df_with_schema.printSchema()\n",
    "df_with_schema.show(3)\n",
    "\n",
    "# ×™×¦×™×¨×ª table ×¢×œ Parquet File\n",
    "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW zipcode3 USING json OPTIONS (path '../data/zipcodes.json')\")\n",
    "spark.sql(\"select * from zipcode3\").show(3)\n",
    "\n",
    "# ×›×ª×™×‘×ª PySpark Parquet File\n",
    "df.write.mode(\"overwrite\").json(\"tmp/03_json/zipcodes2.json\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. ğŸ¯ ×©××œ×•×ª × ×¤×•×¦×•×ª ×¢×œ PySpark Read JSON\n",
    "\n",
    "### ×”×× ×× ×—× ×• ×™×›×•×œ×™× ×œ×§×¨×•× ××¡×¤×¨ ×§×‘×¦×™ JSON ×œ-DataFrame ×‘×•×“×“?\n",
    "\n",
    "×× ×—× ×• ×™×›×•×œ×™× ×œ×§×¨×•× ××¡×¤×¨ ×§×‘×¦×™ JSON ×œ-DataFrame ×‘×•×“×“ ×¢×œ ×™×“×™ ××ª×Ÿ × ×ª×™×‘ ×ª×™×§×™×™×” ×”××›×™×œ ××ª ×§×‘×¦×™ ×”-JSON. PySpark ×™×—×‘×¨ ××•×˜×•××˜×™×ª ××•×ª× ×œ-DataFrame ××—×“.\n",
    "\n",
    "×œ×“×•×’××”:\n",
    "```python\n",
    "df = spark.read.json(\"path/to/json/files/\")\n",
    "```\n",
    "\n",
    "### ××™×š ×œ×¦×™×™×Ÿ ×¡×›×™××” ×‘×–××Ÿ ×§×¨×™××ª × ×ª×•× ×™ JSON?\n",
    "\n",
    "×× ××ª×” ×™×•×“×¢ ××ª ×”×¡×›×™××” ×©×œ ×”×§×•×‘×¥ ××¨××© ×•××™× ×š ×¨×•×¦×” ×œ×”×©×ª××© ×‘××¤×©×¨×•×ª ×‘×¨×™×¨×ª ×”××—×“×œ `inferSchema`, ×”×©×ª××© ×‘××¤×©×¨×•×ª ×”×¡×›×™××” ×›×“×™ ×œ×¦×™×™×Ÿ column names ××•×ª×××™× ×•-data types.\n",
    "\n",
    "### ××™×š ×œ×˜×¤×œ ×‘×¡×›×™××” ×©×œ × ×ª×•× ×™ JSON ×©×™×© ×œ×”× nested structures?\n",
    "\n",
    "PySpark ×™×›×•×œ ×œ×˜×¤×•×œ ×‘-nested structures ×‘× ×ª×•× ×™ JSON. ×”×©×™×˜×” `spark.read.json()` ××¡×™×§×” ××•×˜×•××˜×™×ª ××ª ×”×¡×›×™××”, ×›×•×œ×œ nested structures. × ×™×ª×Ÿ ×œ×’×©×ª ×œ-nested fields ×‘×××¦×¢×•×ª dot notation ×‘×©××™×œ×ª×•×ª DataFrame.\n",
    "\n",
    "### ××” ×× × ×ª×•× ×™ ×”-JSON ×©×œ×™ ×œ× ×‘×§×•×‘×¥ ××œ× ×××•×—×¡× ×™× ×‘××©×ª× ×”?\n",
    "\n",
    "×× × ×ª×•× ×™ ×”-JSON ×©×œ×š ×××•×—×¡× ×™× ×‘××©×ª× ×”, × ×™×ª×Ÿ ×œ×”×©×ª××© ×‘×©×™×˜×ª `spark.read.json()` ×¢× ×”×©×™×˜×” `jsonRDD`. ×œ×“×•×’××”:\n",
    "\n",
    "```python\n",
    "json_object = '{\"name\": \"Cinthia\", \"age\": 20}'\n",
    "df = spark.read.json(spark.sparkContext.parallelize([json_object]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. ğŸ“ ×¡×™×›×•×:\n",
    "\n",
    "×œ×¡×™×›×•×, PySpark ××¡×¤×§ ×™×›×•×œ×•×ª ×¢×•×¦××ª×™×•×ª ×œ×§×¨×™××” ×•×›×ª×™×‘×” ×©×œ ×§×‘×¦×™ JSON, ×•×××¤×©×¨ ×©×™×œ×•×‘ ×—×œ×§ ×¢× ××§×•×¨×•×ª × ×ª×•× ×™× ×•×¡×™× ×§×™× ×©×•× ×™×. ×¢×œ ×™×“×™ ××™× ×•×£ API ×©×œ JSON ×‘-PySpark, ××©×ª××©×™× ×™×›×•×œ×™× ×œ×§×œ×•×˜, ×œ×¢×‘×“ ×•×œ× ×ª×— ×‘×™×¢×™×œ×•×ª × ×ª×•× ×™ JSON ×‘×§× ×” ××™×“×”, ×•×œ×¨×ª×•× ××ª ×›×•×—×• ×©×œ big data analytics.\n",
    "\n",
    "×œ××•×¨×š ×”××“×¨×™×š ×”×–×”, ×œ××“×ª ×ª×•×‘× ×•×ª ×œ×’×‘×™ ×§×¨×™××ª JSON files ×¢× both single-line ×•-multiline records ×œ-PySpark DataFrames. ×‘× ×•×¡×£, ×œ××“×ª ×˜×›× ×™×§×•×ª ×œ×§×¨×™××ª ×§×‘×¦×™× ×‘×•×“×“×™× ×•××¨×•×‘×™× ×‘×• ×–×× ×™×ª, ×›××• ×’× ×©×™×˜×•×ª ×œ×›×ª×™×‘×ª × ×ª×•× ×™ DataFrame ×‘×—×–×¨×” ×œ×§×‘×¦×™ JSON, ×©×•××¨ schema.\n",
    "\n",
    "### ğŸ’¡ ×œ××™×“×” ××”× ×”!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š ××××¨×™× ×§×©×•×¨×™×\n",
    "\n",
    "- PySpark Read and Write Parquet File\n",
    "- PySpark Read and Write SQL Server Table\n",
    "- PySpark Read and Write MySQL Database Table\n",
    "- PySpark Read CSV file into DataFrame\n",
    "- PySpark Read JDBC Table to DataFrame\n",
    "- PySpark Read Multiple Lines (multiline) JSON File\n",
    "- PySpark Write to CSV File\n",
    "- PySpark repartition() â€“ Explained with Example\n",
    "- PySpark SparkContext Explained\n",
    "- Iterate over Elements of Array in PySpark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”— ××§×•×¨×•×ª:\n",
    "\n",
    "- [Databricks read JSON](https://docs.databricks.com/)\n",
    "- [Spark json datasource](https://spark.apache.org/docs/latest/sql-data-sources-json.html)\n",
    "- [jsonlines.org](https://jsonlines.org/)\n",
    "- [json.org](https://www.json.org/)\n",
    "- [Spark JsonFileFormat scala class](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
