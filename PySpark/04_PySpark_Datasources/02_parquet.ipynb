{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    body, * {\n",
    "        direction: rtl !important;\n",
    "        text-align: right !important;\n",
    "    }\n",
    "</style>\n",
    "# ğŸ“¦ PySpark - ×§×¨×™××” ×•×›×ª×™×‘×” ×©×œ ×§×•×‘×¥ Parquet\n",
    "[PySpark Read and Write Parquet File](https://sparkbyexamples.com/pyspark/pyspark-read-and-write-parquet-file/)\n",
    "## ××‘×•×\n",
    "\n",
    "Pyspark SQL ××¡×¤×§ ×©×™×˜×•×ª ×œ×§×¨×™××” ×©×œ Parquet file ×œ-DataFrame ×•×›×ª×™×‘×” ×œ-Parquet file, `parquet()` ××”-DataFrameReader ×•-DataFrameWriter ××©××©×™× ×œ×§×¨×™××” ××§×•×‘×¥ Parquet ×œ-DataFrame ×•×›×ª×™×‘×” ×œ-Parquet file ×‘×”×ª×××”.\n",
    "\n",
    "Parquet files ×©×•××¨×™× ××ª ×”×¡×›×™××” ×™×—×“ ×¢× ×”× ×ª×•× ×™× ×•××›××Ÿ ×©×”×•× ××©××© ×œ×¢×ª×™× ×§×¨×•×‘×•×ª ×œ×¢×™×‘×•×“ structured files.\n",
    "\n",
    "×‘××××¨ ×–×”, ××¡×‘×™×¨ ×›×™×¦×“ ×œ×›×ª×•×‘ parquet file ×-DataFrame ×‘-PySpark, ×›×™×¦×“ ×œ×§×¨×•×, ×•×’× ××¡×‘×™×¨ ×›×™×¦×“ ×œ×¢×©×•×ª partitions ×¢×œ parquet files ×›×“×™ ×œ×©×¤×¨ ××ª ×”×‘×™×¦×•×¢×™×.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ ××” ×–×” Parquet File?\n",
    "\n",
    "Apache Parquet file ×”×•× ×¤×•×¨××˜ ××—×¡×•×Ÿ columnar ×–××™×Ÿ ×œ×›×œ ×¤×¨×•×™×§×˜ ×‘-Hadoop ecosystem, ×œ×œ× ×§×©×¨ ×œ×‘×—×™×¨×ª framework ×¢×™×‘×•×“ ×”× ×ª×•× ×™×, ××•×“×œ ×”× ×ª×•× ×™× ××• ×©×¤×ª ×”×ª×›× ×•×ª.\n",
    "\n",
    "### ğŸ¯ ×™×ª×¨×•× ×•×ª:\n",
    "\n",
    "×‘×–××Ÿ ×©××™×œ×ª×ª ××—×¡×•×Ÿ columnar, ×”×•× ××“×œ×’ ×¢×œ × ×ª×•× ×™× ×©××™× × ×¨×œ×•×•× ×˜×™×™× ×××•×“ ×‘××”×™×¨×•×ª, ××” ×©×”×•×¤×š ××ª ×‘×™×¦×•×¢ ×”-aggregation queries ××”×™×¨ ×™×•×ª×¨. ×›×ª×•×¦××” ××›×š, aggregation queries ×¦×•×¨×›×•×ª ×¤×—×•×ª ×–××Ÿ ×‘×”×©×•×•××” ×œ-row-oriented databases.\n",
    "\n",
    "×”×•× ××¡×•×’×œ ×œ×ª××•×š ×‘compression options ×•-encoding schemes ××ª×§×“××™×."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”§ ×“×•×’××” ×œ-Apache Parquet Pyspark\n",
    "\n",
    "××›×™×•×•×Ÿ ×©××™×Ÿ ×œ× ×• ××ª ×§×•×‘×¥ ×”-parquet, ×‘×•××• × ×™×¦×•×¨ DataFrame ××¨×©×™××ª × ×ª×•× ×™× ×‘×××¦×¢×•×ª `spark.createDataFrame()` method."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×¦×™×¨×ª × ×ª×•× ×™× ×œ×“×•×’××”\n",
    "# ×™×™×‘×•××™×\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"parquetFile\").getOrCreate()\n",
    "\n",
    "data = [(\"James\", \"\", \"Smith\", \"36636\", \"M\", 3000),\n",
    "        (\"Michael\", \"Rose\", \"\", \"40288\", \"M\", 4000),\n",
    "        (\"Robert\", \"\", \"Williams\", \"42114\", \"M\", 4000),\n",
    "        (\"Maria\", \"Anne\", \"Jones\", \"39192\", \"F\", 4000),\n",
    "        (\"Jen\", \"Mary\", \"Brown\", \"\", \"F\", -1)\n",
    "        ]\n",
    "\n",
    "columns = [\"firstname\", \"middlename\", \"lastname\", \"dob\", \"gender\", \"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.show()\n",
    "df.printSchema()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×–×” ×™×•×¦×¨ DataFrame ×¢× ×¢××•×“×•×ª `firstname`, `middlename`, `lastname`, `dob`, `gender`, `salary` ×•×‘××•×¤×Ÿ ×“×™×¤×•×œ×˜×™ ×›×œ ×¡×•×’×™ ×”× ×ª×•× ×™× ×©×œ ×”×¢××•×“×•×ª ××˜×•×¤×œ×™× ×›-String."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¾ Pyspark Write DataFrame ×œ×§×•×‘×¥ Parquet\n",
    "\n",
    "×¢×›×©×™×• ×‘×•××• × ×›×ª×•×‘ ××ª ×”-DataFrame ×©×™×¦×¨× ×• ×œ-parquet file ×‘×××¦×¢×•×ª ×§×¨×™××” ×œ-`parquet()` ××”-DataFrameWriter class."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×›×ª×™×‘×ª DataFrame ×œ×¤×•×¨××˜ parquet ×‘×××¦×¢×•×ª write.parquet()\n",
    "df.write.parquet(\"tmp/02_parquet/people.parquet\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "×–×” ×™×•×¦×¨ parquet file ×‘-parquet format. ×›××©×¨ ××ª×” ××¨×™×¥ ××ª ×”×ª×•×›× ×™×ª ×”×–×• ×-Spyder IDE, ×”×•× ×™×•×¦×¨ `people.parquet` file ×•-`spark-warehouse` directory ×ª×—×ª ×”×ª×™×§×™×™×” ×”× ×•×›×—×™×ª."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“– Pyspark Read Parquet file ×œ-DataFrame\n",
    "\n",
    "Pyspark ××¡×¤×§ ×©×™×˜×ª `parquet()` ×‘-DataFrameReader class ×›×“×™ ×œ×§×¨×•× ××ª ×§×•×‘×¥ ×”-parquet ×œ-dataframe. ×‘×“×•×’××” ×œ××˜×” ×–×” ×§×•×¨× ××ª ×§×•×‘×¥ ×”-parquet ×©×™×¦×¨× ×• ×‘×¡×¢×™×£ ×”×§×•×“×."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×§×¨×™××ª parquet file ×‘×××¦×¢×•×ª read.parquet()\n",
    "parDF = spark.read.parquet(\"tmp/02_parquet/people.parquet\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š ×¡×•×’×™ Saving Modes ×©×œ Parquet File\n",
    "\n",
    "×‘-PySpark, ×œ××—×¨ ×›×ª×™×‘×ª ×”-DataFrame ×œ-Parquet file, ×× ×—× ×• ×™×›×•×œ×™× ×œ×¦×™×™×Ÿ ××ª mode ×©×œ ×”×©××™×¨×” ×‘×××¦×¢×•×ª ×©×™×˜×ª `.mode()` ××”-pyspark.sql.DataFrameWriter.mode module.\n",
    "\n",
    "### ×ª×—×‘×™×¨\n",
    "```\n",
    "DataFrameWriter.mode(saveMode: Optional[str])\n",
    "```\n",
    "\n",
    "### ××¤×©×¨×•×™×•×ª:\n",
    "\n",
    "**append**: mode ×–×” ××•×¡×™×£ ××ª ×”× ×ª×•× ×™× ×œ-DataFrame ××”-file ×”×§×™×™×, ×× ×§×‘×¦×™ ×”×™×¢×“ ×œ× ×§×™×™××™×, ×”×•× ×™×•×¦×¨ parquet file ×—×“×©.\n",
    "\n",
    "×“×•×’××”:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# df.write.mode(\"append\").parquet(\"/path/to/parquet/file\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**overwrite**: mode ×–×” ×“×•×¨×¡ ××ª ×§×•×‘×¥ ×”-Parquet ×”×™×¢×“ ×¢× ×”× ×ª×•× ×™× ××”-DataFrame. ×× ×”×§×•×‘×¥ ×œ× ×§×™×™×, ×”×•× ×™×•×¦×¨ Parquet file ×—×“×©.\n",
    "\n",
    "×“×•×’××”:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# df.write.mode(\"overwrite\").parquet(\"/path/to/parquet/file\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ignore**: ×× ×§×•×‘×¥ Parquet ×”×™×¢×“ ×›×‘×¨ ×§×™×™×, mode ×–×” ×œ× ×¢×•×©×” ×›×œ×•× ×•××™× ×• ×›×•×ª×‘ ××ª ×”-DataFrame ×œ×§×•×‘×¥. ×× ×”×§×•×‘×¥ ×œ× ×§×™×™×, ×”×•× ×™×•×¦×¨ Parquet file ×—×“×©.\n",
    "\n",
    "×“×•×’××”:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# df.write.mode(\"ignore\").parquet(\"/path/to/parquet/file\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**error ××• errorifexists**: ×–×” ×–×•×¨×§ error ×× ×§×•×‘×¥ Parquet ×”×™×¢×“ ×›×‘×¨ ×§×™×™×. ×–×• ×”×”×ª× ×”×’×•×ª ×”×“×™×¤×•×œ×˜×™×ª ×× ×œ× ×¦×•×™×Ÿ saving mode.\n",
    "\n",
    "×“×•×’××”:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# df.write.mode(\"error\").parquet(\"/path/to/parquet/file\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ” ×‘×™×¦×•×¢ SQL queries ×¢×œ Parquet file\n",
    "\n",
    "Pyspark Sql ××¡×¤×§ ×œ×™×¦×•×¨ temporary view ×¢×œ parquet files ×›×“×™ ×œ×‘×¦×¢ sql queries.\n",
    "\n",
    "×§×¨×™××” ×©×œ parquet files ×××¤×©×¨×ª ×œ×™×¦×•×¨ views/tables ×–×× ×™×•×ª ×‘×××¦×¢×•×ª `createOrReplaceTempView()`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×©×™××•×© ×‘-spark sql\n",
    "parDF.createOrReplaceTempView(\"ParquetTable\")\n",
    "parkSQL = spark.sql(\"select * from ParquetTable where salary >= 4000 \")\n",
    "parkSQL.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ×™×¦×™×¨×ª Parquet partition file\n",
    "\n",
    "×›×©×× ×—× ×• ××‘×¦×¢×™× query ××¡×•×™× ×¢×œ parquet table, ×× ×—× ×• ×¡×•×¨×§×™× ××ª ×›×œ ×”×©×•×¨×•×ª ×•×›×œ ×”partitions ×‘×˜×‘×œ×”, ×–×” × ×ª×¤×¡ ×‘×¢×™×™×ª×™ ×›××©×¨ ×™×© ×œ× ×• partitions ×¨×‘×•×ª ×‘-Hive. ×–×” ×œ× ×‘×¨×•×¨ ×××•×“ ×‘×“×•×’××” ×”× ×•×›×—×™×ª ×›×™ ×™×© ×œ× ×• ×¨×§ ×©×ª×™ partitions \"gender\" followed by \"salary\".\n",
    "\n",
    "×”×©×ª××© ×‘-`partitionBy()` ×©×œ pyspark.sql.DataFrameWriter ×›×“×™ ×œ×™×¦×•×¨ parquet file ×‘×“×¨×š ××—×•×œ×§×ª."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.write.partitionBy(\"gender\", \"salary\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"tmp/02_parquet/people2.parquet\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "×›×©××ª×” ×‘×•×“×§ ××ª people2.parquet file, ××ª×” ×¨×•××” partitions \"gender\" followed by \"salary\" ×‘×ª×•×›×”.\n",
    "\n",
    "### ×ª×•×›×Ÿ ×”×ª×™×§×™×™×”:\n",
    "\n",
    "```\n",
    "tmp > 02_parquet > people2.parquet\n",
    "  gender=F\n",
    "    salary=-1\n",
    "    salary=4000\n",
    "  gender=M\n",
    "    salary=3000\n",
    "    salary=4000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ ×§×¨×™××” ×-Partitioned Parquet file\n",
    "\n",
    "×”×“×•×’××” ×œ××˜×” ××¡×‘×™×¨×” ×§×¨×™××ª partitioned parquet file ×œ-DataFrame ×¢× gender=M."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "parDF2 = spark.read.parquet(\"tmp/02_parquet/people2.parquet/gender=M\")\n",
    "parDF2.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ·ï¸ ×™×¦×™×¨×ª table ×¢×œ Partitioned Parquet file\n",
    "\n",
    "×›××Ÿ, ×× ×™ ×™×•×¦×¨ temporary view ×¢×œ partitioned parquet file ×•××‘×¦×¢ query ×©××‘×¦×¢ ×¤×¢×•×œ×•×ª ××”×™×¨×•×ª ×™×•×ª×¨ ××”×˜×‘×œ×” ×œ×œ× partitioning, ×•××›××Ÿ ××©×¤×¨ ××ª ×”×‘×™×¦×•×¢×™×."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×¦×™×¨×ª temporary view ×¢×œ Parquet File ××—×•×œ×§\n",
    "spark.sql(\"CREATE TEMPORARY VIEW PERSON USING parquet OPTIONS (path \\\"tmp/02_parquet/people2.parquet\\\")\")\n",
    "spark.sql(\"SELECT * FROM PERSON\").show()\n",
    "\n",
    "df.write.partitionBy(\"gender\", \"salary\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"tmp/02_parquet/people2.parquet\")\n",
    "\n",
    "spark.sql(\"CREATE TEMPORARY VIEW PERSON2 USING parquet OPTIONS (path \\\"tmp/02_parquet/people2.parquet\\\")\")\n",
    "spark.sql(\"SELECT * FROM PERSON2 \").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ×“×•×’××” ××œ××” ×©×œ PySpark read ×•×›×ª×™×‘×ª Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×™×‘×•××™×\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"parquetFile\").getOrCreate()\n",
    "\n",
    "data = [(\"James\", \"\", \"Smith\", \"36636\", \"M\", 3000),\n",
    "        (\"Michael\", \"Rose\", \"\", \"40288\", \"M\", 4000),\n",
    "        (\"Robert\", \"\", \"Williams\", \"42114\", \"M\", 4000),\n",
    "        (\"Maria\", \"Anne\", \"Jones\", \"39192\", \"F\", 4000),\n",
    "        (\"Jen\", \"Mary\", \"Brown\", \"\", \"F\", -1)\n",
    "        ]\n",
    "\n",
    "columns = [\"firstname\", \"middlename\", \"lastname\", \"dob\", \"gender\", \"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "df.write.mode(\"overwrite\").parquet(\"tmp/02_parquet/people.parquet\")\n",
    "\n",
    "parDF1 = spark.read.parquet(\"tmp/02_parquet/people.parquet\")\n",
    "parDF1.createOrReplaceTempView(\"ParquetTable\")\n",
    "parDF1.printSchema()\n",
    "parDF1.show(truncate=False)\n",
    "\n",
    "parkSQL = spark.sql(\"select * from ParquetTable where salary >= 4000 \")\n",
    "parkSQL.show(truncate=False)\n",
    "\n",
    "spark.sql(\"CREATE TEMPORARY VIEW PERSON3 USING parquet OPTIONS (path \\\"tmp/02_parquet/people.parquet\\\")\")\n",
    "spark.sql(\"SELECT * FROM PERSON3\").show()\n",
    "\n",
    "df.write.partitionBy(\"gender\", \"salary\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"tmp/02_parquet/people2.parquet\")\n",
    "\n",
    "parDF2 = spark.read.parquet(\"tmp/02_parquet/people2.parquet/gender=M\")\n",
    "parDF2.show(truncate=False)\n",
    "\n",
    "spark.sql(\"CREATE TEMPORARY VIEW PERSON4 USING parquet OPTIONS (path \\\"tmp/02_parquet/people2.parquet\\\")\")\n",
    "spark.sql(\"SELECT * FROM PERSON4\").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ×¡×™×›×•×:\n",
    "\n",
    "×œ××“× ×• ×›×™×¦×“ ×œ×›×ª×•×‘ parquet file ×-PySpark DataFrame ×•×§×¨×™××” ×©×œ parquet files, ×œ×™×¦×•×¨ views/tables ×œ-parquet files ×•×¢×•×“ × ×•×©××™× ×§×©×•×¨×™×.\n",
    "\n",
    "×ª×§×•×•×” ×©××”×‘×ª ××ª ×–×”, ×”×©××™×¨×• ×ª×’×•×‘×” ×‘×§×˜×¢ ×”×ª×’×•×‘×•×ª.\n",
    "\n",
    "### ğŸ’¡ ×œ××™×“×” ××”× ×”!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š ××××¨×™× ×§×©×•×¨×™×\n",
    "\n",
    "- PySpark Shell Command Usage with Examples\n",
    "- PySpark Retrieve DataType & Column Names of DataFrame\n",
    "- PySpark Parse JSON from String Column | TEXT File\n",
    "- PySpark SQL Types (DataType) with Examples\n",
    "- PySpark SparkContext Explained\n",
    "- Explain PySpark inferSchema\n",
    "- PySpark Schema Strategies: When to Use inferSchema, mergeSchema, and overwriteSchema"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
