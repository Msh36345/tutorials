{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<style>\n",
    "    body, * {\n",
    "        direction: rtl !important;\n",
    "        text-align: right !important;\n",
    "    }\n",
    "</style>\n",
    "מקור: [Spark By Examples - PySpark RDD](https://sparkbyexamples.com/pyspark-rdd/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📚 מדריך PySpark RDD - למידה עם דוגמאות\n",
    "\n",
    "מדריך מקיף ללמידת PySpark RDD (Resilient Distributed Dataset) בעברית עם דוגמאות קוד מעשיות.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📖 תוכן עניינים\n",
    "\n",
    "1. [מבוא ל-RDD](#intro)\n",
    "2. [מהו RDD?](#what-is-rdd)\n",
    "3. [יתרונות PySpark RDD](#benefits)\n",
    "4. [מגבלות PySpark RDD](#limitations)\n",
    "5. [יצירת RDD](#creation)\n",
    "6. [פרטישנים (Partitions)](#partitions)\n",
    "7. [רה-פרטישן וקואלס (Repartition and Coalesce)](#repartition)\n",
    "8. [פעולות RDD](#operations)\n",
    "9. [סוגי RDD](#types)\n",
    "10. [פעולות Shuffle](#shuffle)\n",
    "11. [Persist RDD](#persist)\n",
    "12. [משתנים משותפים](#shared-variables)\n",
    "13. [Advanced API - DataFrame & DataSet](#advanced)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## 🔍 מבוא ל-RDD\n",
    "\n",
    "**RDD (Resilient Distributed Dataset)** הוא אבן הבניין המרכזית של PySpark. \n",
    "\n",
    "RDD הוא אוסף מבוזר של אובייקטים שהוא:\n",
    "- **Fault-tolerant** (עמיד בפני תקלות)\n",
    "- **Immutable** (בלתי ניתן לשינוי)\n",
    "- **Distributed** (מבוזר)\n",
    "\n",
    "### תכונות עיקריות:\n",
    "- ברגע שיוצרים RDD, לא ניתן לשנות אותו\n",
    "- הנתונים ב-RDD מחולקים לפרטישנים לוגיים\n",
    "- מאפשר חישוב מבוזר על פני מספר צמתים בקלאסטר\n",
    "\n",
    "💡 **שימו לב:** מדריך זה יעזור לכם להבין מהו RDD, את היתרונות שלו, וכיצד ליצור ולהשתמש ב-RDD עם דוגמאות מ-GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='what-is-rdd'></a>\n",
    "## 💡 1. מהו RDD (Resilient Distributed Dataset)?\n",
    "\n",
    "**RDD (Resilient Distributed Dataset)** משמש כרכיב ליבה ב-PySpark, ומציע אוסף מבוזר עמיד בתקלות. \n",
    "\n",
    "### השוואה ל-Python:\n",
    "RDDs דומים לרשימות ב-Python, אך ההבדל הוא ש-RDD מחושב על מספר תהליכים המפוזרים על פני מספר שרתים פיזיים (צמתים בקלאסטר), בעוד שאוסף Python חי ומעובד בתהליך אחד בלבד.\n",
    "\n",
    "### יתרונות מרכזיים:\n",
    "- 🚀 **Fault Tolerance** - סובלנות לתקלות\n",
    "- 📊 **Scalability** - יכולת התרחבות\n",
    "- ⚡ **Parallel Processing** - עיבוד מקבילי\n",
    "\n",
    "באמצעות שימוש ב-RDDs, PySpark מפיק תועלת מ-fault tolerance, scalability, ויכולות עיבוד מקבילי, המאפשרים טיפול יעיל במשימות עיבוד נתונים בקנה מידה גדול."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='benefits'></a>\n",
    "## ✨ 2. יתרונות PySpark RDD\n",
    "\n",
    "PySpark אומץ באופן נרחב בקהילת Machine Learning ו-Data Science בשל היתרונות שלו על פני תכנות Python מסורתי."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💾 עיבוד In-Memory\n",
    "\n",
    "PySpark טוען את הנתונים מהדיסק ומעבד אותם בזיכרון, ושומר את הנתונים בזיכרון. זהו ההבדל העיקרי בין PySpark ל-MapReduce (אינטנסיבי I/O).\n",
    "\n",
    "בין הטרנספורמציות, ניתן גם לשמור/להחזיק (cache/persist) את ה-RDD בזיכרון כדי לעשות שימוש חוזר בחישובים הקודמים."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔒 אי-שינוי (Immutability)\n",
    "\n",
    "PySpark RDDs הם בלתי ניתנים לשינוי באופיים, כלומר ברגע שנוצרים RDDs לא ניתן לשנות אותם.\n",
    "\n",
    "כאשר אנו מבצעים טרנספורמציות על RDD, PySpark יוצר RDD חדש ושומר על קו המוצא (Lineage) של ה-RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🛡️ סובלנות לתקלות (Fault Tolerance)\n",
    "\n",
    "PySpark פועל על מאגרי נתונים עמידים בתקלות כמו HDFS, S3 וכו'.\n",
    "\n",
    "לכן, אם כל פעולת RDD נכשלת, היא אוטומטית טוענת מחדש את הנתונים מפרטישנים אחרים.\n",
    "\n",
    "כמו כן, כאשר אפליקציות PySpark רצות על קלאסטר, כשלי משימות PySpark מתאוששים אוטומטית למספר מסוים של פעמים (לפי התצורה) ומסיימים את האפליקציה בצורה חלקה."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🦥 אבולוציה עצלה (Lazy Evolution)\n",
    "\n",
    "PySpark אינו מעריך את טרנספורמציות ה-RDD כפי שהן מופיעות/נתקלות על ידי הדרייבר.\n",
    "\n",
    "במקום זאת, הוא שומר את כל הטרנספורמציות כפי שהן נתקלות (DAG) ומעריך את כל הטרנספורמציות כאשר הוא רואה את הפעולה הראשונה של RDD.\n",
    "\n",
    "זה מאפשר אופטימיזציה טובה יותר של ביצועים."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 פרטישן (Partitioning)\n",
    "\n",
    "כאשר יוצרים RDD מנתונים, כברירת מחדל הוא מחלק את האלמנטים ב-RDD.\n",
    "\n",
    "כברירת מחדל הוא מחלק למספר הליבות הזמינות.\n",
    "\n",
    "מאפיין זה מאפשר הפצה אוטומטית של הנתונים ואופטימיזציה של ביצועים."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='limitations'></a>\n",
    "## ⚠️ 3. מגבלות PySpark RDD\n",
    "\n",
    "PySpark RDDs אינם מתאימים במיוחד לאפליקציות שמבצעות עדכונים למאגר המצב, כגון מערכות אחסון לאפליקציית אינטרנט.\n",
    "\n",
    "עבור אפליקציות כאלה, יעיל יותר להשתמש במערכות שמבצעות תיעוד עדכונים מסורתי וצ'קפוינטינג של נתונים, כמו מסדי נתונים.\n",
    "\n",
    "**מטרת ה-RDD** היא לספק מודל תכנות יעיל עבור אנליטיקת באצ'ים ולהשאיר אפליקציות אסינכרוניות אלה.\n",
    "\n",
    "💡 **לסיכום:** RDD מתאים במיוחד לעיבוד באצ'ים ואנליזה, לא לעדכונים תכופים של מצב."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='creation'></a>\n",
    "## 🏗️ 4. יצירת RDD\n",
    "\n",
    "ניתן ליצור RDD על ידי:\n",
    "1. **מקבול (parallelizing)** אוסף קיים\n",
    "2. **הפניה למערכת אחסון חיצונית** (HDFS, S3 ועוד)\n",
    "\n",
    "לפני שנסתכל על דוגמאות, נאתחל תחילה SparkSession באמצעות שיטת ה-builder המוגדרת במחלקה SparkSession."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 אתחול SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ייבוא SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# יצירת SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📝 הסברים על הפרמטרים:\n",
    "\n",
    "**master()** - אם אתם מריצים את זה על קלאסטר, תצטרכו להשתמש בשם המאסטר שלכם כארגומנט ל-master().\n",
    "- בדרך כלל, זה יהיה `yarn` (Yet Another Resource Negotiator) או `mesos` תלוי בהגדרת הקלאסטר שלכם.\n",
    "\n",
    "**local[x]** - כאשר פועלים במצב Standalone, ציינו `local[x]`, כאשר `x` הוא מספר שלם גדול מ-0, כדי לקבוע את מספר הפרטישנים עבור RDDs.\n",
    "- באופן אידיאלי, הגדירו `x` להתאים למספר ליבות ה-CPU הזמינות במערכת שלכם לביצועים אופטימליים.\n",
    "\n",
    "**appName()** - משמש להגדרת שם האפליקציה שלכם.\n",
    "\n",
    "**getOrCreate()** - מחזיר אובייקט SparkSession אם כבר קיים, ויוצר חדש אם לא קיים.\n",
    "\n",
    "💡 **שימו לב:** יצירת אובייקט SparkSession יוצרת באופן פנימי SparkContext אחד לכל JVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔄 יצירת RDD באמצעות sparkContext.parallelize()\n",
    "\n",
    "באמצעות שימוש בפונקציה `parallelize()` של SparkContext (`sparkContext.parallelize()`) ניתן ליצור RDD.\n",
    "\n",
    "פונקציה זו טוענת את האוסף הקיים מתוכנית הדרייבר שלכם ל-RDD מקבולי.\n",
    "\n",
    "שיטה זו של יצירת RDD משמשת כאשר יש לכם כבר נתונים בזיכרון שנטענים מקובץ או ממסד נתונים, וכל הנתונים חייבים להיות נוכחים בתוכנית הדרייבר לפני יצירת ה-RDD."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# יצירת RDD מ-parallelize\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RDD Creation Diagram](https://sparkbyexamples.com/wp-content/uploads/2020/08/rdd-creation.png)\n",
    "\n",
    "### 📊 תרשים יצירת RDD\n",
    "התרשים ממחיש כיצד רשימת Python מתחלקת לפרטישנים שונים ב-RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "עבור אפליקציות ייצור, אנו בעיקר יוצרים RDD באמצעות שימוש במערכות אחסון חיצוניות כמו HDFS, S3, HBase וכו'.\n",
    "\n",
    "כדי להפוך את זה פשוט למדריך PySpark RDD זה, אנו משתמשים בקבצים מהמערכת המקומית או טוענים אותו מרשימת Python כדי ליצור RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📄 יצירת RDD באמצעות sparkContext.textFile()\n",
    "\n",
    "השתמשו בשיטת `textFile()` כדי לקרוא קובץ טקסט ל-RDD."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# יצירת RDD ממקור נתונים חיצוני\n",
    "rdd2 = spark.sparkContext.textFile(\"/path/textFile.txt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📚 יצירת RDD באמצעות sparkContext.wholeTextFiles()\n",
    "\n",
    "פונקציה `wholeTextFiles()` מחזירה **PairRDD** כאשר המפתח הוא נתיב הקובץ והערך הוא תוכן הקובץ."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# קריאת קובץ שלם כרשומה בודדת ב-RDD\n",
    "rdd3 = spark.sparkContext.wholeTextFiles(\"/path/textFile.txt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "מלבד שימוש בקבצי טקסט, ניתן גם **ליצור RDD מקבצי CSV**, JSON, ופורמטים נוספים."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🗂️ יצירת RDD ריק באמצעות sparkContext.emptyRDD()\n",
    "\n",
    "שימוש בשיטת `emptyRDD()` על sparkContext מאפשר ליצור RDD ללא נתונים.\n",
    "\n",
    "שיטה זה יוצרת RDD ריק ללא פרטישן."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# יצירת RDD ריק ללא פרטישן\n",
    "rdd = spark.sparkContext.emptyRDD()\n",
    "\n",
    "# Output:\n",
    "# rddString = spark.sparkContext.emptyRDD[String]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 יצירת RDD ריק עם פרטישן\n",
    "\n",
    "לפעמים אנו עשויים להזדקק לכתוב RDD ריק לקבצים לפי פרטישן.\n",
    "\n",
    "במקרה זה, עליכם ליצור RDD ריק עם פרטישן."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# יצירת RDD ריק עם פרטישן\n",
    "rdd2 = spark.sparkContext.parallelize([], 10)  # זה יוצר 10 פרטישנים"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='partitions'></a>\n",
    "## 🧩 5. פרטישנים של RDD\n",
    "\n",
    "כאשר אנו משתמשים בשיטות `parallelize()`, `textFile()` או `wholeTextFiles()` של SparkContext כדי לאתחל RDD, הוא מחלק אוטומטית את הנתונים לפרטישנים על סמך זמינות המשאבים.\n",
    "\n",
    "כאשר אתם מריצים אותו על לפטופ, הוא יוצר פרטישנים כאותו מספר הליבות הזמינות במערכת שלכם."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔢 קבלת מספר הפרטישנים\n",
    "\n",
    "`getNumPartitions()` - זוהי פונקציית RDD שמחזירה את מספר הפרטישנים שמערך הנתונים שלכם מחולק אליהם."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# קבלת מספר הפרטישנים\n",
    "print(\"Initial partition count: \" + str(rdd.getNumPartitions()))\n",
    "\n",
    "# Outputs: Initial partition count:2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚙️ הגדרת פרטישנים באופן ידני\n",
    "\n",
    "אנחנו יכולים גם להגדיר מספר של פרטישנים באופן ידני. כל מה שאנחנו צריכים זה להעביר מספר פרטישנים כפרמטר שני לפונקציות אלה, לדוגמה:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# הגדרת פרטישנים באופן ידני\n",
    "manualrdd = spark.sparkContext.parallelize([1, 2, 3, 4, 56, 7, 8, 9, 12, 3], 10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='repartition'></a>\n",
    "## 🔄 6. Repartition ו-Coalesce\n",
    "\n",
    "לפעמים, ייתכן שנצטרך לבצע repartition של ה-RDD.\n",
    "\n",
    "PySpark מספק שתי דרכים ל-repartition:\n",
    "1. **repartition()** - ערבוב מלא של כל הצמתים\n",
    "2. **coalesce()** - ערבוב מינימלי\n",
    "\n",
    "שתי הפונקציות הללו לוקחות את מספר הפרטישנים ל-repartition של RDD כפי שמוצג להלן.\n",
    "\n",
    "💡 **שימו לב:** `repartition()` היא פעולה יקרה מאוד מכיוון שהיא מערבבת נתונים מכל הצמתים בקלאסטר."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌪️ שימוש ב-repartition()\n",
    "\n",
    "השיטה `repartition()` מערבבת נתונים מכל הצמתים (המכונה גם full shuffle)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Repartition של ה-RDD\n",
    "reparRdd = rdd.repartition(4)\n",
    "print(\"re-partition count: \" + str(reparRdd.getNumPartitions()))\n",
    "\n",
    "# Outputs:\n",
    "# re-partition count:4"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 שימוש ב-coalesce()\n",
    "\n",
    "השיטה `coalesce()` מערבבת נתונים מצמתים מינימליים.\n",
    "\n",
    "לדוגמה, אם יש לכם נתונים ב-4 פרטישנים ועשיתם `coalesce(2)`, היא מעבירה נתונים מ-2 צמתים בלבד."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# שימוש ב-coalesce()\n",
    "rdd3 = rdd.coalesce(2)\n",
    "print(\"coalesce partition count: \" + str(rdd3.getNumPartitions()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 **שימו לב:** שיטות `repartition()` או `coalesce()` גם מחזירות RDD חדש."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='operations'></a>\n",
    "## ⚙️ 7. פעולות PySpark RDD\n",
    "\n",
    "פעולות RDD הן הטרנספורמציות והפעולות הליבה שמבוצעות על RDDs.\n",
    "\n",
    "### 🔄 טרנספורמציות RDD\n",
    "טרנספורמציות הן פעולות עצלות. במקום לעדכן RDD, פעולות אלה מחזירות RDD אחר.\n",
    "\n",
    "### ⚡ פעולות RDD (Actions)\n",
    "פעולות (operations) שמפעילות חישוב ומחזירות ערכי RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔄 טרנספורמציות RDD עם דוגמה\n",
    "\n",
    "טרנספורמציות על PySpark RDD מחזירות RDD אחר, והטרנספורמציות הן עצלות, כלומר הן לא מבצעות עד שאתם קוראים לפעולה על RDD.\n",
    "\n",
    "כמה טרנספורמציות על RDDs הן:\n",
    "- `flatMap()`\n",
    "- `map()`\n",
    "- `reduceByKey()`\n",
    "- `filter()`\n",
    "- `sortByKey()`\n",
    "\n",
    "ומחזירות RDD חדש במקום לעדכן את הנוכחי.\n",
    "\n",
    "בואו נשתמש בכמה מהטרנספורמציות הנפוצות ביותר כדי לבצע ספירת מילים."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "תחילה, באמצעות `textFile()`, קראו את קובץ הטקסט ל-RDD.\n",
    "\n",
    "קובץ הטקסט המשמש בדוגמה זו זמין בפרויקט GitHub."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# קריאת נתונים מקובץ טקסט\n",
    "rdd = spark.sparkContext.textFile(\"/tmp/test.txt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🗺️ flatMap\n",
    "\n",
    "הטרנספורמציה `flatMap()` ב-RDD API משטחת את ה-RDD המתקבל לאחר החלת פונקציה על כל אלמנט, ומייצרת RDD חדש.\n",
    "\n",
    "בדוגמה שסופקה להלן, כל רשומה מפוצלת בתחילה על ידי רווח בתוך RDD, ולאחר מכן, הטרנספורמציה משטחת אותה.\n",
    "\n",
    "ה-RDD המתקבל מורכב מרשומות בודדות, כאשר כל אחד מכיל מילה בודדת."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# פיצול הנתונים לפי רווח ושיטוח\n",
    "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🗺️ map\n",
    "\n",
    "הטרנספורמציה `map()` משמשת לביצוע פעולות מורכבות שונות, כגון הוספה או עדכון אלמנט.\n",
    "\n",
    "התוצאה של טרנספורמציות map שומרת על אותו מספר רשומות כמו הקלט.\n",
    "\n",
    "בדוגמת ספירת המילים שלנו, נוסף עמודה חדשה עם ערך של 1 לכל מילה. התוצאה של ה-RDD שלנו מורכבת מזוגות מפתח-ערך, כאשר כל מפתח, המייצג מילה מסוג String, משויך לערך של 1, מסוג Int."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# החלת טרנספורמציית map() להוספת אלמנט עם ערך 1 לכל מילה\n",
    "rdd3 = rdd2.map(lambda x: (x, 1))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔄 reduceByKey\n",
    "\n",
    "`reduceByKey()` משלבת את הערכים המשויכים לכל מפתח באמצעות הפונקציה המסופקת.\n",
    "\n",
    "בתרחיש שלנו, היא מצברת את מחרוזות המילים באמצעות שימוש בפונקציית sum על הערכים המקבילים.\n",
    "\n",
    "התוצאה של ה-RDD שלנו מורכבת ממילים נבדלות יחד עם הספירה הרספקטיבית שלהם."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# יצירת SparkSession (שכולל SparkContext)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext  # מביאים את ה-SparkContext\n",
    "data = [(\"a\", 1), (\"b\", 2), (\"a\", 3), (\"b\", 4), (\"c\", 5)]\n",
    "rdd3 = sc.parallelize(data)\n",
    "print(rdd3.collect())\n",
    "\n",
    "# שימוש ב-reduceByKey()\n",
    "rdd5 = rdd3.reduceByKey(lambda a, b: a + b)\n",
    "print(rdd5.collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 sortByKey\n",
    "\n",
    "הטרנספורמציה `sortByKey()` מסדרת את האלמנטים של RDD על סמך המפתחות שלהם.\n",
    "\n",
    "בתרחיש שלנו, אנו בתחילה ממירים את ה-RDD מ-(String, Int) ל-(Int, String) באמצעות טרנספורמציית map.\n",
    "\n",
    "לאחר מכן, `sortByKey()` ממיינת את ה-RDD בעיקר על סמך ערכי המספרים השלמים."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# שימוש ב-sortByKey()\n",
    "\n",
    "rdd6 = rdd3.map(lambda x: (x[1], x[0])).sortByKey()\n",
    "\n",
    "# הדפסת התוצאה של rdd5 לקונסול\n",
    "print(rdd6.collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 **שימו לב:** עיינו בעמוד זה לרשימה המלאה של [טרנספורמציות RDD](https://sparkbyexamples.com/pyspark/pyspark-rdd-transformations/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚡ פעולות RDD עם דוגמה\n",
    "\n",
    "פעולות RDD (RDD Action operations) מפעילות את ביצוע הטרנספורמציות על RDDs (Resilient Distributed Datasets) ומייצרות תוצאה שניתן להחזיר לתוכנית הדרייבר או לשמור למערכת אחסון חיצונית.\n",
    "\n",
    "נמשיך להשתמש בדוגמת ספירת המילים שלנו ונבצע כמה פעולות עליה."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔢 count()\n",
    "\n",
    "`count()` - מחזירה את מספר הרשומות ב-RDD."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Action - count\n",
    "print(\"Count : \" + str(rdd6.count()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🥇 first()\n",
    "\n",
    "`first()` - מחזירה את הרשומה הראשונה."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Action - first\n",
    "firstRec = rdd6.first()\n",
    "print(\"First Record : \" + str(firstRec[0]) + \",\" + str(firstRec[1]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🏆 max()\n",
    "\n",
    "`max()` - מחזירה את הרשומה המקסימלית."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Action - max\n",
    "datMax = rdd6.max()\n",
    "print(\"Max Record : \" + str(datMax[0]) + \",\" + str(datMax[1]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ➖ reduce()\n",
    "\n",
    "`reduce()` - מפחית את הרשומות לבודד, ניתן להשתמש בזה לספור או לסכם."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Action - reduce\n",
    "totalWordCount = rdd6.reduce(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "print(\"dataReduce Record : \" + str(totalWordCount[0]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📋 take()\n",
    "\n",
    "`take()` - מחזירה את הרשומה שצוינה כארגומנט."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Action - take\n",
    "data3 = rdd6.take(3)\n",
    "for f in data3:\n",
    "    print(\"data3 Key:\" + str(f[0]) + \", Value:\" + str(f[1]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 collect()\n",
    "\n",
    "`collect()` - מחזירה את כל הנתונים מ-RDD כמערך.\n",
    "\n",
    "היו זהירים כאשר אתם משתמשים בפעולה זו כאשר אתם עובדים עם RDD ענק עם מיליונים ומיליארדי נתונים, מכיוון שייתכן שתיגמר הזיכרון בדרייבר."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Action - collect\n",
    "data = rdd6.collect()\n",
    "for f in data:\n",
    "    print(\"Key:\" + str(f[0]) + \", Value:\" + str(f[1]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💾 saveAsTextFile()\n",
    "\n",
    "`saveAsTextFile()` - באמצעות שימוש בפעולה saveAsTextFile, ניתן לכתוב את ה-RDD לקובץ טקסט."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# שמירת RDD לקובץ טקסט\n",
    "# rdd6.saveAsTextFile(\"/tmp/wordCount\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 **שימו לב:** עיינו בעמוד זה לרשימה המלאה של [פעולות RDD](https://sparkbyexamples.com/pyspark/pyspark-rdd-actions/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='types'></a>\n",
    "## 📑 8. סוגי RDD\n",
    "\n",
    "ישנם מספר סוגים של RDDs ב-PySpark:\n",
    "\n",
    "### סוגי RDD עיקריים:\n",
    "\n",
    "**PairRDDFunctions או PairRDD** - Pair RDD הוא זוג מפתח-ערך. זהו סוג ה-RDD הנפוץ ביותר.\n",
    "\n",
    "**ShuffledRDD** - נוצר כתוצאה מפעולת shuffle\n",
    "\n",
    "**DoubleRDD** - RDD המכיל מספרים כפולים\n",
    "\n",
    "**SequenceFileRDD** - RDD שנקרא מקובץ Sequence\n",
    "\n",
    "**HadoopRDD** - RDD שנקרא מקובץ Hadoop\n",
    "\n",
    "**ParallelCollectionRDD** - RDD שנוצר מאוסף מקובל"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='shuffle'></a>\n",
    "## 🌀 9. פעולות Shuffle\n",
    "\n",
    "Shuffling, ב-PySpark, משמש כאמצעי להפצה מחדש של נתונים בין executors שונים ופוטנציאלית על פני מספר מכונות.\n",
    "\n",
    "זה מתרחש כאשר פעולות טרנספורמציה ספציפיות כמו `groupByKey()`, `reduceByKey()`, ו-`join()` מוחלות על RDDs.\n",
    "\n",
    "### 💰 העלות של Shuffling\n",
    "\n",
    "תהליך ה-shuffling של PySpark גורר עלויות משמעותיות בשל הגורמים הבאים:\n",
    "\n",
    "1. **פעילויות Disk I/O**\n",
    "2. **מעורבות של סריאליזציה ודה-סריאליזציה של נתונים**\n",
    "3. **פעולות Network I/O**\n",
    "\n",
    "### 🔑 מפתח לפרטישן\n",
    "\n",
    "בעת **יצירת RDD**, PySpark לא מאחסן בהכרח את הנתונים עבור כל המפתחות בפרטישן מאז זמן היצירה, אין דרך שבה נוכל להגדיר את המפתח עבור מערך הנתונים.\n",
    "\n",
    "לפיכך, כאשר אנו מריצים את הפעולה `reduceByKey()` כדי לצבור את הנתונים על המפתחות, PySpark עושה את הדברים הבאים:\n",
    "\n",
    "תחילה, PySpark מריץ משימות map על כל הפרטישנים אשר מקבצות את כל הערכים עבור מפתח בודד.\n",
    "\n",
    "לאחר מכן, תוצאות משימות ה-map נשמרות בזיכרון.\n",
    "\n",
    "כאשר התוצאות אינן מתאימות בזיכרון, PySpark מאחסן את הנתונים בדיסק.\n",
    "\n",
    "PySpark מערבב את הנתונים הממופים על פני פרטישנים, לפעמים הוא גם מאחסן את הנתונים המעורבבים בדיסק לשימוש חוזר כאשר הוא צריך לחשב מחדש.\n",
    "\n",
    "מריץ את איסוף האשפה (garbage collection).\n",
    "\n",
    "לבסוף מריץ משימות reduce על כל פרטישן על סמך מפתח."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚡ גודל פרטישן Shuffle וביצועים\n",
    "\n",
    "תלוי בגודל מערך הנתונים שלכם, במספר הליבות, ובזיכרון הזמין, ה-shuffling של PySpark יכול לבצע אופטימיזציה או לפגוע בביצועים של העבודה שלכם.\n",
    "\n",
    "כאשר עוסקים במערכי נתונים קטנים יותר, מומלץ להפחית את פרטישני ה-shuffle כדי להימנע מלרוץ על משימות רבות עם מינימום נתונים לעיבוד.\n",
    "\n",
    "מצב זה מוביל להרצת משימות רבות עם נתונים מינימליים לעיבוד.\n",
    "\n",
    "לעומת זאת, נתונים מופרזים עם פרטישנים מועטים יותר מביאים למשימות ארוכות יותר ועלולים אפילו להוביל לשגיאות out-of-memory.\n",
    "\n",
    "קביעת גודל פרטישן shuffle אופטימלי היא מאתגרת ולעתים קרובות דורשת מספר איטרציות עם ערכים שונים כדי להשיג את האופטימיזציה הרצויה.\n",
    "\n",
    "פרמטר זה חיוני להתייחסות כאשר נתקלים בבעיות ביצועים בעבודות PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='persist'></a>\n",
    "## 💾 10. Persist RDD\n",
    "\n",
    "PySpark **Cache** ו-**Persist** הם טכניקות אופטימיזציה לשיפור ביצועי ה-RDD בעבודות שהן איטרטיביות ואינטראקטיביות.\n",
    "\n",
    "בסעיף מדריך PySpark RDD זה, אסביר איך להשתמש בשיטות persist() ו-cache() על RDD עם דוגמאות.\n",
    "\n",
    "למרות ש-PySpark מגבירה את מהירויות החישוב עד פי 100 מאשר עבודות MapReduce מסורתיות, ירידה בביצועים עלולה להתרחש כאשר עבודות נכשלות למנף חישובים חוזרים, במיוחד כאשר מטפלים במערכי נתונים מאסיביים במיליארדים או טריליונים.\n",
    "\n",
    "לכן, אופטימיזציה של חישובים הופכת חיונית לשיפור הביצועים.\n",
    "\n",
    "PySpark מציע טכניקות אופטימיזציה כמו שיטות `cache()` ו-`persist()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🗄️ RDD Cache\n",
    "\n",
    "שיטת `cache()` של PySpark RDD כברירת מחדל שומרת חישוב RDD ב**storage level `MEMORY_ONLY`**, כלומר היא תאחסן את הנתונים ב-JVM heap כאובייקטים לא מסוריאלים."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "שיטת `cache()` של PySpark במחלקה RDD קוראת באופן פנימי לשיטת `persist()` אשר בתורה משתמשת ב-`sparkSession.sharedState.cacheManager.cacheQuery` כדי לשמור במטמון את סט התוצאות של RDD.\n",
    "\n",
    "בואו נסתכל על דוגמה:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# cache()\n",
    "cachedRdd = rdd.cache()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💿 RDD Persist\n",
    "\n",
    "באמצעות שיטת `persist()`, אתה יכול לשמור את ה-RDD באחד מ-**storage levels**:\n",
    "\n",
    "- `MEMORY_ONLY`\n",
    "- `MEMORY_ONLY_SER`\n",
    "- `MEMORY_AND_DISK`\n",
    "- `MEMORY_AND_DISK_SER`\n",
    "- `DISK_ONLY`\n",
    "- `MEMORY_ONLY_2`\n",
    "- `MEMORY_AND_DISK_2`\n",
    "\n",
    "ועוד."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# המרת RDD ל-DataFrame\n",
    "df = rdd5.toDF([\"key\", \"value\"])\n",
    "\n",
    "# שמירה בזיכרון\n",
    "dfPersist = df.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "# הצגת התוכן\n",
    "dfPersist.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🗑️ RDD Unpersist\n",
    "\n",
    "PySpark עוקב אוטומטית אחר כל קריאה של `persist()` ו-`cache()`, בוחן את השימוש על פני כל צומת.\n",
    "\n",
    "הוא משליך אוטומטית נתונים מתמידים שנשארים לא בשימוש או משתמש באלגוריתם least-recently-used (LRU).\n",
    "\n",
    "בנוסף, משתמשים יכולים להסיר נתונים מתמידים באופן ידני באמצעות שיטת `unpersist()`.\n",
    "\n",
    "פעולה זו מסמנת את ה-RDD כלא מתמיד ומחסלת את כל הבלוקים המשויכים הן מהזיכרון והן מהדיסק."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# unpersist()\n",
    "dfPersist2 = dfPersist.unpersist()\n",
    "dfPersist2.show(truncate=False)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='shared-variables'></a>\n",
    "## 🔗 11. משתנים משותפים של RDD\n",
    "\n",
    "בסעיף זה של מדריך PySpark RDD, בואו נלמד מהם הסוגים השונים של משתנים משותפים של PySpark וכיצד הם משמשים בטרנספורמציות של PySpark.\n",
    "\n",
    "כאשר PySpark מבצע טרנספורמציה באמצעות `map()` או פעולות `reduce()`, הוא מבצע את הטרנספורמציות על צומת מרוחק על ידי שימוש במשתנים שנשלחים עם המשימות.\n",
    "\n",
    "והמשתנים הללו אינם נשלחים בחזרה ל-PySpark Driver, ולכן אין יכולת לעשות שימוש חוזר ולשתף את המשתנים על פני משימות.\n",
    "\n",
    "משתנים משותפים של PySpark פותרים בעיה זו באמצעות שימוש בשתי הטכניקות הבאות.\n",
    "\n",
    "PySpark מספק שני סוגים של משתנים משותפים:\n",
    "\n",
    "1. **Broadcast variables (read-only shared variable)** - משתנים לקריאה בלבד\n",
    "2. **Accumulator variables (updatable shared variables)** - משתנים ניתנים לעדכון"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📡 Broadcast Read-only Variables\n",
    "\n",
    "**Broadcast variables** הם משתנים משותפים לקריאה בלבד שנשמרים במטמון ונגישים על פני כל הצמתים בקלאסטר לשימוש על ידי משימות.\n",
    "\n",
    "במקום להעביר את הנתונים הללו עם כל משימה, PySpark משתמש באלגוריתמים יעילים של broadcast כדי להפיץ משתני broadcast למכונות, ובכך להפחית את עלויות התקשורת.\n",
    "\n",
    "יישום עיקרי של PySpark RDD Broadcast הוא עם נתוני lookup, כמו zip codes, מדינות, או חיפושי מדינות.\n",
    "\n",
    "כאשר מבצעים עבודת PySpark RDD הכוללת משתני Broadcast, PySpark מבצע את השלבים הבאים:\n",
    "\n",
    "1. PySpark מחלק את העבודה לשלבים, כל אחד עם shuffling מבוזר, ומבצע פעולות בתוך כל שלב.\n",
    "2. לאחר מכן, שלבים מאוחרים מתחלקים למשימות.\n",
    "3. PySpark משדר את הנתונים הנפוצים הנדרשים על ידי משימות בתוך כל שלב.\n",
    "4. הנתונים המשודרים נשמרים במטמון בפורמט מסוריאל ומבוטל סריאליזציה לפני ביצוע כל משימה.\n",
    "\n",
    "ה-Broadcast של PySpark נוצר באמצעות שיטת `broadcast(v)` של מחלקת SparkContext.\n",
    "\n",
    "שיטה זו לוקחת את הארגומנט v שאתה רוצה לשדר."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# יצירת broadcast variable\n",
    "broadcastVar = sc.broadcast([0, 1, 2, 3])\n",
    "broadcastVar.value"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 **שימו לב:** משתני broadcast לא נשלחים ל-executors עם קריאת sc.broadcast(variable), אלא הם יישלחו ל-executors כאשר הם משמשים לראשונה.\n",
    "\n",
    "עיינו ב-[PySpark RDD Broadcast shared variable](https://sparkbyexamples.com/pyspark/pyspark-broadcast-variables/) למשל מפורט יותר."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Accumulators\n",
    "\n",
    "PySpark Accumulators מייצגים צורה אחרת של משתנה משותף המתווסף באופן אקסקלוסיבי באמצעות פעולה אסוציאטיבית וקומוטטיבית.\n",
    "\n",
    "הם משמשים לביצוע מונים דומים למונים של MapReduce.\n",
    "\n",
    "כברירת מחדל, PySpark תומך ביצירת accumulators מכל סוג מספרי ומציע את הגמישות לשלב סוגי accumulator מותאמים אישית.\n",
    "\n",
    "מתכנתים יכולים ליצור את סוגי ה-accumulators הבאים:\n",
    "\n",
    "1. **Named accumulators**\n",
    "2. **Unnamed accumulators**\n",
    "\n",
    "יצירת named accumulator הופכת גלויה בממשק המשתמש של PySpark web תחת לשונית \"Accumulator\".\n",
    "\n",
    "בתוך לשונית זו, שני טבלאות קיימים: הטבלה הראשונית, שכותרתה \"accumulable\", כוללת את כל משתני accumulator המכונים לצד הערכים הרספקטיביים שלהם.\n",
    "\n",
    "בינתיים, הטבלה הבאה, שכותרתה \"Tasks\", מציגה את הערך של כל accumulator שהשתנה על ידי משימה.\n",
    "\n",
    "מצד שני, unnamed accumulators אינם מופיעים בממשק המשתמש של PySpark web.\n",
    "\n",
    "למטרות פרגמטיות, מומלץ להשתמש ב-named accumulators ברוב התרחישים.\n",
    "\n",
    "משתני Accumulator נוצרים באמצעות `SparkContext.longAccumulator(v)`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # יצירת accumulator variable\n",
    "# accum = sc.longAccumulator(\"SumAccumulator\")\n",
    "# sc.parallelize([1, 2, 3]).foreach(lambda x: accum.add(x))\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# יצירת Accumulator\n",
    "accum = sc.accumulator(0)  # התחלה מ-0\n",
    "\n",
    "# שימוש ב-Accumulator\n",
    "rdd = sc.parallelize([1, 2, 3])\n",
    "rdd.foreach(lambda x: accum.add(x))\n",
    "\n",
    "print(accum.value)  # יודפס 6"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='advanced'></a>\n",
    "## 🚀 12. Advanced API - DataFrame & DataSet\n",
    "\n",
    "למרות שיש לנו API מתקדמים יותר על RDD, עדיין נצטרך לעתים קרובות להמיר DataFrame ל-RDD או RDD ל-DataFrame.\n",
    "\n",
    "להלן מספר דוגמאות."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔄 יצירת RDD מ-DataFrame ולהיפך"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 2)])\n",
    "df = spark.createDataFrame(rdd, [\"col1\", \"col2\"])\n",
    "\n",
    "# המרת RDD ל-DataFrame\n",
    "dfFromRDD1 = rdd3.toDF()\n",
    "\n",
    "# המרת RDD ל-DataFrame עם שמות עמודות\n",
    "dfFromRDD2 = rdd3.toDF([\"col1\", \"col2\"])\n",
    "\n",
    "# שימוש ב-createDataFrame() להמרת DataFrame ל-RDD\n",
    "df = spark.createDataFrame(rdd).toDF(\"col1\", \"col2\")\n",
    "\n",
    "# המרת DataFrame ל-RDD\n",
    "rdd = df.rdd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 13. לסיכום - מה הלאה?\n",
    "\n",
    "בסיכום, מדריך PySpark RDD זה סיפק הבנה מעמיקה של:\n",
    "\n",
    "✅ מהו RDD ויתרונותיו  \n",
    "✅ כיצד ליצור RDD  \n",
    "✅ פעולות טרנספורמציה ופעולות  \n",
    "✅ פרטישנים ו-Shuffle  \n",
    "✅ Caching ו-Persistence  \n",
    "✅ משתנים משותפים  \n",
    "✅ המרה בין RDD ל-DataFrame  \n",
    "\n",
    "### 📚 משאבים נוספים:\n",
    "- [PySpark DataFrame Tutorial](https://sparkbyexamples.com/pyspark/pyspark-dataframe/)\n",
    "- [PySpark SQL Functions](https://sparkbyexamples.com/pyspark/pyspark-sql-functions/)\n",
    "- [PySpark Examples GitHub](https://github.com/spark-examples/pyspark-examples)\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 טיפים לתרגול:\n",
    "1. נסו להריץ כל דוגמת קוד במחברת זו\n",
    "2. שנו את הפרמטרים וראו את ההשפעה על התוצאות\n",
    "3. צרו RDD משלכם מקבצים שונים\n",
    "4. תרגלו טרנספורמציות ופעולות שונות\n",
    "5. נסו לשלב מספר פעולות יחד\n",
    "\n",
    "**בהצלחה בלמידה! 🎉**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📝 הערות וקרדיטים\n",
    "\n",
    "מחברת זו מבוססת על המדריך המקורי מ-[SparkByExamples.com](https://sparkbyexamples.com/pyspark-rdd/)\n",
    "\n",
    "תורגם ועובד לעברית עם דוגמאות נוספות והרחבות.\n",
    "\n",
    "📅 **תאריך עדכון:** נובמבר 2025\n",
    "\n",
    "🔗 **קישורים שימושיים:**\n",
    "- [תיעוד PySpark הרשמי](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [Apache Spark](https://spark.apache.org/)\n",
    "- [PySpark Tutorial](https://sparkbyexamples.com/pyspark-tutorial/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
