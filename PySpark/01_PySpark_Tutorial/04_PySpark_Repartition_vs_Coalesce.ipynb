{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<style>\n",
    "    body, * {\n",
    "        direction: rtl !important;\n",
    "        text-align: right !important;\n",
    "    }\n",
    "</style>\n",
    "××§×•×¨: [Spark By Examples - PySpark Repartition vs Coalesce](https://sparkbyexamples.com/pyspark/pyspark-repartition-vs-coalesce/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”„ PySpark: Repartition() ×œ×¢×•××ª Coalesce()\n",
    "\n",
    "## ğŸ“š ××‘×•×\n",
    "\n",
    "×‘-PySpark, ×”×‘×—×™×¨×” ×‘×™×Ÿ ×”×¤×•× ×§×¦×™×•×ª `repartition()` ×•-`coalesce()` ×—×©×•×‘×” ×‘×™×•×ª×¨ ×œ××•×¤×˜×™××™×–×¦×™×” ×©×œ ×‘×™×¦×•×¢×™× ×•× ×™×¦×•×œ ××©××‘×™×.\n",
    "\n",
    "×©×™×˜×•×ª ××œ×• ×××œ××•×ª ×ª×¤×§×™×“×™× ××¨×›×–×™×™× ×‘××¨×’×•×Ÿ ××—×“×© ×©×œ ××—×™×¦×•×ª (partitions) ×‘×ª×•×š DataFrame, ××š ×”×Ÿ ×©×•× ×•×ª ×‘×× ×’× ×•× ×™ ×”×¤×¢×•×œ×” ×©×œ×”×Ÿ ×•×”×©×œ×›×•×ª×™×”×Ÿ.\n",
    "\n",
    "### ğŸ¯ × ×§×•×“×•×ª ××¤×ª×—:\n",
    "\n",
    "**×‘××™×œ×™× ×¤×©×•×˜×•×ª:** `repartition()` ××’×“×™×œ×” ××• ××§×˜×™× ×” ××ª ×”××—×™×¦×•×ª, ×‘×¢×•×“ ×©-`coalesce()` ×¨×§ ××§×˜×™× ×” ××ª ××¡×¤×¨ ×”××—×™×¦×•×ª ×‘×¦×•×¨×” ×™×¢×™×œ×”.\n",
    "\n",
    "×‘××××¨ ×–×” × ×œ××“ ××ª ×”×”×‘×“×œ×™× ×‘×™×Ÿ repartition ×œ-coalesce ×©×œ PySpark ×¢× ×“×•×’×××•×ª ××¢×©×™×•×ª.\n",
    "\n",
    "### ğŸ“‹ ×ª×•×›×Ÿ ×¢× ×™×™× ×™×:\n",
    "\n",
    "1. **RDD Partition**\n",
    "   - RDD repartition\n",
    "   - RDD coalesce\n",
    "\n",
    "2. **DataFrame Partition**\n",
    "   - DataFrame repartition\n",
    "   - DataFrame coalesce\n",
    "\n",
    "---\n",
    "\n",
    "### âš ï¸ ×”×¢×¨×” ×—×©×•×‘×”:\n",
    "\n",
    "× ×§×•×“×” ×—×©×•×‘×” ×œ×¦×™×™×Ÿ ×”×™× ×©-`repartition()` ×•-`coalesce()` ×©×œ PySpark ×”×Ÿ **×¤×¢×•×œ×•×ª ×™×§×¨×•×ª ×××•×“** ××›×™×•×•×Ÿ ×©×”×Ÿ **××¢×¨×‘×‘×•×ª ××ª ×”× ×ª×•× ×™× ×¢×œ ×¤× ×™ ××—×™×¦×•×ª ×¨×‘×•×ª**. ×œ×›×Ÿ, ×™×© ×œ×¦××¦× ××ª ×”×©×™××•×© ×‘×”×Ÿ ×›×›×œ ×”××¤×©×¨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1ï¸âƒ£ PySpark RDD: Repartition() ×œ×¢×•××ª Coalesce()\n",
    "\n",
    "× ×™×¦×•×¨ RDD ×¢× ××—×™×¦×•×ª ×•× ×©×ª××© ×‘×• ×›×“×™ ×œ×”×“×’×™× ××ª `repartition()` ×•-`coalesce()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”§ ×™×¦×™×¨×ª Spark Session ×•×”×›× ×ª × ×ª×•× ×™×"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T11:00:51.519946Z",
     "start_time": "2025-11-09T11:00:51.489606Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()\n",
    "\n",
    "# ×™×¦×™×¨×ª spark session ×¢× localhost\n",
    "rdd = spark.sparkContext.parallelize(range(0,20))\n",
    "print(\"From local[5] : \" + str(rdd.getNumPartitions()))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From local[5] : 8\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T11:00:51.533418Z",
     "start_time": "2025-11-09T11:00:51.524628Z"
    }
   },
   "source": [
    "# Create spark session with local[5]\n",
    "rdd = spark.sparkContext.parallelize(range(0,20))\n",
    "print(\"From local[5] : \"+str(rdd.getNumPartitions()))\n",
    "\n",
    "# Use parallelize with 6 partitions\n",
    "rdd1 = spark.sparkContext.parallelize(range(0,25), 6)\n",
    "print(\"parallelize : \"+str(rdd1.getNumPartitions()))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From local[5] : 8\n",
      "parallelize : 6\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T11:00:51.582892Z",
     "start_time": "2025-11-09T11:00:51.539067Z"
    }
   },
   "source": [
    "file_path = \"tmp/test.txt\"\n",
    "\n",
    "import os\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Hello world\\n\")\n",
    "    f.write(\"Python and Spark\\n\")\n",
    "    f.write(\"RDD and DataFrame\\n\")\n",
    "\n",
    "# ×§×¨×™××ª ×§×•×‘×¥ ×˜×§×¡×˜\n",
    "rddFromFile = spark.sparkContext.textFile(file_path, 10)\n",
    "print(\"TextFile : \" + str(rddFromFile.getNumPartitions()))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextFile : 12\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "×”×©×™×˜×” `sparkContext.parallelize()` ×‘-PySpark ××©××©×ª ×œ×”×§×‘×œ×” (parallelize) ×©×œ ××•×¡×£ ×œ×ª×•×š RDD ××‘×•×–×¨ ×¢××™×“ (Resilient Distributed Dataset). ×‘×“×•×’××” ×”× ×ª×•× ×”, `Range(0,20)` ×™×•×¦×¨ ×˜×•×•×— ××¡×¤×¨×™× ×-0 ×¢×“ 19 (×›×•×œ×œ). ×”××¨×’×•×× ×˜ ×”×©× ×™, 6, ××¦×™×™×Ÿ ××ª ××¡×¤×¨ ×”××—×™×¦×•×ª ×©×”× ×ª×•× ×™× ×¦×¨×™×›×™× ×œ×”×™×—×œ×§ ××œ×™×”×Ÿ.\n",
    "\n",
    "×‘×•××• × ×›×ª×•×‘ ×–××ª ×œ×§×•×‘×¥ ×•× ×‘×“×•×§ ××ª ×”× ×ª×•× ×™×. ×©×™××• ×œ×‘ ×©×”××—×™×¦×•×ª ×©×œ×›× ×¢×©×•×™×•×ª ×œ×”×›×™×œ ×¨×©×•××•×ª ×©×•× ×•×ª."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T11:00:51.781205Z",
     "start_time": "2025-11-09T11:00:51.587658Z"
    }
   },
   "source": "rdd1.saveAsTextFile(\"tmp/partition\")",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×–×” ××™×™×¦×¨ ×§×‘×¦×™× ×©×œ 5 ×—×œ×§×™× (part files), ××—×“ ×œ×›×œ ××—×™×¦×”:\n",
    "\n",
    "```\n",
    "Partition 1 : 0 1 2\n",
    "Partition 2 : 3 4 5\n",
    "Partition 3 : 6 7 8 9\n",
    "Partition 4 : 10 11 12\n",
    "Partition 5 : 13 14 15\n",
    "Partition 6 : 16 17 18 19\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.1 ğŸ“Š RDD repartition()\n",
    "\n",
    "`repartition()` ×”×™× ×©×™×˜×ª ×˜×¨× ×¡×¤×•×¨××¦×™×” ×–××™× ×” ×‘-RDDs (Resilient Distributed Datasets) ×©××—×œ×§×ª ××—×“×© × ×ª×•× ×™× ×¢×œ ×¤× ×™ ××¡×¤×¨ ××•×’×“×¨ ×©×œ ××—×™×¦×•×ª.\n",
    "\n",
    "×›××©×¨ ××ª× ×§×•×¨××™× `repartition(n)`, ×›××©×¨ n ×”×•× ××¡×¤×¨ ×”××—×™×¦×•×ª ×”×¨×¦×•×™, Spark ××¢×¨×‘×‘ ××—×“×© ××ª ×”× ×ª×•× ×™× ×‘-RDD ×œ×ª×•×š ×‘×“×™×•×§ n ××—×™×¦×•×ª.\n",
    "\n",
    "×× ××ª× ××’×“×™×œ×™×/××§×˜×™× ×™× ××ª ××¡×¤×¨ ×”××—×™×¦×•×ª ×‘×××¦×¢×•×ª `repartition()`, Spark ×™×‘×¦×¢ ×¢×¨×‘×•×‘ ××œ× (full shuffle) ×©×œ ×”× ×ª×•× ×™× ×¢×œ ×¤× ×™ ×”×¦×•××ª (cluster), ×©×™×›×•×œ ×œ×”×™×•×ª ×¤×¢×•×œ×” ×™×§×¨×”, ×‘××™×•×—×“ ×¢×‘×•×¨ ××¢×¨×›×™ × ×ª×•× ×™× ×’×“×•×œ×™×."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T11:00:51.944330Z",
     "start_time": "2025-11-09T11:00:51.786211Z"
    }
   },
   "source": [
    "# ×©×™××•×© ×‘-repartition\n",
    "rdd2 = rdd1.repartition(4)\n",
    "print(\"Repartition size : \" + str(rdd2.getNumPartitions()))\n",
    "rdd2.saveAsTextFile(\"tmp/re-partition\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition size : 4\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×”×ª×•×¦××” ××¦×™×’×” \"×’×•×“×œ Repartition\" ×©×œ 4, ×”××¦×‘×™×¢×” ×¢×œ ×›×š ×©×”× ×ª×•× ×™× ×—×•×œ×§×• ××—×“×© ×¢×œ ×¤× ×™ 4 ××—×™×¦×•×ª.\n",
    "\n",
    "×¤×¢×•×œ×” ×–×• ×›×•×œ×œ×ª ×¢×¨×‘×•×‘ ××œ×, ×©×™×›×•×œ ×œ×”×™×•×ª ×“×™ ×™×§×¨, ×‘××™×•×—×“ ×›××©×¨ ××˜×¤×œ×™× ×‘××¢×¨×›×™ × ×ª×•× ×™× ×’×“×•×œ×™× ×‘×™×•×ª×¨ ×‘××™×œ×™××¨×“×™× ××• ×˜×¨×™×œ×™×•× ×™×.\n",
    "\n",
    "**×¤×œ×˜:**\n",
    "\n",
    "```\n",
    "Partition 1 : 1 6 10 15 19\n",
    "Partition 2 : 2 3 7 11 16\n",
    "Partition 3 : 4 8 12 13 17\n",
    "Partition 4 : 0 5 9 14 18\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.2 ğŸ”½ RDD coalesce()\n",
    "\n",
    "×‘-PySpark, `coalesce()` ×”×™× ×©×™×˜×ª ×˜×¨× ×¡×¤×•×¨××¦×™×” ×–××™× ×” ×‘-RDDs (Resilient Distributed Datasets) ×©××¤×—×™×ª×” ××ª ××¡×¤×¨ ×”××—×™×¦×•×ª **×œ×œ× ×¢×¨×‘×•×‘** × ×ª×•× ×™× ×¢×œ ×¤× ×™ ×”×¦×•××ª.\n",
    "\n",
    "×›××©×¨ ××ª× ×§×•×¨××™× `coalesce(n)`, ×›××©×¨ n ×”×•× ××¡×¤×¨ ×”××—×™×¦×•×ª ×”×¨×¦×•×™, Spark ×××–×’ ××—×™×¦×•×ª ×§×™×™××•×ª ×›×“×™ ×œ×™×¦×•×¨ n ××—×™×¦×•×ª."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T11:00:52.101914Z",
     "start_time": "2025-11-09T11:00:51.955374Z"
    }
   },
   "source": [
    "# ×©×™××•×© ×‘-coalesce()\n",
    "rdd3 = rdd1.coalesce(4)\n",
    "print(\"Repartition size : \" + str(rdd3.getNumPartitions()))\n",
    "rdd3.saveAsTextFile(\"tmp/coalesce\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition size : 4\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×× × ×©×•×•×” ××ª ×”×¤×œ×˜ ×”×‘× ×¢× ×¡×¢×™×£ 1.1, ×™×ª×‘×¨×¨ ×©××—×™×¦×” 3 ×”×•×¢×‘×¨×” ×œ××—×™×¦×” 2, ×•××—×™×¦×” 6 ×”×•×¢×‘×¨×” ×œ××—×™×¦×” 5.\n",
    "\n",
    "×ª× ×•×¢×ª × ×ª×•× ×™× ×–×• ×”×ª×¨×—×©×” ×¨×§ ×‘×™×Ÿ ×©×ª×™ ××—×™×¦×•×ª.\n",
    "\n",
    "**×¤×œ×˜:**\n",
    "\n",
    "```\n",
    "Partition 1 : 0 1 2\n",
    "Partition 2 : 3 4 5 6 7 8 9\n",
    "Partition 4 : 10 11 12\n",
    "Partition 5 : 13 14 15 16 17 18 19\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2ï¸âƒ£ PySpark DataFrame: repartition() ×œ×¢×•××ª coalesce()\n",
    "\n",
    "×‘×“×•××” ×œ-RDD, ××™× ×›× ×™×›×•×œ×™× ×œ×¦×™×™×Ÿ ××ª ×”××—×™×¦×”/×”×§×‘×œ×” ×‘×¢×ª ×™×¦×™×¨×ª DataFrame.\n",
    "\n",
    "DataFrame ×›×‘×¨×™×¨×ª ××—×“×œ ××©×ª××© ×‘××•×¤×Ÿ ×¤× ×™××™ ×‘×©×™×˜×•×ª ×”××¤×•×¨×˜×•×ª ×‘×¡×¢×™×£ 1 ×›×“×™ ×œ×§×‘×•×¢ ××ª ×”××—×™×¦×” ×‘×¨×™×¨×ª ×”××—×“×œ ×•××—×œ×§ ××ª ×”× ×ª×•× ×™× ×œ×¤×¨×œ×œ×™×–×.\n",
    "\n",
    "×× ××™× ×›× ××›×™×¨×™× ××ª DataFrame, ×××œ×™×¥ ×œ×œ××•×“ ××•×ª×• ×¢×œ ×™×“×™ ×‘×™×§×•×¨ ×‘[PySpark DataFrame Tutorial](https://sparkbyexamples.com/pyspark-tutorial/) ×œ×¤× ×™ ×©×ª××©×™×›×• ×‘××××¨ ×–×”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”§ ×“×•×’××ª DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T11:00:52.282594Z",
     "start_time": "2025-11-09T11:00:52.115443Z"
    }
   },
   "source": [
    "# ×“×•×’××ª DataFrame\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com') \\\n",
    "    .master(\"local[5]\").getOrCreate()\n",
    "\n",
    "df = spark.range(0,20)\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "df.write.mode(\"overwrite\").csv(\"tmp/partition.csv\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×”×“×•×’××” ×œ×¢×™×œ ×™×•×¦×¨×ª 5 ××—×™×¦×•×ª ×›×¤×™ ×©×¦×•×™×Ÿ ×‘-`master(\"local[5]\")` ×•×”× ×ª×•× ×™× ××•×¤×¦×™× ×¢×œ ×¤× ×™ ×›×œ 5 ×”××—×™×¦×•×ª.\n",
    "\n",
    "**×¤×œ×˜:**\n",
    "\n",
    "```\n",
    "Partition 1 : 0 1 2 3\n",
    "Partition 2 : 4 5 6 7\n",
    "Partition 3 : 8 9 10 11\n",
    "Partition 4 : 12 13 14 15\n",
    "Partition 5 : 16 17 18 19\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.1 ğŸ“ˆ DataFrame repartition()\n",
    "\n",
    "×”×©×™×˜×” `repartition()` ×©×œ DataFrame ×‘-PySpark ××—×œ×§×” ××—×“×© (××’×“×™×œ×” ××• ××§×˜×™× ×” ××—×™×¦×•×ª) × ×ª×•× ×™× ×‘××•×¤×Ÿ ×©×•×•×” ×¢×œ ×¤× ×™ ××¡×¤×¨ ××•×’×“×¨ ×©×œ ××—×™×¦×•×ª, ×ª×•×š ××•×¤×˜×™××™×–×¦×™×” ×©×œ ×¤×¨×œ×œ×™×–× ×•× ×™×¦×•×œ ××©××‘×™×.\n",
    "\n",
    "×”×™× ××¤×¢×™×œ×” ×¢×¨×‘×•×‘ ××œ× ×©×œ × ×ª×•× ×™× ×•×©×™××•×©×™×ª ×œ×”×ª×××ª ×¡×›×™××ª ×”×—×œ×•×§×” ×œ××—×™×¦×•×ª ×¢×‘×•×¨ ×¤×¢×•×œ×•×ª ×‘××•×¨×“ ×”×–×¨× ×›××• joins ×•××’×¨×’×¦×™×•×ª.\n",
    "\n",
    "×“×•×’××” ×–×• ××¨×—×™×‘×” ××ª ××¡×¤×¨ ×”××—×™×¦×•×ª ×-5 ×œ-6 ×¢×œ ×™×“×™ ×—×œ×•×§×” ××—×“×© ×©×œ × ×ª×•× ×™× ×¢×œ ×¤× ×™ ×›×œ ×”××—×™×¦×•×ª."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T11:00:52.324815Z",
     "start_time": "2025-11-09T11:00:52.289391Z"
    }
   },
   "source": [
    "# DataFrame repartition\n",
    "df2 = df.repartition(6)\n",
    "print(df2.rdd.getNumPartitions())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×¨×§ ×”×’×“×œ×ª ××—×™×¦×” 1 ×’×•×¨××ª ×œ×ª× ×•×¢×•×ª × ×ª×•× ×™× ××›×œ ×”××—×™×¦×•×ª.\n",
    "\n",
    "**×¤×œ×˜:**\n",
    "\n",
    "```\n",
    "Partition 1 : 14 3 5\n",
    "Partition 2 : 4 16 15\n",
    "Partition 3 : 8 3 18\n",
    "Partition 4 : 12 2 19\n",
    "Partition 5 : 6 17 7 0\n",
    "Partition 6 : 9 10 11 13\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×•×’×, ×”×§×˜× ×ª ×”××—×™×¦×•×ª ×’× ×’×•×¨××ª ×œ×ª× ×•×¢×ª × ×ª×•× ×™× ××›×œ ×”××—×™×¦×•×ª.\n",
    "\n",
    "×œ×›×Ÿ ×›××©×¨ ×¨×¦×™×ª× ×œ×”×§×˜×™×Ÿ ××ª ×”×”××œ×¦×” ×©×œ ×”××—×™×¦×” ×”×™× ×œ×”×©×ª××© ×‘-`coalesce()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.2 ğŸ”½ DataFrame coalesce()\n",
    "\n",
    "Spark DataFrame `coalesce()` ××©××© ×¨×§ ×œ×”×§×˜× ×ª ××¡×¤×¨ ×”××—×™×¦×•×ª. ×–×•×”×™ ×’×¨×¡×” ××©×•×¤×¨×ª ××• ××•×¤×˜×™××œ×™×ª ×©×œ repartition() ×©×‘×” ×ª× ×•×¢×ª ×”× ×ª×•× ×™× ×¢×œ ×¤× ×™ ×”××—×™×¦×•×ª ×¤×—×•×ª×” ×‘×××¦×¢×•×ª coalesce."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T11:00:52.348794Z",
     "start_time": "2025-11-09T11:00:52.329905Z"
    }
   },
   "source": [
    "# DataFrame coalesce\n",
    "df3 = df.coalesce(2)\n",
    "print(df3.rdd.getNumPartitions())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×–×” ×× ×™×‘ ×¤×œ×˜ 2 ×•×”××—×™×¦×” ×”××ª×§×‘×œ×ª × ×¨××™×ª ×›×š:\n",
    "\n",
    "**×¤×œ×˜:**\n",
    "\n",
    "```\n",
    "Partition 1 : 0 1 2 3 8 9 10 11\n",
    "Partition 2 : 4 5 6 7 12 13 14 15 16 17 18 19\n",
    "```\n",
    "\n",
    "××›×™×•×•×Ÿ ×©×× ×—× ×• ××¦××¦××™× 5 ×œ-2 ××—×™×¦×•×ª, ×ª× ×•×¢×ª ×”× ×ª×•× ×™× ××ª×¨×—×©×ª ×¨×§ ×-3 ××—×™×¦×•×ª ×•×”×™× ×¢×•×‘×¨×ª ×œ× ×©××¨ 2 ××—×™×¦×•×ª."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Default Shuffle Partition\n",
    "\n",
    "×‘-PySpark, ××—×™×¦×ª ×”×¢×¨×‘×•×‘ (shuffle partition) ×‘×¨×™×¨×ª ×”××—×“×œ ××ª×™×™×—×¡×ª ×œ××¡×¤×¨ ×”××—×™×¦×•×ª ×©-Spark ××©×ª××© ×‘×”×Ÿ ×‘×¢×ª ×‘×™×¦×•×¢ ×¤×¢×•×œ×•×ª ×¢×¨×‘×•×‘, ×›×’×•×Ÿ joins, group-bys ×•××’×¨×’×¦×™×•×ª.\n",
    "\n",
    "×¤×¢×•×œ×•×ª ×¢×¨×‘×•×‘ ×›×•×œ×œ×•×ª ×—×œ×•×§×” ××—×“×© ×©×œ × ×ª×•× ×™× ×¢×œ ×¤× ×™ ×¦××ª×™× ×©×•× ×™× ×‘×¦×•××ª, ×©×™×›×•×œ×” ×œ×”×™×•×ª ×™×§×¨×” ××‘×—×™× ×ª ×—×™×©×•×‘ ×•×œ×”×©×¤×™×¢ ×¢×œ ×”×‘×™×¦×•×¢×™×.\n",
    "\n",
    "×›×‘×¨×™×¨×ª ××—×“×œ, Spark ××’×“×™×¨ ××ª ××¡×¤×¨ ××—×™×¦×•×ª ×”×¢×¨×‘×•×‘ ×œ-**200**.\n",
    "\n",
    "×¢×¨×š ×‘×¨×™×¨×ª ××—×“×œ ×–×” × ×©×œ×˜ ×¢×œ ×™×“×™ ×¤×¨××˜×¨ ×”×ª×¦×•×¨×” `spark.sql.shuffle.partitions`.\n",
    "\n",
    "××ª× ×™×›×•×œ×™× ×œ×”×ª××™× ×”×’×“×¨×” ×–×• ×¢×œ ×¡××š ×’×•×“×œ ×”× ×ª×•× ×™× ×©×œ×›× ×•×”××©××‘×™× ×©×œ ×”×¦×•××ª ×©×œ×›× ×›×“×™ ×œ×™×™×¢×œ ××ª ×”×‘×™×¦×•×¢×™×."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T11:00:52.378610Z",
     "start_time": "2025-11-09T11:00:52.354094Z"
    }
   },
   "source": [
    "# ×¡×¤×™×¨×ª ××—×™×¦×•×ª ×‘×¨×™×¨×ª ××—×“×œ\n",
    "df4 = df.groupBy(\"id\").count()\n",
    "print(df4.rdd.getNumPartitions())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×œ××—×¨ ×¤×¢×•×œ×•×ª ×¢×¨×‘×•×‘, ××ª× ×™×›×•×œ×™× ×œ×©× ×•×ª ××ª ×”××—×™×¦×•×ª ×‘×××¦×¢×•×ª coalesce() ××• repartition()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4ï¸âƒ£ PySpark: repartition ×œ×¢×•××ª coalesce - ×˜×‘×œ×ª ×”×©×•×•××”\n",
    "\n",
    "×œ×”×œ×Ÿ ×”×”×‘×“×œ×™× ×‘×¤×•×¨××˜ ×˜×‘×œ×”:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **×ª×›×•× ×”** | **REPARTITION** | **COALESCE** |\n",
    "|------------|-----------------|---------------|\n",
    "| **×ª×™××•×¨** | ××ª××™× ××ª ××¡×¤×¨ ×”××—×™×¦×•×ª, ××—×œ×§ ××—×“×© × ×ª×•× ×™× ×¢×œ ×¤× ×™ ×”××¡×¤×¨ ×”××•×’×“×¨ ×©×œ ××—×™×¦×•×ª. | ××¤×—×™×ª ××ª ××¡×¤×¨ ×”××—×™×¦×•×ª ××‘×œ×™ ×œ×¢×¨×‘×‘ × ×ª×•× ×™×, ×××–×’ ××—×™×¦×•×ª ×§×™×™××•×ª. |\n",
    "| **×¢×¨×‘×•×‘ ××œ×** | ×›×Ÿ | ×œ× |\n",
    "| **×™×§×¨ ××‘×—×™× ×ª ××©××‘×™×** | ×™×›×•×œ ×œ×”×™×•×ª ×™×§×¨, ×‘××™×•×—×“ ×¢×‘×•×¨ ××¢×¨×›×™ × ×ª×•× ×™× ×’×“×•×œ×™×, ××›×™×•×•×Ÿ ×©×”×•× ×›×•×œ×œ ×¢×¨×‘×•×‘ ××œ× ×©×œ × ×ª×•× ×™×. | ×¤×—×•×ª ×™×§×¨ ×-repartition, ××›×™×•×•×Ÿ ×©×”×•× ×××–×¢×¨ ×ª× ×•×¢×ª × ×ª×•× ×™× ×¢×œ ×™×“×™ ×©×™×œ×•×‘ ××—×™×¦×•×ª ×¨×§ ×›×©××¤×©×¨. |\n",
    "| **×ª× ×•×¢×ª × ×ª×•× ×™×** | ××—×œ×§ × ×ª×•× ×™× ×¢×œ ×¤× ×™ ××—×™×¦×•×ª ×‘××•×¤×Ÿ ×©×•×•×”, ×¢×œ×•×œ ×œ×”×•×‘×™×œ ×œ×’×“×œ×™ ××—×™×¦×•×ª ×××•×–× ×™×. | ×¢×œ×•×œ ×œ×’×¨×•× ×œ×’×“×œ×™ ××—×™×¦×•×ª ×œ× ×××•×–× ×™×, ×‘××™×•×—×“ ×‘×¢×ª ×”×¤×—×ª×ª ××¡×¤×¨ ×”××—×™×¦×•×ª. |\n",
    "| **××§×¨×™ ×©×™××•×©** | ×©×™××•×©×™ ×‘×¢×ª ×©×™× ×•×™ ××¡×¤×¨ ×”××—×™×¦×•×ª ××• ×—×œ×•×§×” ×©×•×•×” ×©×œ × ×ª×•× ×™× ×¢×œ ×¤× ×™ ××—×™×¦×•×ª. | ×©×™××•×©×™ ×‘×¢×ª ×”×§×˜× ×ª ××¡×¤×¨ ×”××—×™×¦×•×ª ××‘×œ×™ ×œ×©××ª ×‘×¢×œ×•×ª ×©×œ ×¢×¨×‘×•×‘ ××œ×. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ ×¡×™×›×•×\n",
    "\n",
    "×‘××××¨ ×–×” ×¢×œ PySpark repartition() ×œ×¢×•××ª coalesce(), ×œ××“×ª×:\n",
    "\n",
    "- âœ… ×›×™×¦×“ ×œ×™×¦×•×¨ RDD ×¢× ××—×™×¦×•×ª\n",
    "- âœ… ×›×™×¦×“ ×œ×”×—×œ×™×§ ××—×™×¦×•×ª ×‘-RDD ×‘×××¦×¢×•×ª coalesce()\n",
    "- âœ… ×›×™×¦×“ ×œ×—×œ×§ ××—×“×© DataFrame ×‘×××¦×¢×•×ª repartition()\n",
    "- âœ… ×›×™×¦×“ ×œ×—×œ×§ ××—×“×© DataFrame ×‘×××¦×¢×•×ª coalesce()\n",
    "- âœ… ×œ××“×ª× ××ª ×”×”×‘×“×œ ×‘×™×Ÿ repartition ×•-coalesce\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ××××¨×™× ×§×©×•×¨×™×\n",
    "\n",
    "- [PySpark partitionBy() Explained with Examples](https://sparkbyexamples.com/pyspark/pyspark-partitionby-explained-with-examples/)\n",
    "- [PySpark Parallelize | Create RDD](https://sparkbyexamples.com/pyspark/pyspark-parallelize-create-rdd/)\n",
    "- [PySpark repartition() vs partitionBy() with Examples](https://sparkbyexamples.com/pyspark/pyspark-repartition-vs-partitionby-with-examples/)\n",
    "- [PySpark repartition() â€“ Explained with Examples](https://sparkbyexamples.com/pyspark/pyspark-repartition-explained-with-examples/)\n",
    "- [PySpark repartition() vs partitionBy()](https://sparkbyexamples.com/pyspark/pyspark-repartition-vs-partitionby/)\n",
    "- [PySpark Query Database Table using JDBC](https://sparkbyexamples.com/pyspark/pyspark-query-database-table-using-jdbc/)\n",
    "- [PySpark Read and Write SQL Server Table](https://sparkbyexamples.com/pyspark/pyspark-read-and-write-sql-server-table/)\n",
    "- [PySpark Read JDBC Table to DataFrame](https://sparkbyexamples.com/pyspark/pyspark-read-jdbc-table-to-dataframe/)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— ××§×•×¨×•×ª\n",
    "\n",
    "- [Apache Spark Configuration Documentation](https://spark.apache.org/docs/latest/configuration.html)\n",
    "- [Apache Spark RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ ×œ××™×“×” ××”× ×”!\n",
    "\n",
    "**Happy Learning !!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ×”×¢×¨×•×ª × ×•×¡×¤×•×ª\n",
    "\n",
    "### ğŸ’¡ ×˜×™×¤×™× ×—×©×•×‘×™×:\n",
    "\n",
    "1. **×”×©×ª××©×• ×‘-coalesce() ×œ×”×§×˜× ×”**: ×›××©×¨ ××ª× ×¦×¨×™×›×™× ×œ×”×§×˜×™×Ÿ ××ª ××¡×¤×¨ ×”××—×™×¦×•×ª, ×”×¢×“×™×¤×• ×ª××™×“ ××ª coalesce() ×¢×œ ×¤× ×™ repartition() ×œ×‘×™×¦×•×¢×™× ×˜×•×‘×™× ×™×•×ª×¨.\n",
    "\n",
    "2. **×”×©×ª××©×• ×‘-repartition() ×œ×”×’×“×œ×” ××• ×—×œ×•×§×” ×©×•×•×”**: ×›××©×¨ ××ª× ×¦×¨×™×›×™× ×œ×”×’×“×™×œ ××ª ××¡×¤×¨ ×”××—×™×¦×•×ª ××• ×œ×—×œ×§ × ×ª×•× ×™× ×‘××•×¤×Ÿ ×©×•×•×”, ×”×©×ª××©×• ×‘-repartition().\n",
    "\n",
    "3. **×©×§×œ×• ××ª ×’×•×“×œ ×”× ×ª×•× ×™×**: ×¤×¢×•×œ×•×ª ×¢×¨×‘×•×‘ ×™×›×•×œ×•×ª ×œ×”×™×•×ª ×™×§×¨×•×ª ×¢×œ ××¢×¨×›×™ × ×ª×•× ×™× ×’×“×•×œ×™×. ×ª×›× × ×• ××ª ××¡×˜×¨×˜×’×™×™×ª ×”×—×œ×•×§×” ×œ××—×™×¦×•×ª ×©×œ×›× ×‘×§×¤×™×“×”.\n",
    "\n",
    "4. **×”×ª××™××• ××ª spark.sql.shuffle.partitions**: ×¢×‘×•×¨ ××¤×œ×™×§×¦×™×•×ª ×™×™×¦×•×¨, ×”×ª××™××• ××ª ×”×¤×¨××˜×¨ ×”×–×” ×¢×œ ×¡××š ×’×•×“×œ ×”× ×ª×•× ×™× ×•×”××©××‘×™× ×”×–××™× ×™× ×©×œ ×”×¦×•××ª.\n",
    "\n",
    "### âš¡ ×©×™×˜×•×ª ×¢×‘×•×“×” ××•××œ×¦×•×ª:\n",
    "\n",
    "- ××–×¢×¨×• ××ª ×›××•×ª ×¤×¢×•×œ×•×ª ×”×¢×¨×‘×•×‘ ×‘××¤×œ×™×§×¦×™×™×ª Spark ×©×œ×›×\n",
    "- ×”×©×ª××©×• ×‘×—×œ×•×§×” ×œ××—×™×¦×•×ª ××•×¤×˜×™××œ×™×ª ×¢×‘×•×¨ ××¢×¨×›×™ ×”× ×ª×•× ×™× ×©×œ×›×\n",
    "- ×¢×§×‘×• ××—×¨ ××“×“×™ ×”×‘×™×¦×•×¢×™× ×›×“×™ ×œ×–×”×•×ª ×¦×•×•××¨×™ ×‘×§×‘×•×§\n",
    "- ×”×©×ª××©×• ×‘-Spark UI ×›×“×™ ×œ× ×˜×¨ ××ª ×¤×¢×•×œ×•×ª ×”×¢×¨×‘×•×‘\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“§ ×™×¦×™×¨×ª ×§×©×¨ ×•××©×•×‘\n",
    "\n",
    "×× ×™×© ×œ×›× ×©××œ×•×ª ××• ××©×•×‘ ×¢×œ ××“×¨×™×š ×–×”, ×× × ×‘×§×¨×• ×‘-[SparkByExamples.com](https://sparkbyexamples.com)\n",
    "\n",
    "---\n",
    "\n",
    "**×ª××¨×™×š ×™×¦×™×¨×”:** ×™×•×œ×™ 2023  \n",
    "**×ª×•×¨×’× ×•×¢×•×‘×“ ×œ×¢×‘×¨×™×ª:** × ×•×‘××‘×¨ 2025  \n",
    "**×’×¨×¡×”:** 1.0"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T11:00:52.400475Z",
     "start_time": "2025-11-09T11:00:52.394169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "\n",
    "# ××—×™×§×” ×©×œ ×›×œ ×”×ª×§×™×™×” /tmp\n",
    "shutil.rmtree(\"tmp\", ignore_errors=True)"
   ],
   "outputs": [],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
