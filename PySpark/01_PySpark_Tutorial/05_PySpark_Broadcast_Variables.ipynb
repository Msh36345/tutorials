{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<style>\n",
    "    body, * {\n",
    "        direction: rtl !important;\n",
    "        text-align: right !important;\n",
    "    }\n",
    "</style>\n",
    "拽专: [Spark By Examples - PySpark Broadcast Variables](https://sparkbyexamples.com/pyspark/pyspark-broadcast-variables/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 砖转 Broadcast -PySpark\n",
    "\n",
    "## \n",
    "\n",
    "-PySpark RDD -DataFrame, 砖转 Broadcast  **砖转 拽专 ** 砖砖专    爪转 砖 (cluster). 拽 砖 转 转  注  砖 (task), PySpark 驻抓 转 砖转 -broadcast 注 (workers) 爪注转 专转 注  驻转 注转 转拽砖专转.\n",
    "\n",
    "### 转 砖转砖 砖转 Broadcast?\n",
    "\n",
    " 砖转 拽 拽抓 注 拽 转 注 砖转 转转 转 专爪 专 转 砖转 转  (: CA -California, NY -New York ') 爪注转 lookup  驻 转住转. 拽专 住, 转   转  转 砖  驻砖 专 ( zipcode ').\n",
    "\n",
    "拽 驻抓 转 注   注  砖 专 专砖转 (overhead  专),   砖转砖 砖转 broadcast  砖专 转 注 驻砖     砖转砖 注 砖专   爪注 专住驻专爪转.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  注 Broadcast -PySpark?\n",
    "\n",
    "砖转 Broadcast 砖砖 转 专 注专 RDD -DataFrame.\n",
    "\n",
    "砖专 专爪 砖 PySpark RDD  DataFrame 砖 专 注砖 砖砖 砖转 Broadcast, PySpark 爪注 转 驻注转 转:\n",
    "\n",
    "1. **PySpark 驻专拽 转 注 砖** (stages) 砖 砖转 驻爪转 驻注转 砖爪注转 砖\n",
    "2. **砖 专 转专   拽 砖转**\n",
    "3. **Spark 砖专 转 转 砖转驻 (转 砖砖 专) 专砖 注  砖转  砖**\n",
    "4. **转 砖专 砖专  驻专 住专转 -住专转 驻 爪注  砖**\n",
    "\n",
    "**砖:** 转 爪专 爪专 砖转砖 砖转 broadcast 注专 转 砖砖转驻  住驻专 砖 砖转.\n",
    "\n",
    "**砖 :** 砖转 broadcast  砖 爪注 (executors) 注 `sc.broadcast(variable)` - 拽 转,  砖 爪注 砖专 注砖  砖砖 专砖.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  爪专 砖转 Broadcast\n",
    "\n",
    "-Broadcast -PySpark 爪专 爪注转 转 `broadcast(v)` 砖 拽 SparkContext. 转  拽转 转 专 `v` 砖转 专爪 砖专.\n",
    "\n",
    "### -PySpark Shell\n",
    "\n",
    "住拽住 住住 爪专转 砖转 broadcast :\n",
    "\n",
    "```python\n",
    "broadcastVar = sc.broadcast(Array(1, 2, 3))\n",
    "broadcastVar.value\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  砖转 Broadcast -PySpark RDD\n",
    "\n",
    "  驻砖  驻 砖砖 砖转 broadcast -RDD. \n",
    "\n",
    " 专 转 驻爪 (states - 转) 砖转 Map 驻爪 转 砖转 爪注转 `SparkContext.broadcast()` 专  砖转砖转 砖转  -RDD 注 专住驻专爪 砖 `map()`.\n",
    "\n",
    "###  住驻专转 转转 Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 专转 砖转 Broadcast 注 转 转"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爪专转 转 "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "        (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "        (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "        (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "       ]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爪专转 RDD 专 爪注转 砖转 -Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = rdd.map(lambda x: (x[0], x[1], x[2], state_convert(x[3]))).collect()\n",
    "print(result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**驻 爪驻:**\n",
    "\n",
    "```\n",
    "[('James', 'Smith', 'USA', 'California'), \n",
    " ('Michael', 'Rose', 'USA', 'New York'), \n",
    " ('Robert', 'Williams', 'USA', 'California'), \n",
    " ('Maria', 'Jones', 'USA', 'Florida')]\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  砖转 Broadcast -PySpark DataFrame\n",
    "\n",
    "  驻 砖砖 砖转 broadcast -DataFrame,   拽转 注 RDD.\n",
    "\n",
    "  砖转砖 转 驻爪 (states) 砖转 Map 驻爪 转 砖转 爪注转 `SparkContext.broadcast()` 专  砖转砖 砖转  专住驻专爪 砖 `map()` 注 DataFrame.\n",
    "\n",
    "**注专:**   专 转 DataFrame, 抓  注 DataFrame 驻 砖砖 专 .\n",
    "\n",
    "###  住驻专转"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 专转 砖转 Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爪专转 转 "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "        (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "        (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "        (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "       ]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爪专转 DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "columns = [\"firstname\", \"lastname\", \"country\", \"state\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 专转 转 爪注转 砖转 Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = df.rdd.map(lambda x: (x[0], x[1], x[2], state_convert(x[3]))).toDF(columns)\n",
    "result.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**驻 爪驻:**\n",
    "\n",
    "```\n",
    "+---------+--------+-------------+----------+\n",
    "|firstname|lastname|country      |state     |\n",
    "+---------+--------+-------------+----------+\n",
    "|James    |Smith   |United States|California|\n",
    "|Michael  |Rose    |United States|New York  |\n",
    "|Robert   |Williams|United States|California|\n",
    "|Maria    |Jones   |United States|Florida   |\n",
    "+---------+--------+-------------+----------+\n",
    "```\n",
    "\n",
    "### 砖砖 砖转 Broadcast -Filter -Join\n",
    "\n",
    "转  砖转砖 砖转 broadcast 注 filter -joins.   砖砖 -filter:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Broadcast variable on filter\n",
    "filteDf = df.where(df['state'].isin(list(broadcastStates.value)))\n",
    "filteDf.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 住\n",
    "\n",
    "专  注 砖转 Broadcast -PySpark, 转:\n",
    "-  砖转 Broadcast\n",
    "-  转专转 砖\n",
    "- 爪 砖转砖  -RDD -DataFrame 注 转 注砖转\n",
    "\n",
    "---\n",
    "\n",
    "## 拽专转 住驻\n",
    "\n",
    "- [PySpark SparkContext Explained](https://sparkbyexamples.com/pyspark/pyspark-sparkcontext/)\n",
    "- [Dynamic way of doing ETL through PySpark](https://sparkbyexamples.com/pyspark/dynamic-way-of-doing-etl-through-pyspark/)\n",
    "- [PySpark Shell Command Usage with Examples](https://sparkbyexamples.com/pyspark/pyspark-shell-command/)\n",
    "- [PySpark Accumulator with Example](https://sparkbyexamples.com/pyspark/pyspark-accumulator-with-example/)\n",
    "- [PySpark distinct vs dropDuplicates](https://sparkbyexamples.com/pyspark/pyspark-distinct-vs-dropduplicates/)\n",
    "- [PySpark Get Number of Rows and Columns](https://sparkbyexamples.com/pyspark/pyspark-get-number-of-rows-and-columns/)\n",
    "- [Pyspark Select Distinct Rows](https://sparkbyexamples.com/pyspark/pyspark-select-distinct-rows/)\n",
    "\n",
    "### 转注 专砖\n",
    "\n",
    "- [Apache Spark - Broadcast Variables Documentation](https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html#broadcast-variables)\n",
    "\n",
    "---\n",
    "\n",
    "** ! **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
