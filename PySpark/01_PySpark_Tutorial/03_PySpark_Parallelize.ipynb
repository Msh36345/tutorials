{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<style>\n",
    "    body, * {\n",
    "        direction: rtl !important;\n",
    "        text-align: right !important;\n",
    "    }\n",
    "</style>\n",
    "××§×•×¨: [Spark By Examples - PySpark Parallelize](https://sparkbyexamples.com/pyspark/pyspark-parallelize-create-rdd/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ PySpark parallelize() - ×™×¦×™×¨×ª RDD ××¨×©×™××ª × ×ª×•× ×™×\n",
    "\n",
    "## ×¡×§×™×¨×” ×›×œ×œ×™×ª\n",
    "\n",
    "×‘××“×¨×™×š ×–×” × ×œ××“ ×›×™×¦×“ ×œ×”×©×ª××© ×‘×¤×•× ×§×¦×™×” `parallelize()` ×©×œ PySpark ×œ×™×¦×™×¨×ª RDD (Resilient Distributed Dataset) ×××•×¡×£ × ×ª×•× ×™× ××§×•××™.\n",
    "\n",
    "### ğŸ“š ××” ×–×” RDD?\n",
    "\n",
    "**RDD (Resilient Distributed Dataset)** ×”×•× ××‘× ×” × ×ª×•× ×™× ×‘×¡×™×¡×™ ×©×œ PySpark. ×–×”×• ××•×¡×£ × ×ª×•× ×™× ××‘×•×–×¨ ×•×‘×œ×ª×™ × ×™×ª×Ÿ ×œ×©×™× ×•×™ (immutable) ×©×œ ××•×‘×™×™×§×˜×™×. ×›×œ dataset ×‘-RDD ××—×•×œ×§ ×œ××—×™×¦×•×ª ×œ×•×’×™×•×ª (partitions), ××©×¨ ×¢×©×•×™×•×ª ×œ×”×™×•×ª ××—×•×©×‘×•×ª ×¢×œ ×¦××ª×™× ×©×•× ×™× ×©×œ ×”×§×œ××¡×˜×¨.\n",
    "\n",
    "**PySpark ××‘×¦×¢ ×”×§×‘×œ×” (parallelization)** ×©×œ ××•×¡×£ ×§×™×™× ×‘×ª×•×›× ×™×ª ×”-driver ×©×œ×š."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ××˜×¨×•×ª ×”××“×¨×™×š\n",
    "\n",
    "×‘××“×¨×™×š ×–×” × ×œ××“:\n",
    "- ×›×™×¦×“ ×œ×™×¦×•×¨ RDD ××¨×©×™××ª × ×ª×•× ×™× ×‘×××¦×¢×•×ª `parallelize()`\n",
    "- ×›×™×¦×“ ×œ×”×©×ª××© ×‘-`parallelize()` ×‘-PySpark Shell ××• REPL\n",
    "- ×›×™×¦×“ ×œ×”×©×ª××© ×‘-`sparkContext.parallelize()` ×‘××¤×œ×™×§×¦×™×”\n",
    "- ×›×™×¦×“ ×œ×™×¦×•×¨ RDD ×¨×™×§\n",
    "- ×”×‘× ×ª ××—×™×¦×•×ª (partitions) ×‘-RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’» ×©×™××•×© ×‘-sc.parallelize() ×‘-PySpark Shell\n",
    "\n",
    "PySpark Shell ××¡×¤×§ ××©×ª× ×” ×‘×©× `sc`, ××©×¨ ×”×•× ××•×‘×™×™×§×˜ SparkContext. ×”×©×ª××© ×‘-`sc.parallelize()` ×œ×™×¦×™×¨×ª RDD.\n",
    "\n",
    "### ×“×•×’××”: ×™×¦×™×¨×ª RDD ×¢× ×¨×©×™××ª ××¡×¤×¨×™× ×©×œ××™×"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T10:38:23.165772Z",
     "start_time": "2025-11-09T10:38:23.154129Z"
    }
   },
   "source": [
    "# ×™×¦×™×¨×ª RDD ××¨×©×™××ª ××¡×¤×¨×™× ×©×œ××™×\n",
    "# rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×”×¤×•× ×§×¦×™×” ×™×•×¦×¨×ª RDD ×¢× ×¨×©×™××” ×©×œ ××¡×¤×¨×™× ×©×œ××™×."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ ×©×™××•×© ×‘-sparkContext.parallelize() ×‘××¤×œ×™×§×¦×™×”\n",
    "\n",
    "×›××©×¨ ××ª×” ×¢×•×‘×“ ×‘××¤×œ×™×§×¦×™×™×ª PySpark (×œ× ×‘-Shell), ×ª×—×™×œ×” ×¢×œ×™×š ×œ×™×¦×•×¨ `SparkSession` ××©×¨ ×™×•×¦×¨ ×‘××•×¤×Ÿ ×¤× ×™××™ ××ª ×”-`SparkContext`.\n",
    "\n",
    "### ×©×œ×‘ 1: ×™×‘×•× ×”××•×“×•×œ×™× ×”× ×“×¨×©×™×"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T10:38:23.241313Z",
     "start_time": "2025-11-09T10:38:23.174720Z"
    }
   },
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ×©×œ×‘ 2: ×™×¦×™×¨×ª SparkSession ×•××•×‘×™×™×§×˜ SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T10:38:26.431165Z",
     "start_time": "2025-11-09T10:38:23.246130Z"
    }
   },
   "source": [
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()\n",
    "sparkContext = spark.sparkContext"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/09 12:38:24 WARN Utils: Your hostname, MyMac.local, resolves to a loopback address: 127.0.0.1; using 172.20.10.14 instead (on interface en0)\n",
      "25/11/09 12:38:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/09 12:38:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/09 12:38:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/11/09 12:38:25 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/11/09 12:38:25 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ×©×œ×‘ 3: ×©×™××•×© ×‘-sparkContext.parallelize() ×œ×™×¦×™×¨×ª RDD"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T10:38:28.220519Z",
     "start_time": "2025-11-09T10:38:26.448386Z"
    }
   },
   "source": [
    "# ×™×¦×™×¨×ª RDD ××¨×©×™××ª × ×ª×•× ×™×\n",
    "rdd = sparkContext.parallelize([1,2,3,4,5])\n",
    "\n",
    "# ××™×¡×•×£ ×”× ×ª×•× ×™× ××”-RDD\n",
    "rddCollect = rdd.collect()\n",
    "\n",
    "# ×”×“×¤×¡×ª ××™×“×¢ ×¢×œ ×”-RDD\n",
    "print(\"Number of Partitions: \" + str(rdd.getNumPartitions()))\n",
    "print(\"Action: First element: \" + str(rdd.first()))\n",
    "print(rddCollect)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 8\n",
      "Action: First element: 1\n",
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š ×¤×œ×˜ ×¦×¤×•×™:\n",
    "\n",
    "```\n",
    "Number of Partitions: 4\n",
    "Action: First element: 1\n",
    "[1, 2, 3, 4, 5]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” ×”×‘× ×ª Partitions (××—×™×¦×•×ª)\n",
    "\n",
    "×œ×¤×•× ×§×¦×™×” `parallelize()` ×™×© ×—×ª×™××” × ×•×¡×¤×ª ×”×××¤×©×¨×ª ×œ×”×¢×‘×™×¨ ××¨×’×•×× ×˜ ××¡×¤×¨ ×©×œ× ×”××¦×™×™×Ÿ ××ª ××¡×¤×¨ ×”××—×™×¦×•×ª.\n",
    "\n",
    "### ××”×Ÿ Partitions?\n",
    "\n",
    "**Partitions ×”×Ÿ ×™×—×™×“×•×ª ×‘×¡×™×¡×™×•×ª ×©×œ ×”×§×‘×œ×” (parallelism) ×‘-PySpark.**\n",
    "\n",
    "×–×›×•×¨: **RDDs ×‘-PySpark ×”× ××•×¡×£ ×©×œ partitions.**\n",
    "\n",
    "### ×“×•×’××”: ×™×¦×™×¨×ª RDD ×¢× ××¡×¤×¨ ××—×™×¦×•×ª ××•×ª×× ××™×©×™×ª"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T10:38:28.233624Z",
     "start_time": "2025-11-09T10:38:28.229248Z"
    }
   },
   "source": [
    "# ×™×¦×™×¨×ª RDD ×¢× 4 partitions\n",
    "rdd = sparkContext.parallelize([1,2,3,4,5], 4)\n",
    "\n",
    "# ×‘×“×™×§×ª ××¡×¤×¨ ×”-partitions\n",
    "print(\"Number of Partitions: \" + str(rdd.getNumPartitions()))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 4\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—‚ï¸ ×™×¦×™×¨×ª RDD ×¨×™×§ ×‘×××¦×¢×•×ª sparkContext.parallelize\n",
    "\n",
    "×œ×¢×™×ª×™× ×× ×• ×¦×¨×™×›×™× ×œ×™×¦×•×¨ RDD ×¨×™×§, ×•× ×™×ª×Ÿ ×œ×”×©×ª××© ×‘-`parallelize()` ×’× ×œ×©× ×›×š."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T10:38:28.360243Z",
     "start_time": "2025-11-09T10:38:28.243589Z"
    }
   },
   "source": [
    "# ×™×¦×™×¨×ª RDD ×¨×™×§\n",
    "emptyRDD = sparkContext.emptyRDD()\n",
    "\n",
    "# ××•:\n",
    "emptyRDD2 = sparkContext.parallelize([])\n",
    "\n",
    "# ×‘×“×™×§×” ×”×× ×”-RDD ×¨×™×§\n",
    "print(\"Is Empty RDD : \" + str(emptyRDD2.isEmpty()))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Empty RDD : True\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ×§×•×“ ××œ× - ×“×•×’××” ×©×œ××”\n",
    "\n",
    "×œ×”×œ×Ÿ ×“×•×’××” ××œ××” ×”××©×œ×‘×ª ××ª ×›×œ ×”×§×•× ×¡×¤×˜×™× ×©×œ××“× ×•:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T10:38:28.777602Z",
     "start_time": "2025-11-09T10:38:28.368799Z"
    }
   },
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ×™×¦×™×¨×ª SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()\n",
    "sparkContext = spark.sparkContext\n",
    "\n",
    "# ×“×•×’××” 1: ×™×¦×™×¨×ª RDD ×‘×¡×™×¡×™\n",
    "rdd = sparkContext.parallelize([1,2,3,4,5])\n",
    "rddCollect = rdd.collect()\n",
    "print(\"Number of Partitions: \" + str(rdd.getNumPartitions()))\n",
    "print(\"Action: First element: \" + str(rdd.first()))\n",
    "print(rddCollect)\n",
    "\n",
    "# ×“×•×’××” 2: ×™×¦×™×¨×ª RDD ×¨×™×§\n",
    "emptyRDD = sparkContext.emptyRDD()\n",
    "emptyRDD2 = sparkContext.parallelize([])\n",
    "print(\"Is Empty RDD : \" + str(emptyRDD2.isEmpty()))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 8\n",
      "Action: First element: 1\n",
      "[1, 2, 3, 4, 5]\n",
      "Is Empty RDD : True\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ×¡×™×›×•×\n",
    "\n",
    "×‘××“×¨×™×š ×–×” ×œ××“× ×•:\n",
    "\n",
    "âœ… ××”×• RDD ×•××“×•×¢ ×”×•× ×—×©×•×‘ ×‘-PySpark  \n",
    "âœ… ×›×™×¦×“ ×œ×”×©×ª××© ×‘-`sc.parallelize()` ×‘-PySpark Shell  \n",
    "âœ… ×›×™×¦×“ ×œ×”×©×ª××© ×‘-`sparkContext.parallelize()` ×‘××¤×œ×™×§×¦×™×”  \n",
    "âœ… ×”×‘× ×ª Partitions ×•×›×™×¦×“ ×œ×§×‘×•×¢ ××ª ××¡×¤×¨×  \n",
    "âœ… ×™×¦×™×¨×ª RDD ×¨×™×§  \n",
    "\n",
    "×”×¤×•× ×§×¦×™×” `parallelize()` ×”×™× ×›×œ×™ ×‘×¡×™×¡×™ ×•×—×©×•×‘ ×‘-PySpark ×”×××¤×©×¨ ×œ× ×• ×œ×”××™×¨ × ×ª×•× ×™× ××§×•××™×™× ×œ× ×ª×•× ×™× ××‘×•×–×¨×™×."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”— ××××¨×™× ×§×©×•×¨×™×\n",
    "\n",
    "×œ×”×¨×—×‘×ª ×”×™×“×¢, ××•××œ×¥ ×œ×§×¨×•× ××ª ×”××××¨×™× ×”×‘××™×:\n",
    "\n",
    "- [PySpark Create RDD with Examples](https://sparkbyexamples.com/pyspark/pyspark-create-rdd/)\n",
    "- [PySpark Replace Column Values in DataFrame](https://sparkbyexamples.com/pyspark/pyspark-replace-column-values/)\n",
    "- [PySpark repartition() â€“ Explained with Examples](https://sparkbyexamples.com/pyspark/pyspark-repartition/)\n",
    "- [What is PySpark DataFrame?](https://sparkbyexamples.com/pyspark/what-is-pyspark-dataframe/)\n",
    "- [PySpark RDD Actions with examples](https://sparkbyexamples.com/pyspark/pyspark-rdd-actions/)\n",
    "- [PySpark RDD Transformations with examples](https://sparkbyexamples.com/pyspark/pyspark-rdd-transformations/)\n",
    "- [Convert PySpark RDD to DataFrame](https://sparkbyexamples.com/pyspark/convert-pyspark-rdd-to-dataframe/)\n",
    "- [PySpark Row using on DataFrame and RDD](https://sparkbyexamples.com/pyspark/pyspark-row-using-on-dataframe-and-rdd/)\n",
    "- [PySpark RDD Actions with examples](https://sparkbyexamples.com/pyspark/pyspark-rdd-actions/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Œ ×ª×’×™×•×ª\n",
    "\n",
    "`#PySpark` `#RDD` `#parallelize` `#BigData` `#SparkContext` `#DataEngineering`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ“š ××§×•×¨\n",
    "\n",
    "××“×¨×™×š ×–×” ××‘×•×¡×¡ ×¢×œ ×”××××¨ ×”××§×•×¨×™ ×××ª×¨ [SparkByExamples.com](https://sparkbyexamples.com/pyspark/pyspark-parallelize-create-rdd/)\n",
    "\n",
    "**×ª××¨×™×š:** 23 ×‘×¤×‘×¨×•××¨, 2025  \n",
    "**××—×‘×¨:** Naveen Nelamali\n",
    "\n",
    "---\n",
    "\n",
    "**×”×¢×¨×”:** ×”×§×•×“ ×‘××—×‘×¨×ª ×–×• ×“×•×¨×© ×”×ª×§× ×” ×©×œ PySpark. × ×™×ª×Ÿ ×œ×”×ª×§×™×Ÿ ×‘×××¦×¢×•×ª:  \n",
    "`pip install pyspark`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
