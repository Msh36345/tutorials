{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<style>\n",
    "    body, * {\n",
    "        direction: rtl !important;\n",
    "        text-align: right !important;\n",
    "    }\n",
    "</style>\n",
    "××§×•×¨: [Spark By Examples - PySpark sparkcontext explained](https://sparkbyexamples.com/pyspark/pyspark-sparkcontext-explained/)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ğŸš€ PySpark SparkContext - ××“×¨×™×š ××§×™×£ ×‘×¢×‘×¨×™×ª\n",
    "\n",
    "**×××ª:** Naveen Nelamali\n",
    "**×ª××¨×™×š:** 9 ×‘×™×•×œ×™, 2025\n",
    "**×–××Ÿ ×§×¨×™××” ××©×•×¢×¨:** 10 ×“×§×•×ª\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ ×ª×•×›×Ÿ ×¢× ×™×™× ×™×\n",
    "\n",
    "1. [SparkContext ×‘-PySpark Shell](#1-sparkcontext-×‘-pyspark-shell)\n",
    "2. [×™×¦×™×¨×ª SparkContext ×‘-PySpark](#2-×™×¦×™×¨×ª-sparkcontext-×‘-pyspark)\n",
    "3. [×¢×¦×™×¨×ª SparkContext](#3-×¢×¦×™×¨×ª-sparkcontext)\n",
    "4. [×™×¦×™×¨×ª SparkContext ×œ×¤× ×™ PySpark 2.0](#4-×™×¦×™×¨×ª-sparkcontext-×œ×¤× ×™-pyspark-20)\n",
    "5. [×™×¦×™×¨×ª PySpark RDD](#5-×™×¦×™×¨×ª-pyspark-rdd)\n",
    "6. [××©×ª× ×™× × ×¤×•×¦×™× ×‘-SparkContext](#6-××©×ª× ×™×-× ×¤×•×¦×™×-×‘-sparkcontext)\n",
    "7. [××ª×•×“×•×ª × ×¤×•×¦×•×ª ×‘-SparkContext](#7-××ª×•×“×•×ª-× ×¤×•×¦×•×ª-×‘-sparkcontext)\n",
    "8. [×¡×™×›×•×](#8-×¡×™×›×•×)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“– ××‘×•×\n",
    "\n",
    "**`pyspark.SparkContext`** ×”×•× × ×§×•×“×ª ×”×›× ×™×¡×” ×œ×¤×•× ×§×¦×™×•× ×œ×™×•×ª ×©×œ PySpark. ×”×•× ××©××© ×œ×ª×§×©×•×¨×ª ×¢× ×”××©×›×•×œ (cluster) ×•×œ×™×¦×™×¨×ª RDD, Accumulator ×•××©×ª× ×™ Broadcast.\n",
    "\n",
    "×‘××“×¨×™×š ×–×” ×ª×œ××“×•:\n",
    "- âœ… ×›×™×¦×“ ×œ×™×¦×•×¨ PySpark SparkContext ×¢× ×“×•×’×××•×ª\n",
    "- âœ… ×©×™××•×© ×‘-SparkContext ×‘-PySpark Shell\n",
    "- âœ… ×¢×¦×™×¨×ª SparkContext\n",
    "- âœ… ×™×¦×™×¨×ª SparkContext ×œ×¤× ×™ ×’×¨×¡×” 2.0\n",
    "- âœ… ××©×ª× ×™× ×•××ª×•×“×•×ª × ×¤×•×¦×™×\n",
    "\n",
    "> **âš ï¸ ×—×©×•×‘:** × ×™×ª×Ÿ ×œ×™×¦×•×¨ ×¨×§ **SparkContext ××—×“** ×œ-JVM. ×›×“×™ ×œ×™×¦×•×¨ ××—×¨ ×—×“×©, ×™×© ×œ×¢×¦×•×¨ ××ª ×”×§×™×™× ×‘×××¦×¢×•×ª ×”××ª×•×“×” `stop()`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ ××¨×›×™×˜×§×˜×•×¨×ª SparkContext\n",
    "\n",
    "×ª×•×›× ×ª ×”-Driver ×©×œ Spark ×™×•×¦×¨×ª ×•××©×ª××©×ª ×‘-SparkContext ×›×“×™ ×œ×”×ª×—×‘×¨ ×œ×× ×”×œ ×”××©×›×•×œ (Cluster Manager) ×œ×©×œ×™×—×ª ×¢×‘×•×“×•×ª PySpark, ×•×œ×“×¢×ª ×œ××™×–×” ×× ×”×œ ××©××‘×™× (YARN, Mesos, ××• Standalone) ×œ×ª×§×©×¨.\n",
    "\n",
    "**SparkContext ×”×•× ×”×œ×‘ ×©×œ ××¤×œ×™×§×¦×™×™×ª PySpark!**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Driver Program  â”‚\n",
    "â”‚                 â”‚\n",
    "â”‚  SparkContext â”€â”€â”¼â”€â”€â–º Cluster Manager â”€â”€â–º Worker Nodes\n",
    "â”‚                 â”‚                          (Executors + Cache + Tasks)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "*××§×•×¨: spark.apache.org*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. SparkContext ×‘-PySpark Shell\n",
    "\n",
    "×›××©×¨ ××ª× ×¤×•×ª×—×™× ××ª ×”-PySpark Shell, ×”××¢×¨×›×ª ×™×•×¦×¨×ª ×•××¡×¤×§×ª ×‘××•×¤×Ÿ ××•×˜×•××˜×™ ××•×‘×™×™×§×˜ `sc`, ×©×”×•× ××•×¤×¢ ×©×œ ××—×œ×§×ª SparkContext.\n",
    "\n",
    "× ×™×ª×Ÿ ×œ×”×©×ª××© ×‘××•×‘×™×™×§×˜ ×–×” ×™×©×™×¨×•×ª ×œ×œ× ×¦×•×¨×š ×œ×™×¦×•×¨ ××•×ª×• ××—×“×©.\n",
    "\n",
    "### ×“×•×’××” - ×©×™××•×© ×™×©×™×¨ ×‘-sc"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×‘-PySpark Shell, sc ×›×‘×¨ ×–××™×Ÿ\n",
    "# ××™×Ÿ ×¦×•×¨×š ×œ×™×¦×•×¨ ××•×ª×•\n",
    "##sc.appName"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ ×”×¡×‘×¨\n",
    "\n",
    "×‘×¨×•×‘ ×”×›×œ×™×, ×”××—×‘×¨×•×ª ×•-Azure Databricks, ×”×¡×‘×™×‘×” ×¢×¦××” ×™×•×¦×¨×ª ××•×‘×™×™×§×˜ SparkContext ×‘×¨×™×¨×ª ××—×“×œ ×©××¤×©×¨ ×œ×”×©×ª××© ×‘×•, ×›×š ×©××™×Ÿ ×¦×•×¨×š ×œ×™×¦×•×¨ ×”×§×©×¨ (context) ×©×œ PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. ×™×¦×™×¨×ª SparkContext ×‘-PySpark\n",
    "\n",
    "×××– PySark 2.0, ×™×¦×™×¨×ª **SparkSession** ×™×•×¦×¨×ª SparkContext ×‘××•×¤×Ÿ ×¤× ×™××™ ×•×—×•×©×¤×ª ××ª ×”××©×ª× ×” `sparkContext` ×œ×©×™××•×©.\n",
    "\n",
    "### ×“×•×’××” - ×™×¦×™×¨×” ×“×¨×š SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×¦×™×¨×ª SparkSession ×-builder\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName(\"SparkByExamples.com\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "print(spark.sparkContext)\n",
    "print(\"Spark App Name : \" + spark.sparkContext.appName)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“¤ ×¤×œ×˜ ×¦×¤×•×™\n",
    "\n",
    "```\n",
    "<SparkContext master=local[1] appName=SparkByExamples.com>\n",
    "Spark App Name : SparkByExamples.com\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ ×”×¡×‘×¨\n",
    "\n",
    "×›×¤×™ ×©×”×•×¡×‘×¨ ×‘××××¨ ×¢×œ [SparkSession](https://sparkbyexamples.com/pyspark/pyspark-sparksession/), × ×™×ª×Ÿ ×œ×™×¦×•×¨ ×›×œ ××¡×¤×¨ ×©×œ ××•×‘×™×™×§×˜×™ SparkSession, ××•×œ× ×¢×‘×•×¨ ×›×œ ×”××•×‘×™×™×§×˜×™× ×”×œ×œ×• ×™×”×™×” ×¨×§ SparkContext ××—×“."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš ï¸ ××–×”×¨×” - ×‘×¢×™×” × ×¤×•×¦×”\n",
    "\n",
    "×‘×›×œ ×–××Ÿ × ×ª×•×Ÿ ×™×›×•×œ ×œ×”×™×•×ª ×¨×§ ××•×¤×¢ **××—×“** ×©×œ `SparkContext` ×¤×¢×™×œ ×œ-JVM. ×‘××§×¨×” ×©×ª×¨×¦×• ×œ×™×¦×•×¨ ××—×¨ × ×•×¡×£, ×ª×—×™×œ×” ×¢×œ×™×›× ×œ×¢×¦×•×¨ ××ª ×”-SparkContext ×”×§×™×™× ×‘×××¦×¢×•×ª `stop()` ×œ×¤× ×™ ×™×¦×™×¨×ª ××—×“ ×—×“×©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. ×¢×¦×™×¨×ª SparkContext\n",
    "\n",
    "× ×™×ª×Ÿ ×œ×¢×¦×•×¨ ××ª ×”-SparkContext ×¢×œ ×™×“×™ ×§×¨×™××” ×œ××ª×•×“×” `stop()`. \n",
    "\n",
    "### ×“×•×’××” - ×¢×¦×™×¨×ª SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ××ª×•×“×ª stop() ×©×œ SparkContext\n",
    "spark.sparkContext.stop()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“¤ ×¤×œ×˜ ×¦×¤×•×™\n",
    "\n",
    "×›××©×¨ PySpark ××‘×¦×¢ ×”×¦×”×¨×” ×–×•, ×”×•× ×¨×•×©× ××ª ×”×”×•×“×¢×”:\n",
    "\n",
    "```\n",
    "INFO SparkContext: Successfully stopped SparkContext\n",
    "```\n",
    "\n",
    "×œ×”×•×“×¢×” ×–×• × ×™×ª×Ÿ ×œ×¨××•×ª ×‘-console ××• ×‘×§×•×‘×¥ log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš« ×©×’×™××” - × ×™×¡×™×•×Ÿ ×œ×™×¦×•×¨ SparkContext × ×•×¡×£\n",
    "\n",
    "×›××©×¨ ×ª× ×¡×• ×œ×™×¦×•×¨ ××¡×¤×¨ ××•×‘×™×™×§×˜×™ SparkContext, ×ª×§×‘×œ×• ××ª ×”×©×’×™××” ×”×‘××”:\n",
    "\n",
    "```python\n",
    "ValueError: Cannot run multiple SparkContexts at once;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. ×™×¦×™×¨×ª SparkContext ×œ×¤× ×™ PySpark 2.0\n",
    "\n",
    "×œ×¤× ×™ ×’×¨×¡×ª PySpark 2.0, ×”×™×” ×¦×•×¨×š ×œ×™×¦×•×¨ SparkContext ×‘××•×¤×Ÿ ×ª×›× ×•×ª×™ ×‘×××¦×¢×•×ª ×”×§×•× ×¡×˜×¨×•×§×˜×•×¨, ×•×œ×”×¢×‘×™×¨ ×¤×¨××˜×¨×™× ×›××• master ×•-appName ×›×¤×¨××˜×¨×™× ×—×•×‘×” ×œ×¤×—×•×ª.\n",
    "\n",
    "### ×“×•×’××” 1 - ×©×™××•×© ×‘×§×•× ×¡×˜×¨×•×§×˜×•×¨ ×™×©×™×¨"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×¦×™×¨×ª SparkContext\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"Spark_Example_App\")\n",
    "print(sc.appName)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "×”×“×•×’××” ×œ×¢×™×œ ×™×•×¦×¨×ª ×”×§×©×¨ ×¢× master ×›-`local` ×•×©× ×”××¤×œ×™×§×¦×™×” ×›-`Spark_Example_App`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ×“×•×’××” 2 - ×©×™××•×© ×‘-getOrCreate()\n",
    "\n",
    "× ×™×ª×Ÿ ×’× ×œ×™×¦×•×¨ ××•×ª×• ×‘×××¦×¢×•×ª `SparkContext.getOrCreate()`. ×©×™×˜×” ×–×• ××—×–×™×¨×” SparkContext ×§×™×™× ×¤×¢×™×œ, ××• ×™×•×¦×¨×ª ×—×“×© ×¢× master ×•×©× ××¤×œ×™×§×¦×™×” ××•×’×“×¨×™×."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×¦×™×¨×ª Spark Context\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local\").setAppName(\"Spark_Example_App\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "print(sc.appName)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. ×™×¦×™×¨×ª PySpark RDD\n",
    "\n",
    "×‘×¨×’×¢ ×©×™×© ×œ×›× ××•×‘×™×™×§×˜ SparkContext, ×ª×•×›×œ×• ×œ×™×¦×•×¨ PySpark RDD ×‘××¡×¤×¨ ×“×¨×›×™×. ×œ×”×œ×Ÿ ×”×©×ª××©×ª×™ ×‘×¤×•× ×§×¦×™×” `range()` ×œ×™×¦×™×¨×ª RDD.\n",
    "\n",
    "### ×“×•×’××” - ×™×¦×™×¨×ª RDD ×¤×©×•×˜"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ××ª×—×œ ×¡×©×Ÿ ×—×“×© (×›×©×”Ö¾master=local[*] ×¢×•×‘×“ ×‘×¡×‘×™×‘×ª ×¤×™×ª×•×— ××§×•××™×ª)\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(\"SparkContext active:\", sc is not None)\n",
    "\n",
    "# ×™×¦×™×¨×ª RDD\n",
    "rdd = spark.sparkContext.range(1, 5)\n",
    "print(rdd.collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“¤ ×¤×œ×˜\n",
    "\n",
    "```python\n",
    "[1, 2, 3, 4]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. ××©×ª× ×™× × ×¤×•×¦×™× ×‘-SparkContext\n",
    "\n",
    "×œ×”×œ×Ÿ ×¨×©×™××ª ×”××©×ª× ×™× ×”× ×¤×•×¦×™× ×‘×™×•×ª×¨ ×©×–××™× ×™× ×‘×××¦×¢×•×ª SparkContext:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ†” applicationId\n",
    "\n",
    "××—×–×™×¨ ××–×”×” ×™×™×—×•×“×™ (Unique ID) ×©×œ ××¤×œ×™×§×¦×™×™×ª PySpark."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×§×‘×œ×ª Application ID\n",
    "print(\"Application ID: \" + spark.sparkContext.applicationId)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Œ version\n",
    "\n",
    "××—×–×™×¨ ××ª ×’×¨×¡×ª ××©×›×•×œ PySpark ×©×‘×” ×”×¢×‘×•×“×” ×©×œ×›× ×¨×¦×”."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×§×‘×œ×ª ×’×¨×¡×ª Spark\n",
    "print(\"Spark Version: \" + spark.sparkContext.version)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸŒ uiWebUrl\n",
    "\n",
    "××¡×¤×§ ××ª ×”-URL ×©×œ ×××©×§ ×”××©×ª××© (Spark Web UI) ×©×”×•×¤×¢×œ ×¢×œ ×™×“×™ SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×§×‘×œ×ª Spark UI URL\n",
    "print(\"Spark UI URL: \" + spark.sparkContext.uiWebUrl)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. ××ª×•×“×•×ª × ×¤×•×¦×•×ª ×‘-SparkContext\n",
    "\n",
    "×œ×”×œ×Ÿ ×¨×©×™××ª ×”××ª×•×“×•×ª ×”× ×¤×•×¦×•×ª ×‘×™×•×ª×¨ ×©×–××™× ×•×ª ×“×¨×š SparkContext:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š accumulator(value[, accum_param])\n",
    "\n",
    "×™×•×¦×¨×ª [××©×ª× ×” accumulator ×©×œ PySpark](https://sparkbyexamples.com/pyspark/pyspark-accumulator-with-example/) ×¢× ×¢×¨×š ×”×ª×—×œ×ª×™ ×©×¦×•×™×Ÿ.\n",
    "\n",
    "×¨×§ ×ª×•×›× ×ª ×”-driver ×™×›×•×œ×” ×œ×’×©×ª ×œ××©×ª× ×™ accumulator.\n",
    "\n",
    "### ×“×•×’××”"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×¦×™×¨×ª Accumulator\n",
    "accum = spark.sparkContext.accumulator(0)\n",
    "\n",
    "# ×©×™××•×© ×‘-Accumulator\n",
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "rdd.foreach(lambda x: accum.add(x))\n",
    "\n",
    "print(\"Accumulator value: \" + str(accum.value))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ“¡ broadcast(value)\n",
    "\n",
    "×™×•×¦×¨×ª [××©×ª× ×” broadcast ×©×œ PySpark](https://sparkbyexamples.com/pyspark/pyspark-broadcast-variables/) ×œ×§×¨×™××” ×‘×œ×‘×“.\n",
    "\n",
    "×”××©×ª× ×” ×™×©×•×“×¨ ×œ×›×œ ×”××©×›×•×œ. × ×™×ª×Ÿ ×œ×©×“×¨ ××©×ª× ×” ×œ××©×›×•×œ PySpark ×¤×¢× ××—×ª ×‘×œ×‘×“.\n",
    "\n",
    "### ×“×•×’××”"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×¦×™×¨×ª Broadcast Variable\n",
    "broadcast_var = spark.sparkContext.broadcast([1, 2, 3])\n",
    "\n",
    "# ×©×™××•×© ×‘-Broadcast Variable\n",
    "print(\"Broadcast value: \" + str(broadcast_var.value))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ“ emptyRDD()\n",
    "\n",
    "×™×•×¦×¨×ª [RDD ×¨×™×§](https://sparkbyexamples.com/pyspark/pyspark-create-an-empty-rdd/).\n",
    "\n",
    "### ×“×•×’××”"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×¦×™×¨×ª Empty RDD\n",
    "empty_rdd = spark.sparkContext.emptyRDD()\n",
    "print(\"Is empty: \" + str(empty_rdd.isEmpty()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ”„ getOrCreate()\n",
    "\n",
    "×™×•×¦×¨×ª ××• ××—×–×™×¨×” SparkContext ×§×™×™×.\n",
    "\n",
    "### ×“×•×’××”"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×§×‘×œ×” ××• ×™×¦×™×¨×” ×©×œ SparkContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"MyApp\").setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "print(sc.appName)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ“‚ hadoopFile()\n",
    "\n",
    "××—×–×™×¨×” [RDD](https://sparkbyexamples.com/pyspark/pyspark-rdd-tutorial/) ×©×œ ×§×•×‘×¥ Hadoop.\n",
    "\n",
    "### ğŸ“‚ newAPIHadoopFile()\n",
    "\n",
    "×™×•×¦×¨×ª [RDD](https://sparkbyexamples.com/pyspark/pyspark-rdd-tutorial/) ×¢×‘×•×¨ ×§×•×‘×¥ Hadoop ×¢× API ×—×“×© ×©×œ InputFormat.\n",
    "\n",
    "### ğŸ“‹ sequenceFile()\n",
    "\n",
    "××§×‘×œ×ª [RDD](https://sparkbyexamples.com/pyspark/pyspark-rdd-tutorial/) ×¢×‘×•×¨ SequenceFile ×©×œ Hadoop ×¢× ××¤×ª×— ×¢×¨×š × ×ª×•× ×™×.\n",
    "\n",
    "### âš™ï¸ setLogLevel()\n",
    "\n",
    "××©× ×” ××ª ×¨××ª ×”-log ×œ-debug, info, warn, fatal, ×•-error.\n",
    "\n",
    "### ğŸ“„ textFile()\n",
    "\n",
    "[×§×•×¨××ª ×§×•×‘×¥ ×˜×§×¡×˜](https://sparkbyexamples.com/pyspark/pyspark-read-text-file-rdd-dataframe/) ×-HDFS, ××§×•××™ ××• ×›×œ ××¢×¨×›×ª ×§×‘×¦×™× × ×ª××›×ª ×©×œ Hadoop ×•××—×–×™×¨×” RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ”— union()\n",
    "\n",
    "×××—×“×ª ×©× ×™ RDDs.\n",
    "\n",
    "### ×“×•×’××”"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ××™×—×•×“ ×©× ×™ RDDs\n",
    "rdd1 = spark.sparkContext.parallelize([1, 2, 3])\n",
    "rdd2 = spark.sparkContext.parallelize([4, 5, 6])\n",
    "union_rdd = rdd1.union(rdd2)\n",
    "print(union_rdd.collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ“ wholeTextFiles()\n",
    "\n",
    "[×§×•×¨××ª ×§×•×‘×¥ ×˜×§×¡×˜ ××ª×™×§×™×™×”](https://sparkbyexamples.com/pyspark/pyspark-read-text-file-rdd-dataframe/) ×-HDFS, ××§×•××™ ××• ×›×œ ××¢×¨×›×ª ×§×‘×¦×™× × ×ª××›×ª ×©×œ Hadoop ×•××—×–×™×¨×” RDD ×©×œ Tuple2.\n",
    "\n",
    "×”××œ×× ×˜ ×”×¨××©×•×Ÿ ×©×œ ×”-tuple ××›×™×œ ××ª ×©× ×”×§×•×‘×¥ ×•×”××œ×× ×˜ ×”×©× ×™ ××›×™×œ ××ª ×ª×•×›×Ÿ ×§×•×‘×¥ ×”×˜×§×¡×˜."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. ×¡×™×›×•× ğŸ¯\n",
    "\n",
    "×‘××××¨ PySpark Context ×–×”, ×œ××“×ª×:\n",
    "\n",
    "âœ… **××”×• SparkContext** - × ×§×•×“×ª ×”×›× ×™×¡×” ×œ×¤×•× ×§×¦×™×•× ×œ×™×•×ª PySpark  \n",
    "âœ… **×›×™×¦×“ ×œ×™×¦×•×¨ ××•×ª×•** - ×“×¨×š SparkSession ××• ×‘××•×¤×Ÿ ×™×©×™×¨  \n",
    "âœ… **×›×™×¦×“ ×œ×¢×¦×•×¨ ××•×ª×•** - ×©×™××•×© ×‘××ª×•×“×” `stop()`  \n",
    "âœ… **×©×™××•×© ×¢× ×”××©×›×•×œ** - ×ª×§×©×•×¨×ª ×¢× Cluster Manager  \n",
    "\n",
    "×‘×××¦×¢×•×ª SparkContext ×ª×•×›×œ×• ×œ×™×¦×•×¨:\n",
    "- ğŸ“Š **RDD** (Resilient Distributed Datasets)\n",
    "- ğŸ“ˆ **Accumulators** (××©×ª× ×™ ×¦×‘×™×¨×”)\n",
    "- ğŸ“¡ **Broadcast variables** (××©×ª× ×™ ×©×™×“×•×¨)\n",
    "\n",
    "### ğŸ’¡ ×–×›×¨×•\n",
    "\n",
    "SparkContext ×”×•× × ×§×•×“×ª ×”×›× ×™×¡×” ×œ×× ×•×¢ ×”×‘×™×¦×•×¢ ×©×œ PySpark ××©×¨ ××ª×§×©×¨ ×¢× ×”××©×›×•×œ!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š ××××¨×™× ×§×©×•×¨×™×\n",
    "\n",
    "- [How to add Multiple Jars to PySpark](https://sparkbyexamples.com/pyspark/)\n",
    "- [PySpark Read and Write SQL Server Table](https://sparkbyexamples.com/pyspark/pyspark-read-and-write-sql-server-table/)\n",
    "- [PySpark Read and Write MySQL Database Table](https://sparkbyexamples.com/pyspark/pyspark-read-and-write-mysql-database-table/)\n",
    "- [PySpark createOrReplaceTempView() Explained](https://sparkbyexamples.com/pyspark/pyspark-createorreplacetempview/)\n",
    "- [PySpark cache() Explained](https://sparkbyexamples.com/pyspark/pyspark-cache-explained/)\n",
    "- [PySpark repartition() â€“ Explained with Examples](https://sparkbyexamples.com/pyspark/pyspark-repartition/)\n",
    "- [PySpark Create RDD with Examples](https://sparkbyexamples.com/pyspark/different-ways-to-create-dataframe-in-pyspark/)\n",
    "- [PySpark printSchema() Example](https://sparkbyexamples.com/pyspark/pyspark-printschema/)\n",
    "- [What's New in PySpark 4.0: Features, Improvements, and Enhancements](https://sparkbyexamples.com/pyspark/whats-new-in-pyspark-4-0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”— ××§×•×¨×•×ª ×•×”×¤× ×™×•×ª\n",
    "\n",
    "- **×ª×™×¢×•×“ ×¨×©××™:** [SparkContext.scala - Apache Spark GitHub](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala)\n",
    "- **××ª×¨ ×”××“×¨×™×š:** [SparkByExamples.com](https://sparkbyexamples.com/)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Happy Learning!\n",
    "\n",
    "**×ª×’×™×•×ª:** `SQLCONTEXT` | `PYSPARK` | `BIG DATA`\n",
    "\n",
    "*××“×¨×™×š ×–×” ×ª×•×¨×’× ×•×¢×•×‘×“ ×œ×¢×‘×¨×™×ª ××ª×•×š ×”××§×•×¨ ×‘××ª×¨ SparkByExamples.com*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
