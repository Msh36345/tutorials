{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[מקור](https://www.machinelearningplus.com/pyspark/pyspark-exercises-101-pyspark-exercises-for-data-analysis/)",
   "id": "95d0100cae8d5ba4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. How to import PySpark and check the version?\n",
    "\n",
    "Difficulty Level: L1"
   ],
   "id": "b769901a13bbee8f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:08.561160Z",
     "start_time": "2025-11-13T13:00:08.538624Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Name\").getOrCreate()\n",
    "print(spark.version)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.1\n"
     ]
    }
   ],
   "execution_count": 217
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. How to convert the index of a PySpark DataFrame into a column?",
   "id": "f9e379c8871abcee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:08.691399Z",
     "start_time": "2025-11-13T13:00:08.567845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = spark.createDataFrame([\n",
    "(\"Alice\", 1),\n",
    "(\"Bob\", 2),\n",
    "(\"Charlie\", 3),\n",
    "], [\"Name\", \"Value\"])\n",
    "\n",
    "df.show()"
   ],
   "id": "db0af2fd65911489",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Name|Value|\n",
      "+-------+-----+\n",
      "|  Alice|    1|\n",
      "|    Bob|    2|\n",
      "|Charlie|    3|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 218
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:09.000862Z",
     "start_time": "2025-11-13T13:00:08.722482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import row_number, to_date\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "win = Window.orderBy(\"Name\")\n",
    "df_index = df.withColumn(\"index\", row_number().over(win) - 1)\n",
    "df_index.show()"
   ],
   "id": "e87baf5652dd6a47",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/13 15:00:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 15:00:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 15:00:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+\n",
      "|   Name|Value|index|\n",
      "+-------+-----+-----+\n",
      "|  Alice|    1|    0|\n",
      "|    Bob|    2|    1|\n",
      "|Charlie|    3|    2|\n",
      "+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 219
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3. How to combine many lists to form a PySpark DataFrame?",
   "id": "f68e1f04b40b7425"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:09.122152Z",
     "start_time": "2025-11-13T13:00:09.027045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "list1 = [\"a\", \"b\", \"c\", \"d\"]\n",
    "list2 = [1, 2, 3, 4]\n",
    "\n",
    "\n",
    "data = list(zip(list1, list2))\n",
    "df_list = spark.createDataFrame(data, [\"Letter\", \"Number\"])\n",
    "df_list.show()"
   ],
   "id": "87ade9cc1b0500be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|Letter|Number|\n",
      "+------+------+\n",
      "|     a|     1|\n",
      "|     b|     2|\n",
      "|     c|     3|\n",
      "|     d|     4|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 220
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4. How to get the items of list A not present in list B?",
   "id": "1180f875bfa2aeb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:09.452970Z",
     "start_time": "2025-11-13T13:00:09.197313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "list_A = [1, 2, 3, 4, 5]\n",
    "list_B = [4, 5, 6, 7, 8]\n",
    "\n",
    "\n",
    "df_A = spark.createDataFrame([(x,) for x in list_A], [\"value\"])\n",
    "df_B = spark.createDataFrame([(x,) for x in list_B], [\"value\"])\n",
    "df_diff = df_A.join(df_B, on=\"value\", how=\"left_anti\")\n",
    "df_diff.show()\n",
    "\n",
    "\n",
    "# sc = spark.sparkContext\n",
    "# # Convert lists to RDD\n",
    "# rdd_A = sc.parallelize(list_A)\n",
    "# rdd_B = sc.parallelize(list_B)\n",
    "# # Perform subtract operation\n",
    "# result_rdd_A = rdd_A.subtract(rdd_B)\n",
    "# result_rdd_B = rdd_B.subtract(rdd_A)\n",
    "# # Union the two RDDs\n",
    "# result_rdd = result_rdd_A.union(result_rdd_B)\n",
    "# print(result_rdd.collect())"
   ],
   "id": "6d6ab51b2765597e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 221
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6. How to get the minimum, 25th percentile, median, 75th, and max of a numeric column?\n",
   "id": "541360e9c06142cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:09.788255Z",
     "start_time": "2025-11-13T13:00:09.487628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a sample DataFrame\n",
    "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "\n",
    "quantile = df.approxQuantile(\"Age\", [0.0, 0.25, 0.5, 0.75, 1.0], 0)\n",
    "\n",
    "print(f\"min : {quantile[0]}\")\n",
    "print(f\"25th percentile : {quantile[1]}\")\n",
    "print(f\"median : {quantile[2]}\")\n",
    "print(f\"75th percentile : {quantile[3]}\")\n",
    "print(f\"max : {quantile[4]}\")"
   ],
   "id": "6191adfc6ab095d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min : 10.0\n",
      "25th percentile : 20.0\n",
      "median : 30.0\n",
      "75th percentile : 50.0\n",
      "max : 86.0\n"
     ]
    }
   ],
   "execution_count": 222
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "7. How to get frequency counts of unique items of a column?\n",
   "id": "411f43395ae0c556"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:10.066526Z",
     "start_time": "2025-11-13T13:00:09.799156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import Row\n",
    "data = [\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='Mary', job='Scientist'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Scientist'),\n",
    "Row(name='Sam', job='Doctor'),\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "\n",
    "count_df = df.groupBy(\"job\").count().orderBy(\"count\", ascending=False)\n",
    "count_df.show()"
   ],
   "id": "39dab846cd13ea2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|      job|count|\n",
      "+---------+-----+\n",
      "| Engineer|    4|\n",
      "|Scientist|    2|\n",
      "|   Doctor|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 223
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "8. How to keep only top 2 most frequent values as it is and replace everything else as ‘Other’?",
   "id": "d5b6ceb107ae27c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:10.351601Z",
     "start_time": "2025-11-13T13:00:10.121858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "top_2 = count_df.select(\"job\").limit(2).rdd.flatMap(lambda x: x).collect()\n",
    "df_top_2 = df.withColumn('job', when(col('job').isin(top_2), col('job')).otherwise('Other'))\n",
    "df_top_2.show()\n"
   ],
   "id": "4d4aa4890f628e56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|name|      job|\n",
      "+----+---------+\n",
      "|John| Engineer|\n",
      "|John| Engineer|\n",
      "|Mary|Scientist|\n",
      "| Bob| Engineer|\n",
      "| Bob| Engineer|\n",
      "| Bob|Scientist|\n",
      "| Sam|    Other|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 224
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "9. How to Drop rows with NA values specific to a particular column?",
   "id": "febb220c656c5df9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:10.538355Z",
     "start_time": "2025-11-13T13:00:10.384203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df_dropped = df.na.drop(subset=[\"Value\"])\n",
    "df_dropped.show()"
   ],
   "id": "c0f835317dbac52e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n",
      "|Name|Value|  id|\n",
      "+----+-----+----+\n",
      "|   A|    1|NULL|\n",
      "|   B|    3| 456|\n",
      "+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 225
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "10. How to rename columns of a PySpark DataFrame using two lists – one containing the old column names and the other containing the new column names?",
   "id": "a0fb11a63e5030e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:10.764581Z",
     "start_time": "2025-11-13T13:00:10.571222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# suppose you have the following DataFrame\n",
    "df = spark.createDataFrame([(1, 2, 3), (4, 5, 6)], [\"col1\", \"col2\", \"col3\"])\n",
    "# old column names\n",
    "old_names = [\"col1\", \"col2\", \"col3\"]\n",
    "# new column names\n",
    "new_names = [\"new_col1\", \"new_col2\", \"new_col3\"]\n",
    "df.show()\n",
    "\n",
    "for old_name, new_name in zip(old_names, new_names):\n",
    "    df = df.withColumnRenamed(old_name, new_name)\n",
    "df.show()"
   ],
   "id": "2d03df7df6ba272d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   4|   5|   6|\n",
      "+----+----+----+\n",
      "\n",
      "+--------+--------+--------+\n",
      "|new_col1|new_col2|new_col3|\n",
      "+--------+--------+--------+\n",
      "|       1|       2|       3|\n",
      "|       4|       5|       6|\n",
      "+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 226
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "11. How to bin a numeric list to 10 groups of equal size?",
   "id": "81ebd148c61ba59a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:11.206947Z",
     "start_time": "2025-11-13T13:00:10.793139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import rand\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "df = spark.range(100).select(rand(seed=42).alias(\"values\"))\n",
    "\n",
    "\n",
    "num_buckets = 10\n",
    "list_quantiles = [i / num_buckets for i in range(num_buckets + 1)]\n",
    "quantiles = df.stat.approxQuantile(\"values\", list_quantiles, 0)\n",
    "bucketizer = Bucketizer(splits=quantiles, inputCol=\"values\", outputCol=\"buckets\")\n",
    "df_buck = bucketizer.transform(df)\n",
    "df_buck.groupBy(\"buckets\").count().orderBy(\"buckets\").show()\n"
   ],
   "id": "b1244261fb4a06e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|buckets|count|\n",
      "+-------+-----+\n",
      "|    0.0|    9|\n",
      "|    1.0|   10|\n",
      "|    2.0|   10|\n",
      "|    3.0|   10|\n",
      "|    4.0|   10|\n",
      "|    5.0|   10|\n",
      "|    6.0|   10|\n",
      "|    7.0|   10|\n",
      "|    8.0|   10|\n",
      "|    9.0|   11|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 227
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "12. How to create contigency table?\n",
   "id": "d3568b57906d8873"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:11.887972Z",
     "start_time": "2025-11-13T13:00:11.253151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = [(\"A\", \"X\"), (\"A\", \"Y\"), (\"A\", \"X\"), (\"B\", \"Y\"), (\"B\", \"X\"), (\"C\", \"X\"), (\"C\", \"X\"), (\"C\", \"Y\")]\n",
    "df = spark.createDataFrame(data, [\"category1\", \"category2\"])\n",
    "\n",
    "df.groupBy(\"category1\", \"category2\").count().show()\n",
    "\n",
    "\n",
    "df.crosstab('category1', 'category2').show()\n"
   ],
   "id": "e128b11d3db49199",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+\n",
      "|category1|category2|count|\n",
      "+---------+---------+-----+\n",
      "|        A|        X|    2|\n",
      "|        A|        Y|    1|\n",
      "|        B|        Y|    1|\n",
      "|        B|        X|    1|\n",
      "|        C|        X|    2|\n",
      "|        C|        Y|    1|\n",
      "+---------+---------+-----+\n",
      "\n",
      "+-------------------+---+---+\n",
      "|category1_category2|  X|  Y|\n",
      "+-------------------+---+---+\n",
      "|                  B|  1|  1|\n",
      "|                  C|  2|  1|\n",
      "|                  A|  2|  1|\n",
      "+-------------------+---+---+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 228
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "13. How to find the numbers that are multiples of 3 from a column?",
   "id": "258027d05091cce5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:11.995148Z",
     "start_time": "2025-11-13T13:00:11.909686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "df = spark.range(10)\n",
    "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
    "\n",
    "\n",
    "df = df.withColumn(\"multiple_of_3\", when(col(\"random\") % 3 == 0, 1).otherwise(0))\n",
    "df.show()"
   ],
   "id": "c0e01cc0e6dd4d87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+\n",
      "| id|random|multiple_of_3|\n",
      "+---+------+-------------+\n",
      "|  0|     7|            0|\n",
      "|  1|     9|            1|\n",
      "|  2|     8|            0|\n",
      "|  3|     8|            0|\n",
      "|  4|     3|            1|\n",
      "|  5|     1|            0|\n",
      "|  6|     7|            0|\n",
      "|  7|     4|            0|\n",
      "|  8|     5|            0|\n",
      "|  9|     1|            0|\n",
      "+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 229
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "14. How to extract items at given positions from a column?",
   "id": "922e79e9dccab0d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:12.108605Z",
     "start_time": "2025-11-13T13:00:12.012260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "df = spark.range(10)\n",
    "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
    "pos = [0, 4, 8, 5]\n",
    "\n",
    "\n",
    "df_filtered = df.filter(col(\"id\").isin(pos))\n",
    "df_filtered.show()"
   ],
   "id": "2dc03abaa90b876c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|random|\n",
      "+---+------+\n",
      "|  0|     7|\n",
      "|  4|     3|\n",
      "|  5|     1|\n",
      "|  8|     5|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 230
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "15. How to stack two DataFrames vertically ?",
   "id": "bde95970ec5d8bef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:12.477872Z",
     "start_time": "2025-11-13T13:00:12.122209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_A = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 10), (\"orange\", 2, 8)], [\"Name\", \"Col_1\", \"Col_2\"])\n",
    "df_B = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 15), (\"grape\", 4, 6)], [\"Name\", \"Col_1\", \"Col_3\"])\n",
    "\n",
    "\n",
    "df_A.union(df_B).groupBy(\"Name\").sum().show()"
   ],
   "id": "993296b4622edac9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+\n",
      "|  Name|sum(Col_1)|sum(Col_2)|\n",
      "+------+----------+----------+\n",
      "| apple|         6|        10|\n",
      "|banana|         2|        25|\n",
      "|orange|         2|         8|\n",
      "| grape|         4|         6|\n",
      "+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 231
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "16. How to compute the mean squared error on a truth and predicted columns?",
   "id": "7c2e14912d81a464"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:12.722209Z",
     "start_time": "2025-11-13T13:00:12.563939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "data = [(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]\n",
    "df = spark.createDataFrame(data, [\"actual\", \"predicted\"])\n",
    "\n",
    "\n",
    "# Calculate the squared differences\n",
    "df = df.withColumn(\"squared_error\", pow((col(\"actual\") - col(\"predicted\")), 2))\n",
    "\n",
    "# Calculate the mean squared error\n",
    "df.agg(mean(\"squared_error\")).show()\n"
   ],
   "id": "5af28141de4fa0c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|avg(squared_error)|\n",
      "+------------------+\n",
      "|             116.8|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 232
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "17. How to convert the first character of each element in a series to uppercase?",
   "id": "a4ff661bc985b993"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:12.869819Z",
     "start_time": "2025-11-13T13:00:12.750182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "\n",
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "\n",
    "\n",
    "df = df.withColumn(\"name\", initcap(df[\"name\"]))\n",
    "df.show()"
   ],
   "id": "785be64dcd2be3f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "| John|\n",
      "|Alice|\n",
      "|  Bob|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 233
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "18. How to compute summary statistics for all columns in a dataframe",
   "id": "cb847fbba945a6e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:13.200967Z",
     "start_time": "2025-11-13T13:00:12.915395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = [('James', 34, 55000),\n",
    "('Michael', 30, 70000),\n",
    "('Robert', 37, 60000),\n",
    "('Maria', 29, 80000),\n",
    "('Jen', 32, 65000)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
    "\n",
    "\n",
    "df.summary().show()\n"
   ],
   "id": "d2eaec231be76fa2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------------+-----------------+\n",
      "|summary|  name|              age|           salary|\n",
      "+-------+------+-----------------+-----------------+\n",
      "|  count|     5|                5|                5|\n",
      "|   mean|  NULL|             32.4|          66000.0|\n",
      "| stddev|  NULL|3.209361307176242|9617.692030835675|\n",
      "|    min| James|               29|            55000|\n",
      "|    25%|  NULL|               30|            60000|\n",
      "|    50%|  NULL|               32|            65000|\n",
      "|    75%|  NULL|               34|            70000|\n",
      "|    max|Robert|               37|            80000|\n",
      "+-------+------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 234
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "19. How to calculate the number of characters in each word in a column?",
   "id": "115e275e0126d1d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:13.404820Z",
     "start_time": "2025-11-13T13:00:13.241117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import length\n",
    "\n",
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "\n",
    "df = df.withColumn(\"name_length\", length(df[\"name\"]))\n",
    "df.show()"
   ],
   "id": "d021efdbcb1e095a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "| name|name_length|\n",
      "+-----+-----------+\n",
      "| john|          4|\n",
      "|alice|          5|\n",
      "|  bob|          3|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 235
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "20 How to compute difference of differences between consecutive numbers of a column?",
   "id": "a743db483e63c11f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:13.603481Z",
     "start_time": "2025-11-13T13:00:13.449114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, col\n",
    "\n",
    "data = [('James', 34, 55000),\n",
    "('Michael', 30, 70000),\n",
    "('Robert', 37, 60000),\n",
    "('Maria', 29, 80000),\n",
    "('Jen', 32, 65000)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
    "\n",
    "\n",
    "win = Window.orderBy(\"salary\")\n",
    "df = df.withColumn(\"salary_diff\", col(\"salary\") - lag(col(\"salary\"), 1).over(win))\n",
    "df.show()"
   ],
   "id": "ea40c48920f0e084",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+-----------+\n",
      "|   name|age|salary|salary_diff|\n",
      "+-------+---+------+-----------+\n",
      "|  James| 34| 55000|       NULL|\n",
      "| Robert| 37| 60000|       5000|\n",
      "|    Jen| 32| 65000|       5000|\n",
      "|Michael| 30| 70000|       5000|\n",
      "|  Maria| 29| 80000|      10000|\n",
      "+-------+---+------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/13 15:00:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 15:00:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 15:00:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 15:00:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 15:00:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "execution_count": 236
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "21. How to get the day of month, week number, day of year and day of week from a date strings?",
   "id": "a3e7b14970bb83e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:13.790573Z",
     "start_time": "2025-11-13T13:00:13.627888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import to_date, dayofmonth, weekofyear, dayofyear, date_format\n",
    "\n",
    "data = [(\"2023-05-18\",\"01 Jan 2010\",), (\"2023-12-31\", \"01 Jan 2010\",)]\n",
    "df = spark.createDataFrame(data, [\"date_str_1\", \"date_str_2\"])\n",
    "\n",
    "\n",
    "df = df.withColumn(\"date_1\", to_date(col(\"date_str_1\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"date_2\", to_date(col(\"date_str_2\"), \"dd MMM yyyy\"))\n",
    "\n",
    "df = df.withColumn(\"day_of_month\", dayofmonth(col(\"date_1\"))) \\\n",
    "    .withColumn(\"week_of_year\", weekofyear(col(\"date_1\"))) \\\n",
    "    .withColumn(\"day_of_year\", dayofyear(col(\"date_1\"))) \\\n",
    "    .withColumn(\"day_of_week\", date_format(col(\"date_1\"), \"EEEE\"))\n",
    "df.select(\"date_str_1\", \"day_of_month\", \"week_of_year\", \"day_of_year\", \"day_of_week\",\"date_str_2\",\"date_2\").show()"
   ],
   "id": "bdf323d0464758d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+-----------+-----------+-----------+----------+\n",
      "|date_str_1|day_of_month|week_of_year|day_of_year|day_of_week| date_str_2|    date_2|\n",
      "+----------+------------+------------+-----------+-----------+-----------+----------+\n",
      "|2023-05-18|          18|          20|        138|   Thursday|01 Jan 2010|2010-01-01|\n",
      "|2023-12-31|          31|          52|        365|     Sunday|01 Jan 2010|2010-01-01|\n",
      "+----------+------------+------------+-----------+-----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 237
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "22. How to convert year-month string to dates corresponding to the 4th day of the month?",
   "id": "ab314a26061e9045"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:14.002702Z",
     "start_time": "2025-11-13T13:00:13.800387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "df = spark.createDataFrame([('Jan 2010',), ('Feb 2011',), ('Mar 2012',)], ['MonthYear'])\n",
    "\n",
    "\n",
    "df = df.withColumn('Date', to_date(\"MonthYear\", 'MMM yyyy')+3)\n",
    "df.show()"
   ],
   "id": "28e02f2ba8a0658f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|MonthYear|      Date|\n",
      "+---------+----------+\n",
      "| Jan 2010|2010-01-04|\n",
      "| Feb 2011|2011-02-04|\n",
      "| Mar 2012|2012-03-04|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 238
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "23 How to filter words that contain atleast 2 vowels from a series?",
   "id": "10c2e004b4b4cda3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:00:14.527867Z",
     "start_time": "2025-11-13T13:00:14.058501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import translate , col, length\n",
    "df = spark.createDataFrame([('Apple',), ('Orange',), ('Plan',) , ('Python',) , ('Money',)], ['Word'])\n",
    "df.show()\n",
    "\n",
    "df_filt = df.withColumn(\"Wrd\",translate(col('Word'),'AEIOUaeiou', ''))\\\n",
    "    .withColumn(\"diff\", length(col('Word')) - length(col('Wrd')))\\\n",
    "    .filter(col('diff') >= 2)\\\n",
    "    .drop('Wrd','diff')\n",
    "df_filt.show()\n"
   ],
   "id": "b857cdd686d02b0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  Word|\n",
      "+------+\n",
      "| Apple|\n",
      "|Orange|\n",
      "|  Plan|\n",
      "|Python|\n",
      "| Money|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|  Word|\n",
      "+------+\n",
      "| Apple|\n",
      "|Orange|\n",
      "| Money|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 239
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "24. How to filter valid emails from a list?",
   "id": "ddf47c5c3aea2f2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:14:07.355482Z",
     "start_time": "2025-11-13T13:14:07.147655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = [('buying books at amazom.com',),\n",
    "        ('rameses@egypt.com',),\n",
    "        ('matt@t.co',),\n",
    "        ('narendra@modi.com',)]\n",
    "df = spark.createDataFrame(data, [\"value\"])\n",
    "\n",
    "\n",
    "pattern = r'^[A-Za-z0-9_.+\\-]+@[A-Za-z0-9\\-]+\\.[A-Za-z0-9\\-.]+$'\n",
    "df.filter(col(\"value\").rlike(pattern)).show(truncate =False)"
   ],
   "id": "4a104ba3348f6f1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|value            |\n",
      "+-----------------+\n",
      "|rameses@egypt.com|\n",
      "|matt@t.co        |\n",
      "|narendra@modi.com|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 272
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "25. How to Pivot PySpark DataFrame?",
   "id": "bbe973c4b1c208fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:16:04.817030Z",
     "start_time": "2025-11-13T13:16:04.363252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = [\n",
    "(2021, 1, \"US\", 5000),\n",
    "(2021, 1, \"EU\", 4000),\n",
    "(2021, 2, \"US\", 5500),\n",
    "(2021, 2, \"EU\", 4500),\n",
    "(2021, 3, \"US\", 6000),\n",
    "(2021, 3, \"EU\", 5000),\n",
    "(2021, 4, \"US\", 7000),\n",
    "(2021, 4, \"EU\", 6000),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"year\", \"quarter\", \"region\", \"revenue\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "\n",
    "pivot_df = df.groupBy(\"year\", \"quarter\").pivot(\"region\").sum(\"revenue\").orderBy(\"year\",\"quarter\").show()"
   ],
   "id": "31129e2be93c7ba0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+-------+\n",
      "|year|quarter|region|revenue|\n",
      "+----+-------+------+-------+\n",
      "|2021|      1|    US|   5000|\n",
      "|2021|      1|    EU|   4000|\n",
      "|2021|      2|    US|   5500|\n",
      "|2021|      2|    EU|   4500|\n",
      "|2021|      3|    US|   6000|\n",
      "|2021|      3|    EU|   5000|\n",
      "|2021|      4|    US|   7000|\n",
      "|2021|      4|    EU|   6000|\n",
      "+----+-------+------+-------+\n",
      "\n",
      "+----+-------+----+----+\n",
      "|year|quarter|  EU|  US|\n",
      "+----+-------+----+----+\n",
      "|2021|      1|4000|5000|\n",
      "|2021|      2|4500|5500|\n",
      "|2021|      3|5000|6000|\n",
      "|2021|      4|6000|7000|\n",
      "+----+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 275
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "26. How to get the mean of a variable grouped by another variable?",
   "id": "3c1e1fbbc978cc69"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:18:36.163682Z",
     "start_time": "2025-11-13T13:18:35.696807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "data = [(\"1001\", \"Laptop\", 1000),\n",
    "(\"1002\", \"Mouse\", 50),\n",
    "(\"1003\", \"Laptop\", 1200),\n",
    "(\"1004\", \"Mouse\", 30),\n",
    "(\"1005\", \"Smartphone\", 700)]\n",
    "columns = [\"OrderID\", \"Product\", \"Price\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "\n",
    "result = df.groupBy(\"Product\").agg(avg(\"Price\").alias(\"Total_Sales\"))\n",
    "result.show()"
   ],
   "id": "6e2592e70c26e1bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+\n",
      "|OrderID|   Product|Price|\n",
      "+-------+----------+-----+\n",
      "|   1001|    Laptop| 1000|\n",
      "|   1002|     Mouse|   50|\n",
      "|   1003|    Laptop| 1200|\n",
      "|   1004|     Mouse|   30|\n",
      "|   1005|Smartphone|  700|\n",
      "+-------+----------+-----+\n",
      "\n",
      "+----------+-----------+\n",
      "|   Product|Total_Sales|\n",
      "+----------+-----------+\n",
      "|    Laptop|     1100.0|\n",
      "|     Mouse|       40.0|\n",
      "|Smartphone|      700.0|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 277
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "27. How to compute the euclidean distance between two columns?",
   "id": "83f88f735565d06d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:27:50.179549Z",
     "start_time": "2025-11-13T13:27:49.862776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import expr\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "data = [(1, 10), (2, 9), (3, 8), (4, 7), (5, 6), (6, 5), (7, 4), (8, 3), (9, 2), (10, 1)]\n",
    "df = spark.createDataFrame(data, [\"series1\", \"series2\"])\n",
    "\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=[\"series1\", \"series2\"], outputCol=\"vectors\")\n",
    "df = vecAssembler.transform(df)\n",
    "df = df.withColumn(\"squared_diff\", expr(\"POW(series1 - series2, 2)\"))\n",
    "df.agg(expr(\"SQRT(SUM(squared_diff))\").alias(\"euclidean_distance\")).show()"
   ],
   "id": "d5db313cf61864dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|euclidean_distance|\n",
      "+------------------+\n",
      "| 18.16590212458495|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 282
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "28. How to replace missing spaces in a string with the least frequent character?",
   "id": "15f88c68bbf1b209"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:40:20.924652Z",
     "start_time": "2025-11-13T13:40:20.662629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from collections import Counter\n",
    "\n",
    "df = spark.createDataFrame([('dbc deb abed gade',),], [\"string\"])\n",
    "\n",
    "\n",
    "def least_freq_char_replace_spaces(s):\n",
    "    counter = Counter(s.replace(\" \", \"\"))\n",
    "    least_freq_char = min(counter, key = counter.get)\n",
    "    return s.replace(' ', least_freq_char)\n",
    "\n",
    "udf_least_freq_char_replace_spaces = udf(least_freq_char_replace_spaces, StringType())\n",
    "\n",
    "df2 = df.withColumn('modified_string', udf_least_freq_char_replace_spaces(df['string']))\n",
    "df2.show(truncate=False)"
   ],
   "id": "6e61786198e3b1d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|string           |modified_string  |\n",
      "+-----------------+-----------------+\n",
      "|dbc deb abed gade|dbccdebcabedcgade|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 297
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "29. How to create a TimeSeries starting ‘2000-01-01’ and 10 weekends (saturdays) after that having random numbers as values?",
   "id": "fc361e86b56dd8fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T14:17:03.991494Z",
     "start_time": "2025-11-13T14:17:03.745034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import explode, sequence, rand, dayofweek\n",
    "\n",
    "data = [('2000-01-01','2000-04-01')]\n",
    "df = (spark.createDataFrame(data, [\"start_date\",\"end_date\"])\n",
    "      .withColumn(\"start_date\", to_date(col(\"start_date\")))\n",
    "      .withColumn(\"end_date\", to_date(col(\"end_date\"))))\n",
    "\n",
    "df = df.withColumn(\"dates\" , explode(sequence(col(\"start_date\"), col(\"end_date\"))))\n",
    "\n",
    "df = df.filter(dayofweek(df.dates) == 7).limit(10)\n",
    "\n",
    "df = df.withColumn(\"random_numbers\", (rand(seed=42)*10+1).cast(\"int\")).drop(\"start_date\",\"end_date\")\n",
    "df.show(truncate=False)"
   ],
   "id": "ee6ea21c1f789650",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|dates     |random_numbers|\n",
      "+----------+--------------+\n",
      "|2000-01-01|7             |\n",
      "|2000-01-08|6             |\n",
      "|2000-01-15|9             |\n",
      "|2000-01-22|3             |\n",
      "|2000-01-29|7             |\n",
      "|2000-02-05|6             |\n",
      "|2000-02-12|10            |\n",
      "|2000-02-19|1             |\n",
      "|2000-02-26|10            |\n",
      "|2000-03-04|8             |\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 313
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "30. How to get the nrows, ncolumns, datatype of a dataframe?",
   "id": "eccb34cf01099b7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T14:25:56.140743Z",
     "start_time": "2025-11-13T14:25:55.879743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = [(\"1001\", \"Laptop\", 1000),\n",
    "(\"1002\", \"Mouse\", 50),\n",
    "(\"1003\", \"Laptop\", 1200),\n",
    "(\"1004\", \"Mouse\", 30),\n",
    "(\"1005\", \"Smartphone\", 700)]\n",
    "columns = [\"OrderID\", \"Product\", \"Price\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Number of Rows: \", df.count())\n",
    "print(\"Number of Columns: \", len(df.columns))\n",
    "df.printSchema()"
   ],
   "id": "5d0043c611dd569f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows:  5\n",
      "Number of Columns:  3\n",
      "root\n",
      " |-- OrderID: string (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Price: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 320
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "31. How to rename a specific columns in a dataframe?",
   "id": "666868cdfc51ad97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T14:25:08.692973Z",
     "start_time": "2025-11-13T14:25:08.384731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = spark.createDataFrame([('Alice', 1, 30),('Bob', 2, 35)], [\"name\", \"age\", \"qty\"])\n",
    "old_names = [\"qty\", \"age\"]\n",
    "new_names = [\"user_qty\", \"user_age\"]\n",
    "\n",
    "\n",
    "for old_name, new_name in zip(old_names, new_names):\n",
    "    df = df.withColumnRenamed(old_name, new_name)\n",
    "df.show()"
   ],
   "id": "71c9d654c0f711dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+\n",
      "| name|user_age|user_qty|\n",
      "+-----+--------+--------+\n",
      "|Alice|       1|      30|\n",
      "|  Bob|       2|      35|\n",
      "+-----+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 319
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "32. How to check if a dataframe has any missing values and count of missing values in each column?",
   "id": "965e900bd4be11a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T14:33:14.113531Z",
     "start_time": "2025-11-13T14:33:13.344058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "\n",
    "missing_counts = df.select([sum(when(col(c).isNull() , 1).otherwise(0)).alias(c) for c in df.columns])\n",
    "missing_counts.show()"
   ],
   "id": "47af31241390149f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+\n",
      "|Name|Value| id|\n",
      "+----+-----+---+\n",
      "|   0|    2|  2|\n",
      "+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 321
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "33 How to replace missing values of multiple numeric columns with the mean?",
   "id": "ed8196b26aa36587"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T14:51:58.584885Z",
     "start_time": "2025-11-13T14:51:58.258481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import  LongType\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, 123 ),\n",
    "(\"B\", 3, 456),\n",
    "(\"D\", 6, None),\n",
    "], [\"Name\", \"var1\", \"var2\"])\n",
    "\n",
    "\n",
    "column_names = [field.name for field in df.schema.fields if isinstance(field.dataType, LongType)]\n",
    "imputer = Imputer(inputCols= column_names, outputCols= column_names, strategy=\"mean\")\n",
    "model = imputer.fit(df)\n",
    "imputed_df = model.transform(df)\n",
    "imputed_df.show(5)"
   ],
   "id": "99beb7a709c27131",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|Name|var1|var2|\n",
      "+----+----+----+\n",
      "|   A|   1| 289|\n",
      "|   B|   3| 123|\n",
      "|   B|   3| 456|\n",
      "|   D|   6| 289|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 329
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "34. How to change the order of columns of a dataframe?",
   "id": "4c4b7072e48c9e0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T14:53:18.731605Z",
     "start_time": "2025-11-13T14:53:18.471446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = [(\"John\", \"Doe\", 30), (\"Jane\", \"Doe\", 25), (\"Alice\", \"Smith\", 22)]\n",
    "df = spark.createDataFrame(data, [\"First_Name\", \"Last_Name\", \"Age\"])\n",
    "df.show()\n",
    "\n",
    "\n",
    "df = df.select(\"Age\", \"First_Name\", \"Last_Name\")\n",
    "df.show()"
   ],
   "id": "8d96c5bd0be919cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+\n",
      "|First_Name|Last_Name|Age|\n",
      "+----------+---------+---+\n",
      "|      John|      Doe| 30|\n",
      "|      Jane|      Doe| 25|\n",
      "|     Alice|    Smith| 22|\n",
      "+----------+---------+---+\n",
      "\n",
      "+---+----------+---------+\n",
      "|Age|First_Name|Last_Name|\n",
      "+---+----------+---------+\n",
      "| 30|      John|      Doe|\n",
      "| 25|      Jane|      Doe|\n",
      "| 22|     Alice|    Smith|\n",
      "+---+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 331
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "35. How to format or suppress scientific notations in a PySpark DataFrame?\n",
   "id": "1ca2185f3503e3eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T14:54:37.278247Z",
     "start_time": "2025-11-13T14:54:37.051101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "df = spark.createDataFrame([(1, 0.000000123), (2, 0.000023456), (3, 0.000345678)], [\"id\", \"your_column\"])\n",
    "\n",
    "\n",
    "df = df.withColumn(\"your_column\", format_number(\"your_column\", 10))\n",
    "df.show()"
   ],
   "id": "59769202cb9aa51d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id| your_column|\n",
      "+---+------------+\n",
      "|  1|0.0000001230|\n",
      "|  2|0.0000234560|\n",
      "|  3|0.0003456780|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 333
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "36. How to format all the values in a dataframe as percentages?",
   "id": "4f78e4dce8ec52a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T14:56:35.236600Z",
     "start_time": "2025-11-13T14:56:34.907101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "columns = [\"numbers_1\", \"numbers_2\"]\n",
    "data = [(0.1, .08), (0.2, .06), (0.33, .02)]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "columns = [\"numbers_1\", \"numbers_2\"]\n",
    "for col_name in columns:\n",
    "    df = df.withColumn(col_name, concat((col(col_name) * 100).cast('decimal(10, 2)'), lit(\"%\")))\n",
    "\n",
    "df.show()"
   ],
   "id": "b00e5c53ee93396c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|numbers_1|numbers_2|\n",
      "+---------+---------+\n",
      "|   10.00%|    8.00%|\n",
      "|   20.00%|    6.00%|\n",
      "|   33.00%|    2.00%|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 334
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "37. How to filter every nth row in a dataframe?\n",
   "id": "77957c453cb7f4be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T14:59:52.482224Z",
     "start_time": "2025-11-13T14:59:52.049398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3), (\"Dave\", 4), (\"Eve\", 5),\n",
    "(\"Frank\", 6), (\"Grace\", 7), (\"Hannah\", 8), (\"Igor\", 9), (\"Jack\", 10)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Number\"])\n",
    "\n",
    "\n",
    "window = Window.orderBy(monotonically_increasing_id())\n",
    "df = df.withColumn(\"rn\", row_number().over(window))\n",
    "n = 5\n",
    "df = df.filter((df.rn % n) == 0)\n",
    "df.show()"
   ],
   "id": "2629d5727475207",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/13 16:59:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 16:59:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 16:59:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 16:59:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 16:59:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+---+\n",
      "|Name|Number| rn|\n",
      "+----+------+---+\n",
      "| Eve|     5|  5|\n",
      "|Jack|    10| 10|\n",
      "+----+------+---+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 335
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "38 How to get the row number of the nth largest value in a column?\n",
   "id": "db8b4a84aa0fbfe1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:08:29.964403Z",
     "start_time": "2025-11-13T15:08:29.735622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, row_number\n",
    "\n",
    "data = [\n",
    "Row(id=1, column1=5),\n",
    "Row(id=2, column1=8),\n",
    "Row(id=3, column1=12),\n",
    "Row(id=4, column1=1),\n",
    "Row(id=5, column1=15),\n",
    "Row(id=6, column1=7),\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "\n",
    "win = Window.orderBy(desc(\"column1\"))\n",
    "df = df.withColumn(\"row_number\", row_number().over(win))\n",
    "\n",
    "n = 3\n",
    "row = df.filter(df.row_number == n)\n",
    "\n",
    "row.show()"
   ],
   "id": "7ae29ff37cc210f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+\n",
      "| id|column1|row_number|\n",
      "+---+-------+----------+\n",
      "|  2|      8|         3|\n",
      "+---+-------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/13 17:08:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 17:08:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 17:08:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "execution_count": 344
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "39. How to get the last n rows of a dataframe with row sum > 100?\n",
   "id": "3600533fd3afe84f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:21:00.850671Z",
     "start_time": "2025-11-13T15:21:00.523147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from functools import reduce\n",
    "\n",
    "data = [(10, 25, 70),\n",
    "(40, 5, 20),\n",
    "(70, 80, 100),\n",
    "(10, 2, 60),\n",
    "(40, 50, 20)]\n",
    "df = spark.createDataFrame(data, [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "\n",
    "df = df.withColumn('row_sum', reduce(lambda a, b: a+b, [col(x) for x in df.columns]))\n",
    "df = df.filter(col('row_sum') > 100)\n",
    "df = df.withColumn('id', monotonically_increasing_id())\n",
    "df_last_2 = df.sort(desc('id')).limit(2)\n",
    "df_last_2.show()"
   ],
   "id": "fe5b0764476c89fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-------+-----------+\n",
      "|col1|col2|col3|row_sum|         id|\n",
      "+----+----+----+-------+-----------+\n",
      "|  40|  50|  20|    110|60129542144|\n",
      "|  70|  80| 100|    250|34359738368|\n",
      "+----+----+----+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 349
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "40. How to create a column containing the minimum by maximum of each row?\n",
   "id": "11ba4d8eca246bc1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:22:48.704751Z",
     "start_time": "2025-11-13T15:22:48.421456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import udf, array\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "data = [(1, 2, 3), (4, 5, 6), (7, 8, 9), (10, 11, 12)]\n",
    "df = spark.createDataFrame(data, [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "\n",
    "def min_max_ratio(row):\n",
    "    return float(min(row)) / max(row)\n",
    "\n",
    "min_max_ratio_udf = udf(min_max_ratio, FloatType())\n",
    "df = df.withColumn('min_by_max', min_max_ratio_udf(array(df.columns)))\n",
    "df.show()"
   ],
   "id": "18e21d1e8a3bf656",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----------+\n",
      "|col1|col2|col3|min_by_max|\n",
      "+----+----+----+----------+\n",
      "|   1|   2|   3|0.33333334|\n",
      "|   4|   5|   6| 0.6666667|\n",
      "|   7|   8|   9| 0.7777778|\n",
      "|  10|  11|  12| 0.8333333|\n",
      "+----+----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 354
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "41. How to create a column that contains the penultimate value in each row?\n",
   "id": "28e6d3464fb0305"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:24:43.637379Z",
     "start_time": "2025-11-13T15:24:43.195722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "\n",
    "data = [(10, 20, 30),\n",
    "(40, 60, 50),\n",
    "(80, 70, 90)]\n",
    "df = spark.createDataFrame(data, [\"Column1\", \"Column2\", \"Column3\"])\n",
    "\n",
    "\n",
    "sort_array_desc = udf(lambda arr: sorted(arr), ArrayType(IntegerType()))\n",
    "\n",
    "df = df.withColumn(\"row_as_array\", sort_array_desc(array(df.columns)))\n",
    "df = df.withColumn(\"Penultimate\", df['row_as_array'].getItem(1))\n",
    "df = df.drop('row_as_array')\n",
    "df.show()"
   ],
   "id": "663107def17e209b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-----------+\n",
      "|Column1|Column2|Column3|Penultimate|\n",
      "+-------+-------+-------+-----------+\n",
      "|     10|     20|     30|         20|\n",
      "|     40|     60|     50|         50|\n",
      "|     80|     70|     90|         80|\n",
      "+-------+-------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 355
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "42. How to normalize all columns in a dataframe?\n",
   "id": "52bd5cd75708fe33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:27:07.785191Z",
     "start_time": "2025-11-13T15:27:06.516877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "input_cols = [\"Col1\", \"Col2\", \"Col3\"]\n",
    "data = [(1, 2, 3),\n",
    "(2, 3, 4),\n",
    "(3, 4, 5),\n",
    "(4, 5, 6)]\n",
    "df = spark.createDataFrame(data, input_cols)\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "df_assembled = assembler.transform(df)\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "scalerModel = scaler.fit(df_assembled)\n",
    "df_normalized = scalerModel.transform(df_assembled)\n",
    "df_normalized = df_normalized.drop('features')\n",
    "df_normalized.show(truncate=False)"
   ],
   "id": "b54ef02c0ced7320",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-------------------------------------------------------------+\n",
      "|Col1|Col2|Col3|scaled_features                                              |\n",
      "+----+----+----+-------------------------------------------------------------+\n",
      "|1   |2   |3   |[-1.161895003862225,-1.161895003862225,-1.161895003862225]   |\n",
      "|2   |3   |4   |[-0.3872983346207417,-0.3872983346207417,-0.3872983346207417]|\n",
      "|3   |4   |5   |[0.3872983346207417,0.3872983346207417,0.3872983346207417]   |\n",
      "|4   |5   |6   |[1.161895003862225,1.161895003862225,1.161895003862225]      |\n",
      "+----+----+----+-------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 356
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "43. How to get the positions where values of two columns match?\n",
   "id": "87d54a357d2d6f12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:28:27.773051Z",
     "start_time": "2025-11-13T15:28:27.419538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = [(\"John\", \"John\"), (\"Lily\", \"Lucy\"), (\"Sam\", \"Sam\"), (\"Lucy\", \"Lily\")]\n",
    "df = spark.createDataFrame(data, [\"Name1\", \"Name2\"])\n",
    "\n",
    "\n",
    "df = df.withColumn(\"Match\", when(col(\"Name1\") == col(\"Name2\"), True).otherwise(False))\n",
    "df.show()"
   ],
   "id": "a0e60fb1c9e4f1ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|Name1|Name2|Match|\n",
      "+-----+-----+-----+\n",
      "| John| John| true|\n",
      "| Lily| Lucy|false|\n",
      "|  Sam|  Sam| true|\n",
      "| Lucy| Lily|false|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 357
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "44. How to create lags and leads of a column by group in a dataframe?\n",
   "id": "2a8e042e2be2d15"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:30:12.915523Z",
     "start_time": "2025-11-13T15:30:12.430761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import lead\n",
    "\n",
    "data = [(\"2023-01-01\", \"Store1\", 100),\n",
    "(\"2023-01-02\", \"Store1\", 150),\n",
    "(\"2023-01-03\", \"Store1\", 200),\n",
    "(\"2023-01-04\", \"Store1\", 250),\n",
    "(\"2023-01-05\", \"Store1\", 300),\n",
    "(\"2023-01-01\", \"Store2\", 50),\n",
    "(\"2023-01-02\", \"Store2\", 60),\n",
    "(\"2023-01-03\", \"Store2\", 80),\n",
    "(\"2023-01-04\", \"Store2\", 90),\n",
    "(\"2023-01-05\", \"Store2\", 120)]\n",
    "df = spark.createDataFrame(data, [\"Date\", \"Store\", \"Sales\"])\n",
    "\n",
    "\n",
    "df = df.withColumn(\"Date\", to_date(df.Date, 'yyyy-MM-dd'))\n",
    "windowSpec = Window.partitionBy(\"Store\").orderBy(\"Date\")\n",
    "\n",
    "df = df.withColumn(\"Lag_Sales\", lag(df[\"Sales\"]).over(windowSpec))\n",
    "df = df.withColumn(\"Lead_Sales\", lead(df[\"Sales\"]).over(windowSpec))\n",
    "\n",
    "df.show()"
   ],
   "id": "c3f0723c9be8cf4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+---------+----------+\n",
      "|      Date| Store|Sales|Lag_Sales|Lead_Sales|\n",
      "+----------+------+-----+---------+----------+\n",
      "|2023-01-01|Store1|  100|     NULL|       150|\n",
      "|2023-01-02|Store1|  150|      100|       200|\n",
      "|2023-01-03|Store1|  200|      150|       250|\n",
      "|2023-01-04|Store1|  250|      200|       300|\n",
      "|2023-01-05|Store1|  300|      250|      NULL|\n",
      "|2023-01-01|Store2|   50|     NULL|        60|\n",
      "|2023-01-02|Store2|   60|       50|        80|\n",
      "|2023-01-03|Store2|   80|       60|        90|\n",
      "|2023-01-04|Store2|   90|       80|       120|\n",
      "|2023-01-05|Store2|  120|       90|      NULL|\n",
      "+----------+------+-----+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 358
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "45. How to get the frequency of unique values in the entire dataframe?\n",
   "id": "47e43137d4f8bdd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:35:09.521238Z",
     "start_time": "2025-11-13T15:35:08.661549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "columns = [\"Column1\", \"Column2\", \"Column3\"]\n",
    "data = [(1, 2, 3),\n",
    "(2, 3, 4),\n",
    "(1, 2, 3),\n",
    "(4, 5, 6),\n",
    "(2, 3, 4)]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "df_single = None\n",
    "for c in columns:\n",
    "    if df_single is None:\n",
    "        df_single = df.select(col(c).alias(\"single_column\"))\n",
    "    else:\n",
    "        df_single = df_single.union(df.select(col(c).alias(\"single_column\")))\n",
    "frequency_table = df_single.groupBy(\"single_column\").count().orderBy('count', ascending=False)\n",
    "frequency_table.show()"
   ],
   "id": "dd4a7efe3380a842",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|single_column|count|\n",
      "+-------------+-----+\n",
      "|            2|    4|\n",
      "|            3|    4|\n",
      "|            4|    3|\n",
      "|            1|    2|\n",
      "|            5|    1|\n",
      "|            6|    1|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 359
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "46. How to replace both the diagonals of dataframe with 0?\n",
   "id": "2a16f10069906d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:41:56.784119Z",
     "start_time": "2025-11-13T15:41:55.488295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = [(1, 2, 3, 4),\n",
    "(2, 3, 4, 5),\n",
    "(1, 2, 3, 4),\n",
    "(4, 5, 6, 7)]\n",
    "df = spark.createDataFrame(data, [\"col_1\", \"col_2\", \"col_3\", \"col_4\"])\n",
    "\n",
    "\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "df = df.withColumn(\"id\", row_number().over(w) - 1)\n",
    "df = df.select([when(col(\"id\") == i, 0).otherwise(col(\"col_\"+str(i+1))).alias(\"col_\"+str(i+1)) for i in range(4)])\n",
    "\n",
    "# Create a reverse id column\n",
    "df = df.withColumn(\"id\", row_number().over(w) - 1)\n",
    "df = df.withColumn(\"id_2\", df.count() - 1 - df[\"id\"])\n",
    "df_with_diag_zero = df.select([when(col(\"id_2\") == i, 0).otherwise(col(\"col_\"+str(i+1))).alias(\"col_\"+str(i+1)) for i in range(4)])\n",
    "df_with_diag_zero.show()"
   ],
   "id": "e411bb8c224669c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+\n",
      "|col_1|col_2|col_3|col_4|\n",
      "+-----+-----+-----+-----+\n",
      "|    0|    2|    3|    0|\n",
      "|    2|    0|    0|    5|\n",
      "|    1|    0|    0|    4|\n",
      "|    0|    5|    6|    0|\n",
      "+-----+-----+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/13 17:41:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 17:41:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 17:41:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 17:41:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 17:41:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 17:41:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 17:41:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 17:41:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 17:41:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "execution_count": 360
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "47. How to reverse the rows of a dataframe?\n",
   "id": "948d20b9b1c766dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:42:56.703345Z",
     "start_time": "2025-11-13T15:42:56.414912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = [(1, 2, 3, 4),\n",
    "(2, 3, 4, 5),\n",
    "(3, 4, 5, 6),\n",
    "(4, 5, 6, 7)]\n",
    "df = spark.createDataFrame(data, [\"col_1\", \"col_2\", \"col_3\", \"col_4\"])\n",
    "\n",
    "\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "df = df.withColumn(\"id\", row_number().over(w) - 1)\n",
    "df_2 = df.orderBy(\"id\", ascending=False).drop(\"id\")\n",
    "df_2.show()"
   ],
   "id": "4a720c6e842edf68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+\n",
      "|col_1|col_2|col_3|col_4|\n",
      "+-----+-----+-----+-----+\n",
      "|    4|    5|    6|    7|\n",
      "|    3|    4|    5|    6|\n",
      "|    2|    3|    4|    5|\n",
      "|    1|    2|    3|    4|\n",
      "+-----+-----+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/13 17:42:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 17:42:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 17:42:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 17:42:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/13 17:42:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "execution_count": 361
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "48. How to create one-hot encodings of a categorical variable (dummy variables)?\n",
   "id": "961b6cce4ba8f828"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:45:48.414778Z",
     "start_time": "2025-11-13T15:45:47.486090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "data = [(\"A\", 10),(\"A\", 20),(\"B\", 30),(\"B\", 20),(\"B\", 30),(\"C\", 40),(\"C\", 10),(\"D\", 10)]\n",
    "columns = [\"Categories\", \"Value\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Categories\", outputCol=\"Categories_Indexed\")\n",
    "indexerModel = indexer.fit(df)\n",
    "indexed_df = indexerModel.transform(df)\n",
    "encoder = OneHotEncoder(inputCol=\"Categories_Indexed\", outputCol=\"Categories_onehot\")\n",
    "encoded_df = encoder.fit(indexed_df).transform(indexed_df)\n",
    "encoded_df = encoded_df.drop(\"Categories_Indexed\")\n",
    "encoded_df.show(truncate=False)\n"
   ],
   "id": "eea0e07242e47b72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------------+\n",
      "|Categories|Value|Categories_onehot|\n",
      "+----------+-----+-----------------+\n",
      "|A         |10   |(3,[1],[1.0])    |\n",
      "|A         |20   |(3,[1],[1.0])    |\n",
      "|B         |30   |(3,[0],[1.0])    |\n",
      "|B         |20   |(3,[0],[1.0])    |\n",
      "|B         |30   |(3,[0],[1.0])    |\n",
      "|C         |40   |(3,[2],[1.0])    |\n",
      "|C         |10   |(3,[2],[1.0])    |\n",
      "|D         |10   |(3,[],[])        |\n",
      "+----------+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 362
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "50. How to UnPivot the dataframe (converting columns into rows) ?\n",
   "id": "c387d7ea3f384b61"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:48:00.910637Z",
     "start_time": "2025-11-13T15:48:00.457329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = [(2021, 2, 4500, 5500),\n",
    "(2021, 1, 4000, 5000),\n",
    "(2021, 3, 5000, 6000),\n",
    "(2021, 4, 6000, 7000)]\n",
    "columns = [\"year\", \"quarter\", \"EU\", \"US\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "unpivotExpr = \"stack(2, 'EU',EU, 'US', US) as (region,revenue)\"\n",
    "unPivotDF = pivot_df.select(\"year\",\"quarter\", expr(unpivotExpr)).where(\"revenue is not null\")\n",
    "unPivotDF.show()"
   ],
   "id": "f0b4552c44b2ca76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+-------+\n",
      "|year|quarter|region|revenue|\n",
      "+----+-------+------+-------+\n",
      "|2021|      2|    EU|   4500|\n",
      "|2021|      2|    US|   5500|\n",
      "|2021|      1|    EU|   4000|\n",
      "|2021|      1|    US|   5000|\n",
      "|2021|      3|    EU|   5000|\n",
      "|2021|      3|    US|   6000|\n",
      "|2021|      4|    EU|   6000|\n",
      "|2021|      4|    US|   7000|\n",
      "+----+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 364
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
