{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    body, * {\n",
    "        direction: rtl !important;\n",
    "        text-align: right !important;\n",
    "    }\n",
    "</style>\n",
    "# ğŸ”— PySpark - concat_ws() - ×”××¨×ª ××¢×¨×š ×œ××—×¨×•×–×ª\n",
    "[PySpark â€“ Convert array column to a String](https://sparkbyexamples.com/pyspark/pyspark-convert-array-column-to-string-column/)\n",
    "## ×ª×•×›×Ÿ ×¢× ×™×™× ×™×\n",
    "1. [×”×§×“××”](#introduction)\n",
    "2. [×ª×—×‘×™×¨ concat_ws()](#syntax)\n",
    "3. [×”××¨×ª ×¢××•×“×ª ××¢×¨×š ×œ××—×¨×•×–×ª](#convert-array-to-string)\n",
    "4. [×©×™××•×© ×¢× PySpark SQL Expression](#with-sql-expr)\n",
    "5. [×“×•×’××” ××œ××”](#complete-example)\n",
    "6. [××§×•×¨×•×ª](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ×”×§×“××” <a id='introduction'></a>\n",
    "\n",
    "**PySpark SQL** ××¡×¤×§×ª ×¤×•× ×§×¦×™×” ××•×‘× ×™×ª `concat_ws()` ×©×ª×¨×’××ª ×œ×× ×’×œ×™×ª ×”×™× \"concatenate with separator\".\n",
    "\n",
    "×”×¤×•× ×§×¦×™×” ×œ×•×§×—×ª delimiter ×œ×¤×™ ×‘×—×™×¨×” ×©×œ×š ×›××¨×’×•×× ×˜ ×¨××©×•×Ÿ ×•×¢××•×“×ª ××¢×¨×š (Array Column) ×›××¨×’×•×× ×˜ ×©× ×™, ×•××—×–×™×¨×” ×¢××•×“×” ×—×“×©×” ××¡×•×’ string.\n",
    "\n",
    "### ××ª×™ ×œ×”×©×ª××©:\n",
    "×›××©×¨ ××ª×” ×××¨×’×Ÿ × ×ª×•× ×™× ×‘-DataFrame ×•×¨×•×¦×” ×œ×”××™×¨ ××ª ×”-DataFrame ×¢× ××‘× ×™ × ×ª×•× ×™× ××•×¨×›×‘×™× (structs, datatypes, arrays ×•-maps) ×œ××‘× ×” ×©×˜×•×—."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ×ª×—×‘×™×¨ concat_ws() <a id='syntax'></a>\n",
    "\n",
    "```python\n",
    "concat_ws(sep, *cols)\n",
    "```\n",
    "\n",
    "### ×¤×¨××˜×¨×™×:\n",
    "\n",
    "- **sep**: delimiter ×œ×‘×—×™×¨×ª×š (×¡×˜×¨×™× ×’)\n",
    "- **cols**: ×¢××•×“×ª ××¢×¨×š ××• ××¡×¤×¨ ×¢××•×“×•×ª (×˜×™×¤×•×¡ Column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ ×™×™×‘×•× ×¡×¤×¨×™×•×ª ×•×”×›× ×ª × ×ª×•× ×™×\n",
    "\n",
    "×¨××©×™×ª, × ×™×¦×•×¨ DataFrame ×¢× ×¢××•×“×ª ××¢×¨×š ×©××›×™×œ×” ×©××•×ª ×©×¤×•×ª ×ª×›× ×•×ª:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()\n",
    "\n",
    "# × ×ª×•× ×™× ×¢× ××¢×¨×›×™×\n",
    "columns = [\"name\", \"languagesAtSchool\", \"currentState\"]\n",
    "data = [\n",
    "    (\"James\", [\"Java\", \"Scala\", \"C++\"], \"CA\"),\n",
    "    (\"Michael\", [\"Spark\", \"Java\", \"C++\"], \"NJ\"),\n",
    "    (\"Robert\", [\"CSharp\", \"VB\"], \"NV\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“‹ ×”×¡×‘×¨ ×”× ×ª×•× ×™×:\n",
    "\n",
    "×‘×“×•×’××” ×–×•, ×¢××•×“×ª \"languagesAtSchool\" ×”×™× ××¡×•×’ array ×©××›×™×œ×” ×¨×©×™××ª ×©××•×ª ×©×¤×•×ª ×ª×›× ×•×ª."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ ×”××¨×ª ×¢××•×“×ª ××¢×¨×š ×œ××—×¨×•×–×ª <a id='convert-array-to-string'></a>\n",
    "\n",
    "×¢×œ ×× ×ª ×œ×”×©×ª××© ×‘×¤×•× ×§×¦×™×” `concat_ws()`, ×ª×—×™×œ×” ×¦×¨×™×š ×œ×™×™×‘× ××•×ª×” ×‘×××¦×¢×•×ª:\n",
    "\n",
    "```python\n",
    "pyspark.sql.functions.concat_ws\n",
    "```\n",
    "\n",
    "××›×™×•×•×Ÿ ×©×¤×•× ×§×¦×™×” ×–×• ×œ×•×§×—×ª ××ª Column type ×›××¨×’×•×× ×˜ ×©× ×™, ×¦×¨×™×š ×œ×”×©×ª××© ×‘-`col()`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import col, concat_ws\n",
    "\n",
    "# ×”××¨×ª ××¢×¨×š ×œ××—×¨×•×–×ª ×¢× ×¤×¡×™×§\n",
    "df2 = df.withColumn(\n",
    "    \"languagesAtSchool\",\n",
    "    concat_ws(\",\", col(\"languagesAtSchool\"))\n",
    ")\n",
    "\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š ×”×¡×‘×¨ ×”×¤×œ×˜:\n",
    "\n",
    "×”×¤×œ×˜ ××¦×™×’ ××ª ×¢××•×“×ª languagesAtSchool ×‘×ª×•×¨ string ××•×¤×¨×“×ª ×‘×¤×¡×™×§×™×."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ×©×™××•×© ×¢× PySpark SQL Expression <a id='with-sql-expr'></a>\n",
    "\n",
    "××¤×©×¨ ×’× ×œ×”×©×ª××© ×‘×¤×•× ×§×¦×™×” `concat_ws()` ×¢× SQL expression:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×¦×™×¨×ª Temporary View\n",
    "df.createOrReplaceTempView(\"ARRAY_STRING\")\n",
    "\n",
    "# ×©×™××•×© ×‘-SQL\n",
    "spark.sql(\n",
    "    \"select name, concat_ws(',', languagesAtSchool) as languagesAtSchool,\" +\n",
    "    \"currentState from ARRAY_STRING\"\n",
    ").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ×“×•×’××” ××œ××” <a id='complete-example'></a>\n",
    "\n",
    "×”× ×” ×“×•×’××” ××§×™×¤×” ×”××©×œ×‘×ª ××ª ×›×œ ××” ×©×œ××“× ×•:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()\n",
    "\n",
    "columns = [\"name\", \"languagesAtSchool\", \"currentState\"]\n",
    "data = [\n",
    "    (\"James\", [\"Java\", \"Scala\", \"C++\"], \"CA\"),\n",
    "    (\"Michael\", [\"Spark\", \"Java\", \"C++\"], \"NJ\"),\n",
    "    (\"Robert\", [\"CSharp\", \"VB\"], \"NV\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "# ×”××¨×ª ××¢×¨×š ×œ××—×¨×•×–×ª\n",
    "from pyspark.sql.functions import col, concat_ws\n",
    "df2 = df.withColumn(\n",
    "    \"languagesAtSchool\",\n",
    "    concat_ws(\",\", col(\"languagesAtSchool\"))\n",
    ")\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "# ×©×™××•×© ×¢× SQL Expression\n",
    "df.createOrReplaceTempView(\"ARRAY_STRING\")\n",
    "spark.sql(\n",
    "    \"select name, concat_ws(',', languagesAtSchool) as languagesAtSchool,\" +\n",
    "    \"currentState from ARRAY_STRING\"\n",
    ").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ¨ ×¡×™×›×•×\n",
    "\n",
    "×”×¤×•× ×§×¦×™×” `concat_ws()` ×‘-PySpark ××©××©×ª ×œ×”××¨×ª ×¢××•×“×ª ××¢×¨×š ×©×œ String ×œ-String column (××•×¤×¨×“×ª ××• ××©×•×¨×©×¨×ª ×¢× separator).\n",
    "\n",
    "**× ×§×•×“×•×ª ××¤×ª×—:**\n",
    "- âœ… `concat_ws()` - ×©×¨×©×•×¨ ×¢××•×“×•×ª/××¢×¨×›×™× ×¢× separator\n",
    "- âœ… ×”×©×™××•×© ×¢× `withColumn()` ×œ×™×¦×™×¨×ª ×¢××•×“×” ×—×“×©×”\n",
    "- âœ… ×”×©×™××•×© ×¢× `col()` ×›×“×™ ×œ×”×¤× ×•×ª ×œ×¢××•×“×”\n",
    "- âœ… ×ª××™×›×” ×‘-SQL expressions ×“×¨×š `createOrReplaceTempView()`\n",
    "\n",
    "**××ª×™ ×œ×”×©×ª××©:**\n",
    "- ×”××¨×ª ××¢×¨×›×™× ×œ××—×¨×•×–×•×ª ××•×¤×¨×“×•×ª\n",
    "- ×¤×™×©×•×˜ ××‘× ×™ × ×ª×•× ×™× ××•×¨×›×‘×™×\n",
    "- ×™×¦×™×¨×ª ××—×¨×•×–×•×ª ××¢×•×¦×‘×•×ª ××¢×¨×›×™×\n",
    "- ×”×›× ×ª × ×ª×•× ×™× ×œ×™×™×¦×•× ×œ×§×‘×¦×™ CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ××§×•×¨×•×ª <a id='references'></a>\n",
    "\n",
    "- [PySpark SQL Functions Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)\n",
    "- [SparkByExamples - PySpark Convert Array to String](https://sparkbyexamples.com/pyspark/pyspark-convert-array-column-to-string-column/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
