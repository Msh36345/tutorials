{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    body, * {\n",
    "        direction: rtl !important;\n",
    "        text-align: right !important;\n",
    "    }\n",
    "</style>\n",
    "# ğŸ“Š PySpark: ×”×—×œ×¤×ª ×¢×¨×›×™ ×¢××•×“×•×ª ×‘-DataFrame\n",
    "[PySpark Replace Column Values in DataFrame](https://sparkbyexamples.com/pyspark/pyspark-replace-column-values/)\n",
    "## ×ª×•×›×Ÿ ×¢× ×™×™× ×™×\n",
    "1. [×”×—×œ×¤×ª ×¢×¨×›×™ ×¢××•×“×ª ××—×¨×•×–×•×ª](#section1)\n",
    "2. [×”×—×œ×¤×ª ×¢×¨×›×™× ×‘×ª× ××™](#section2)\n",
    "3. [×”×—×œ×¤×ª ×¢×¨×›×™× ×‘×××¦×¢×•×ª ××™×œ×•×Ÿ (Dictionary)](#section3)\n",
    "4. [×”×—×œ×¤×ª ×ª×• ×‘×ª×•](#section4)\n",
    "5. [×”×—×œ×¤×ª ×¢××•×“×” ×‘×¢×¨×š ××¢××•×“×” ××—×¨×ª](#section5)\n",
    "6. [×”×—×œ×¤×ª ×›×œ ×”×¢××•×“×•×ª ××• ××¡×¤×¨ ×¢××•×“×•×ª](#section6)\n",
    "7. [×©×™××•×© ×‘×¤×•× ×§×¦×™×” overlay()](#section7)\n",
    "8. [×“×•×’××” ××œ××”](#section8)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ××‘×•×\n",
    "\n",
    "×‘××“×¨×™×š ×–×” × ×œ××“ ×›×™×¦×“ ×œ×”×—×œ×™×£ ×¢×¨×›×™× ×‘×¢××•×“×•×ª ×©×œ PySpark DataFrame ×‘×××¦×¢×•×ª ×¤×•× ×§×¦×™×•×ª SQL ×›××•:\n",
    "- `regexp_replace()` - ×”×—×œ×¤×” ×‘×××¦×¢×•×ª ×‘×™×˜×•×™×™× ×¨×’×•×œ×¨×™×™×\n",
    "- `translate()` - ×”×—×œ×¤×ª ×ª×•×•×™× ×‘×•×“×“×™×\n",
    "- `overlay()` - ×”×—×œ×¤×ª ×—×œ×§ ×××—×¨×•×–×ª\n",
    "\n",
    "× ×›×¡×” ×“×•×’×××•×ª ××¢×©×™×•×ª ×œ×”×—×œ×¤×ª ×—×œ×§ ×××—×¨×•×–×ª, ×”×—×œ×¤×ª ×›×œ ×”×¢××•×“×•×ª, ×©×™× ×•×™ ×¢×¨×›×™× ×‘×ª× ××™, ×”×—×œ×¤×ª ×¢×¨×›×™× ×××™×œ×•×Ÿ Python, ×•×”×—×œ×¤×ª ×¢×¨×š ×¢××•×“×” ××¢××•×“×” ××—×¨×ª ×‘-DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”§ ×”×›× ×ª ×”×¡×‘×™×‘×” ×•×˜×¢×™× ×ª × ×ª×•× ×™×\n",
    "\n",
    "×¨××©×™×ª, × ×™×™×¦×¨ DataFrame ×œ×“×•×’××” ×¢× ×›×ª×•×‘×•×ª ×©× ×©×ª××© ×‘×”×Ÿ ×œ×›×œ ×”×“×•×’×××•×ª ×‘××“×¨×™×š."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×™×‘×•× ×”×¡×¤×¨×™×•×ª ×”× ×“×¨×©×•×ª\n",
    "# Create sample Data\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate()\n",
    "\n",
    "address = [\n",
    "    (1, \"14851 Jeffrey Rd\", \"DE\"),\n",
    "    (2, \"43421 Margarita St\", \"NY\"),\n",
    "    (3, \"13111 Siemon Ave\", \"CA\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(address, [\"id\", \"address\", \"state\"])\n",
    "df.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section1\"></a>\n",
    "## 1ï¸âƒ£ ×”×—×œ×¤×ª ×¢×¨×›×™ ×¢××•×“×ª ××—×¨×•×–×•×ª (Replace String Column Values)\n",
    "\n",
    "### ğŸ“ ×”×¡×‘×¨\n",
    "\n",
    "×‘×××¦×¢×•×ª ×”×¤×•× ×§×¦×™×” `regexp_replace()` ×©×œ PySpark SQL × ×™×ª×Ÿ ×œ×”×—×œ×™×£ ×¢×¨×š ×¢××•×“×” ×‘××—×¨×•×–×ª ××—×¨×ª ××• ×‘×ª×ª-××—×¨×•×–×ª.\n",
    "\n",
    "×”×¤×•× ×§×¦×™×” `regexp_replace()` ××©×ª××©×ª ×‘-**Java regex** ×œ×”×ª×××”. ×× ×”×‘×™×˜×•×™ ×”×¨×’×•×œ×¨×™ ×œ× ×ª×•××, ×”×¤×•× ×§×¦×™×” ××—×–×™×¨×” ××—×¨×•×–×ª ×¨×§×”.\n",
    "\n",
    "×‘×“×•×’××” ×”×‘××”, × ×—×œ×™×£ ××ª ×©× ×”×¨×—×•×‘ `Rd` ×‘××™×œ×” `Road` ×‘×¢××•×“×ª `address`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”×—×œ×¤×ª ×—×œ×§ ×××—×¨×•×–×ª ×‘××—×¨×•×–×ª ××—×¨×ª\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "df.withColumn(\n",
    "    'address', \n",
    "    regexp_replace(df.address, 'Rd', 'Road')\n",
    ").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section2\"></a>\n",
    "## 2ï¸âƒ£ ×”×—×œ×¤×ª ×¢×¨×›×™ ×¢××•×“×” ×‘×ª× ××™ (Replace Column Values Conditionally)\n",
    "\n",
    "### ğŸ“ ×”×¡×‘×¨\n",
    "\n",
    "×‘×“×•×’××” ×”×§×•×“××ª ×”×—×œ×¤× ×• ×¨×§ ××ª `Rd` ×‘-`Road`, ××š ×œ× ×”×—×œ×¤× ×• ××ª `St` ×•-`Ave`. \n",
    "\n",
    "×›×¢×ª × ×¨××” ×›×™×¦×“ ×œ×”×—×œ×™×£ ×¢×¨×›×™ ×¢××•×“×” **×‘×ª× ××™** ×‘-PySpark DataFrame ×‘×××¦×¢×•×ª ×¤×•× ×§×¦×™×•×ª:\n",
    "- `when()` - ×ª× ××™ IF\n",
    "- `otherwise()` - ×ª× ××™ ELSE\n",
    "- SQL condition functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”×—×œ×¤×ª ×¢×¨×›×™ ×¢××•×“×ª ××—×¨×•×–×ª ×‘×ª× ××™\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df.withColumn(\n",
    "    'address',\n",
    "    when(df.address.endswith('Rd'), regexp_replace(df.address, 'Rd', 'Road'))\n",
    "    .when(df.address.endswith('St'), regexp_replace(df.address, 'St', 'Street'))\n",
    "    .when(df.address.endswith('Ave'), regexp_replace(df.address, 'Ave', 'Avenue'))\n",
    "    .otherwise(df.address)\n",
    ").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section3\"></a>\n",
    "## 3ï¸âƒ£ ×”×—×œ×¤×ª ×¢×¨×›×™ ×¢××•×“×” ×¢× ××™×œ×•×Ÿ - Dictionary (map)\n",
    "\n",
    "### ğŸ“ ×”×¡×‘×¨\n",
    "\n",
    "× ×™×ª×Ÿ ×’× ×œ×”×—×œ×™×£ ×¢×¨×›×™ ×¢××•×“×” ×-**Python dictionary (map)**.\n",
    "\n",
    "×‘×“×•×’××” ×”×‘××”, × ×—×œ×™×£ ××ª ×¢×¨×š ×”××—×¨×•×–×ª ×‘×¢××•×“×ª `state` ×¢× ×”×©× ×”××œ× ×”××§×•×¦×¨ ××”××™×œ×•×Ÿ (key-value pair).\n",
    "\n",
    "×›×“×™ ×œ×¢×©×•×ª ×–××ª, × ×©×ª××© ×‘-**PySpark map() transformation ×›×“×™ ×œ×¢×‘×•×¨ ×¢×œ ×›×œ ×©×•×¨×”** ×‘-DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”×—×œ×¤×ª ×¢×¨×›×™× ×××™×œ×•×Ÿ (Dictionary)\n",
    "stateDic = {\n",
    "    'CA': 'California',\n",
    "    'NY': 'New York',\n",
    "    'DE': 'Delaware'\n",
    "}\n",
    "\n",
    "df2 = df.rdd.map(\n",
    "    lambda x: (x.id, x.address, stateDic[x.state])\n",
    ").toDF([\"id\", \"address\", \"state\"])\n",
    "\n",
    "df2.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section4\"></a>\n",
    "## 4ï¸âƒ£ ×”×—×œ×¤×ª ×ª×• ×‘×ª×• (Replace Column Value Character by Character)\n",
    "\n",
    "### ğŸ“ ×”×¡×‘×¨\n",
    "\n",
    "×‘×××¦×¢×•×ª ×”×¤×•× ×§×¦×™×” `translate()` × ×™×ª×Ÿ **×œ×”×—×œ×™×£ ×ª×• ×‘×ª×•** ×‘×¢×¨×š ×¢××•×“×ª DataFrame.\n",
    "\n",
    "×‘×“×•×’××” ×”×‘××”, ×›×œ ×ª×• `1` ××•×—×œ×£ ×‘-`A`, `2` ××•×—×œ×£ ×‘-`B`, ×•-`3` ××•×—×œ×£ ×‘-`C` ×‘×¢××•×“×ª `address`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×©×™××•×© ×‘-translate ×œ×”×—×œ×¤×ª ×ª×•×•×™× ×‘×•×“×“×™×\n",
    "from pyspark.sql.functions import translate\n",
    "\n",
    "df.withColumn(\n",
    "    'address', \n",
    "    translate(df.address, '123', 'ABC')\n",
    ").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section5\"></a>\n",
    "## 5ï¸âƒ£ ×”×—×œ×¤×ª ×¢××•×“×” ×‘×¢×¨×š ××¢××•×“×” ××—×¨×ª (Replace Column with Another Column Value)\n",
    "\n",
    "### ğŸ“ ×”×¡×‘×¨\n",
    "\n",
    "×‘×××¦×¢×•×ª `expr()` ×•-`regexp_replace()` × ×™×ª×Ÿ **×œ×”×—×œ×™×£ ×¢×¨×š ×¢××•×“×” ×‘×¢×¨×š ××¢××•×“×ª DataFrame ××—×¨×ª**.\n",
    "\n",
    "×‘×“×•×’××” ×”×‘××”, × ×ª××™× ××ª ×”×¢×¨×š ××¢××•×“×” `col2` ×‘×¢××•×“×” `col1` ×•× ×—×œ×™×£ ××•×ª×• ×‘-`col3` ×›×“×™ ×œ×™×¦×•×¨ ×¢××•×“×” ×—×“×©×” `new_column`.\n",
    "\n",
    "××©×ª××©×™× ×‘-`expr()` ×›×“×™ ×œ×¡×¤×§ ×‘×™×˜×•×™×™× ×“××•×™×™ SQL, ×•×”×•× ××©××© ×œ×”×ª×™×™×—×¡×•×ª ×œ×¢××•×“×” ××—×¨×ª ×‘×‘×™×¦×•×¢ ×¤×¢×•×œ×•×ª:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”×—×œ×¤×ª ×¢××•×“×” ×‘×¢×¨×š ××¢××•×“×” ××—×¨×ª\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [('ABCDE_XYZ', 'XYZ', 'FGH'),('111_222_333', '222', '888')],\n",
    "    ['col1', 'col2', 'col3']\n",
    ")\n",
    "df.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.withColumn(\n",
    "    'new_column',\n",
    "    expr(\"regexp_replace(col1, col2, col3)\")\n",
    ").alias(\"replaced_value\").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section6\"></a>\n",
    "## 6ï¸âƒ£ ×”×—×œ×¤×ª ×›×œ ×”×¢××•×“×•×ª ××• ××¡×¤×¨ ×¢××•×“×•×ª (Replace All or Multiple Column Values)\n",
    "\n",
    "### ğŸ“ ×”×¡×‘×¨\n",
    "\n",
    "×× ×‘×¨×¦×•× ×š ×œ×”×—×œ×™×£ ×¢×¨×›×™× ×‘×›×œ ×”×¢××•×“×•×ª ××• ×‘××¡×¤×¨ ×¢××•×“×•×ª × ×‘×—×¨×•×ª ×‘-DataFrame, ×¢×™×™×Ÿ ×‘××“×¨×™×›×™× ×”×‘××™×:\n",
    "\n",
    "- ğŸ“˜ [×›×™×¦×“ ×œ×”×—×œ×™×£ NULL/None values ×‘×›×œ ×”×¢××•×“×•×ª ×‘-PySpark](https://sparkbyexamples.com/pyspark/pyspark-fillna-fill-replace-null-values/)\n",
    "- ğŸ“˜ [×›×™×¦×“ ×œ×”×—×œ×™×£ ××—×¨×•×–×ª ×¨×™×§×” ×¢× NULL/None value](https://sparkbyexamples.com/pyspark/pyspark-replace-empty-value-with-none/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section7\"></a>\n",
    "## 7ï¸âƒ£ ×©×™××•×© ×‘×¤×•× ×§×¦×™×” overlay()\n",
    "\n",
    "### ğŸ“ ×”×¡×‘×¨\n",
    "\n",
    "×”×¤×•× ×§×¦×™×” `overlay()` ×××¤×©×¨×ª **×œ×”×—×œ×™×£ ×¢×¨×š ×¢××•×“×ª ××—×¨×•×–×ª ×‘×¢×¨×š ××—×¨×•×–×ª ××¢××•×“×” ××—×¨×ª**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×“×•×’××” ×œ×©×™××•×© ×‘-overlay\n",
    "from pyspark.sql.functions import overlay\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [('ABCDE_XYZ', 'FGH'),('111_222_333', '888')],\n",
    "    ['col1', 'col2']\n",
    ")\n",
    "df.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = df.withColumn('overlayed',\n",
    "    overlay('col1', 'col2', 7)\n",
    ")\n",
    "df.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section8\"></a>\n",
    "## 8ï¸âƒ£ ×“×•×’××” ××œ××” - ×›×œ ×”×§×•×“ ×‘×™×—×“\n",
    "\n",
    "### ğŸ“ ×”×¡×‘×¨\n",
    "\n",
    "×œ×”×œ×Ÿ ×“×•×’××” ××œ××” ×”×›×•×œ×œ×ª ××ª ×›×œ ×”×©×™×˜×•×ª ×©×œ××“× ×• ×‘××“×¨×™×š ×–×”:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×§×•×“ ××œ× - ×›×œ ×”×“×•×’×××•×ª ×‘×™×—×“\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate()\n",
    "\n",
    "address = [\n",
    "    (1, \"14851 Jeffrey Rd\", \"DE\"),\n",
    "    (2, \"43421 Margarita St\", \"NY\"),\n",
    "    (3, \"13111 Siemon Ave\", \"CA\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(address, [\"id\", \"address\", \"state\"])\n",
    "df.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”×—×œ×¤×ª ××—×¨×•×–×ª ×¤×©×•×˜×”\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "df.withColumn(\n",
    "    'address', \n",
    "    regexp_replace(df.address, 'Rd', 'Road')\n",
    ").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”×—×œ×¤×ª ××—×¨×•×–×ª ×‘×ª× ××™\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df.withColumn(\n",
    "    'address',\n",
    "    when(df.address.endswith('Rd'), regexp_replace(df.address, 'Rd', 'Road'))\n",
    "    .when(df.address.endswith('St'), regexp_replace(df.address, 'St', 'Street'))\n",
    "    .when(df.address.endswith('Ave'), regexp_replace(df.address, 'Ave', 'Avenue'))\n",
    "    .otherwise(df.address)\n",
    ").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”×—×œ×¤×ª ×¢×¨×›×™× ×××™×œ×•×Ÿ (Dictionary)\n",
    "stateDic = {\n",
    "    'CA': 'California',\n",
    "    'NY': 'New York',\n",
    "    'DE': 'Delaware'\n",
    "}\n",
    "\n",
    "df2 = df.rdd.map(\n",
    "    lambda x: (x.id, x.address, stateDic[x.state])\n",
    ").toDF([\"id\", \"address\", \"state\"])\n",
    "\n",
    "df2.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×©×™××•×© ×‘-translate\n",
    "from pyspark.sql.functions import translate\n",
    "\n",
    "df.withColumn(\n",
    "    'address', \n",
    "    translate(df.address, '123', 'ABC')\n",
    ").show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”×—×œ×¤×ª ×¢××•×“×” ×‘×¢×¨×š ××¢××•×“×” ××—×¨×ª\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [('ABCDE_XYZ', 'XYZ', 'FGH')],\n",
    "    ['col1', 'col2', 'col3']\n",
    ")\n",
    "\n",
    "df.withColumn(\n",
    "    'new_column',\n",
    "    expr(\"regexp_replace(col1, col2, col3)\")\n",
    ").alias(\"replaced_value\").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Overlay\n",
    "from pyspark.sql.functions import overlay\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [('ABCDE_XYZ', 'FGH')],\n",
    "    ['col1', 'col2']\n",
    ")\n",
    "\n",
    "df.select(\n",
    "    overlay('col1', 'col2', 7).alias('overlayed')\n",
    ").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ×¡×™×›×•×\n",
    "\n",
    "×‘××“×¨×™×š ×–×” ×œ××“× ×•:\n",
    "\n",
    "âœ… ×”×¤×•× ×§×¦×™×” `regexp_replace()` ××©××©×ª ×œ×”×—×œ×¤×ª ××—×¨×•×–×ª ×‘×¢××•×“×ª DataFrame ×¢× ×¢×¨×š ××—×¨\n",
    "\n",
    "âœ… ×”×¤×•× ×§×¦×™×” `translate()` ××©××©×ª ×œ×”×—×œ×¤×ª ×ª×• ×‘×ª×• ×©×œ ×¢×¨×›×™ ×¢××•×“×•×ª\n",
    "\n",
    "âœ… ×”×¤×•× ×§×¦×™×” `overlay()` ××©××©×ª ×œ×©×›×ª×•×‘ ××—×¨×•×–×ª ×¢× ××—×¨×•×–×ª ×¢××•×“×” ××—×¨×ª ×××™×§×•× ×”×ª×—×œ×” ×•××¡×¤×¨ ×ª×•×•×™×\n",
    "\n",
    "âœ… ×‘× ×•×¡×£, ×œ××“× ×• ×›×™×¦×“ ×œ×”×—×œ×™×£ ×¢×¨×›×™ ×¢××•×“×•×ª ×××™×œ×•×Ÿ (dictionary) ×‘×××¦×¢×•×ª ×“×•×’×××•×ª Python\n",
    "\n",
    "**ğŸ‰ ×œ××™×“×” ××”× ×”!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š ××§×•×¨×•×ª ×•××××¨×™× × ×•×¡×¤×™×\n",
    "\n",
    "### ××××¨×™× ×§×©×•×¨×™×:\n",
    "- [PySpark repartition() â€“ Explained with Examples](https://sparkbyexamples.com/pyspark/pyspark-repartition/)\n",
    "- [PySpark Replace Empty Value With None/null on DataFrame](https://sparkbyexamples.com/pyspark/pyspark-replace-empty-value-with-none/)\n",
    "- [PySpark createOrReplaceTempView() Explained](https://sparkbyexamples.com/pyspark/pyspark-create-or-replace-temp-view/)\n",
    "- [PySpark fillna() & fill() â€“ Replace NULL/None Values](https://sparkbyexamples.com/pyspark/pyspark-fillna-fill-replace-null-values/)\n",
    "- [PySpark repartition() vs partitionBy()](https://sparkbyexamples.com/pyspark/pyspark-repartition-vs-partitionby/)\n",
    "- [PySpark Repartition() vs Coalesce()](https://sparkbyexamples.com/pyspark/pyspark-repartition-vs-coalesce/)\n",
    "- [PySpark Replace Empty Value With None/null on DataFrame](https://sparkbyexamples.com/pyspark/pyspark-replace-empty-value-with-none/)\n",
    "- [Pyspark Select Distinct Rows](https://sparkbyexamples.com/pyspark/pyspark-distinct-to-drop-duplicates/)\n",
    "- [PySpark Get Number of Rows and Columns](https://sparkbyexamples.com/pyspark/pyspark-get-number-of-rows-and-columns/)\n",
    "\n",
    "### ××§×•×¨×•×ª ×—×™×¦×•× ×™×™×:\n",
    "- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)\n",
    "- [PySpark SQL Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html)\n",
    "- [Handling Null/Empty Strings in PySpark](https://kb.databricks.com/data/null-empty-strings.html)\n",
    "\n",
    "---\n",
    "\n",
    "**Â© SparkByExamples.com - All rights reserved**\n",
    "\n",
    "*××“×¨×™×š ×–×” ×ª×•×¨×’× ×•×¢×•×‘×“ ×œ×¢×‘×¨×™×ª ×œ××˜×¨×•×ª ×œ×™××•×“*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
