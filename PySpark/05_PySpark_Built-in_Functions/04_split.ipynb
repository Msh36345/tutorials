{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    body, * {\n",
    "        direction: rtl !important;\n",
    "        text-align: right !important;\n",
    "    }\n",
    "</style>\n",
    "# âœ‚ï¸ PySpark - split() - ×¤×™×¦×•×œ ××—×¨×•×–×•×ª ×œ××¢×¨×›×™×\n",
    "[PySpark Convert String to Array Column](https://sparkbyexamples.com/pyspark/pyspark-convert-string-to-array-column/)\n",
    "## ×ª×•×›×Ÿ ×¢× ×™×™× ×™×\n",
    "1. [×”×§×“××”](#introduction)\n",
    "2. [×ª×—×‘×™×¨ split()](#syntax)\n",
    "3. [×”××¨×ª ×¢××•×“×ª ××—×¨×•×–×ª ×œ××¢×¨×š](#convert-string-to-array)\n",
    "4. [×©×™××•×© ×‘-split() ×¢× withColumn()](#with-column)\n",
    "5. [×©×™××•×© ×‘-split() ×¢× SQL Query](#with-sql)\n",
    "6. [×“×•×’××” ××œ××”](#complete-example)\n",
    "7. [××§×•×¨×•×ª](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ×”×§×“××” <a id='introduction'></a>\n",
    "\n",
    "**PySpark SQL** ××¡×¤×§×ª ×¤×•× ×§×¦×™×” `split()` ××ª×•×š ××•×“×•×œ `pyspark.sql.functions`. ×¤×•× ×§×¦×™×” ×–×• ××©××©×ª ×œ×”××¨×ª ×¢××•×“×ª ××—×¨×•×–×ª ×œ-×¢××•×“×ª ××¢×¨×š.\n",
    "\n",
    "×”×¤×•× ×§×¦×™×” ××¤×¦×œ×ª ××—×¨×•×–×ª ×¢×œ ×‘×¡×™×¡ ×ª×•×•×™ ×”×¤×¨×“×” (delimiter) ×›××• ×¤×¡×™×§, ×¨×•×•×—, ×¦×™× ×•×¨ ×•×›×•', ×•××—×–×™×¨×” ××•×‘×™×™×§×˜ ××¡×•×’ `pyspark.sql.Column` ××¡×•×’ Array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ×ª×—×‘×™×¨ split() <a id='syntax'></a>\n",
    "\n",
    "```python\n",
    "pyspark.sql.functions.split(str, pattern, limit=-1)\n",
    "```\n",
    "\n",
    "### ×¤×¨××˜×¨×™×:\n",
    "\n",
    "- **str**: ×©× ×”×¢××•×“×” ×”××›×™×œ×” ××ª ×”××—×¨×•×–×ª ×©××× ×” ××ª×” ×¨×•×¦×” ×œ×—×œ×¥ ×ª×ª-××—×¨×•×–×ª\n",
    "- **pattern**: ×ª×‘× ×™×ª ×”×¤×¨×“×” (delimiter). ×× ×œ× ××¦×•×™× ×ª, ×”×¤×•× ×§×¦×™×” ×ª×—×œ×¥ ××¢××“×ª ×”×¤×™×¦×•×œ ×¢×“ ×¡×•×£ ×”××—×¨×•×–×ª\n",
    "- **limit** (××•×¤×¦×™×•× ×œ×™): ××¡×¤×¨ ×”××œ×× ×˜×™× ×”××§×¡×™××œ×™ ×œ×”×—×–×¨×”. ×× ×œ× ××¦×•×™×Ÿ, ×”×¤×•× ×§×¦×™×” ×ª×—×–×™×¨ ××ª ×›×œ ×”××œ×× ×˜×™×"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ ×™×™×‘×•× ×¡×¤×¨×™×•×ª ×•×”×›× ×ª × ×ª×•× ×™×\n",
    "\n",
    "×¨××©×™×ª, × ×™×™×‘× ××ª ×”×¡×¤×¨×™×•×ª ×”× ×“×¨×©×•×ª ×•× ×™×¦×•×¨ DataFrame ×œ×“×•×’××” ×¢× ×¢××•×“×ª ×©××•×ª ××•×¤×¨×“×ª ×‘×¤×¡×™×§×™×:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ×™×¦×™×¨×ª Spark Session\n",
    "spark = SparkSession.builder.appName(\"stringoperations\").getOrCreate()\n",
    "\n",
    "# ×™×¦×™×¨×ª DataFrame ×¢× × ×ª×•× ×™ ×©××•×ª\n",
    "data = [\n",
    "    (\"James, A, Smith\", \"2018\", \"M\", 3000),\n",
    "    (\"Michael, Rose, Jones\", \"2010\", \"M\", 4000),\n",
    "    (\"Robert, K, Williams\", \"2010\", \"M\", 4000),\n",
    "    (\"Maria, Anne, Jones\", \"2005\", \"F\", 4000),\n",
    "    (\"Jen, Mary, Brown\", \"2010\", \"\", -1)\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"dob_year\", \"gender\", \"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ ×”××¨×ª ×¢××•×“×ª ××—×¨×•×–×ª ×œ××¢×¨×š <a id='convert-string-to-array'></a>\n",
    "\n",
    "× ×™×™×‘× ××ª `split` ×•-`col` ×•× ×©×ª××© ×‘×¤×•× ×§×¦×™×” `split()` ×¢×œ ×¢××•×“×ª ×”-name ×›×“×™ ×œ×¤×¦×œ ××•×ª×” ×œ××¢×¨×š ×¢×œ ×‘×¡×™×¡ ×¤×¡×™×§×™×:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "# ×¤×™×¦×•×œ ×¢××•×“×ª 'name' ×œ××¢×¨×š\n",
    "df2 = df.select(\n",
    "    split(col(\"name\"), \",\").alias(\"NameArray\")\n",
    ").alias(\"NameArray\")\n",
    "\n",
    "df2.printSchema()\n",
    "df2.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“‹ ×”×¡×‘×¨ ×”×¤×œ×˜:\n",
    "\n",
    "×›×¤×™ ×©××ª×” ×¨×•××” ×‘×¤×œ×˜ ×œ××¢×œ×”, ×¢××•×“×ª NameArray ×”×™× ××¡×•×’ array ×”××›×™×œ×” ××œ×× ×˜×™× ××¡×•×’ string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¨ ×©×™××•×© ×‘-split() ×¢× withColumn() <a id='with-column'></a>\n",
    "\n",
    "××¤×©×¨ ×œ×”×©×ª××© ×‘×¤×•× ×§×¦×™×” `split()` ×‘×ª×•×š ××ª×•×“×ª `withColumn()` ×›×“×™ ×œ×™×¦×•×¨ ×¢××•×“×” ×—×“×©×” ×¢× ××¢×¨×š ×¢×œ ×”-DataFrame.\n",
    "\n",
    "×× ××™× ×š ×–×§×•×§ ×œ×¢××•×“×” ×”××§×•×¨×™×ª, ×”×©×ª××© ×‘-`drop()` ×›×“×™ ×œ×”×¡×™×¨ ××•×ª×”:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×¤×™×¦×•×œ ×¢××•×“×ª 'name' ×œ×©×œ×•×©×” ×—×œ×§×™×: ×©× ×¤×¨×˜×™, ×©× ×××¦×¢, ×©× ××©×¤×—×”\n",
    "df = df.withColumn(\"name_array\", split(df[\"name\"], \",\"))\n",
    "\n",
    "# ×”×¦×’×ª DataFrame ×”××¢×•×“×›×Ÿ\n",
    "df.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š ×©×™××•×© ×‘-split() ×¢× SQL Query <a id='with-sql'></a>\n",
    "\n",
    "×œ×—×œ×•×¤×™×Ÿ, ××ª×” ×™×›×•×œ ×œ×›×ª×•×‘ ××ª ××•×ª×• ×”×“×•×’××” ×‘×××¦×¢×•×ª ×©××™×œ×ª×ª SQL.\n",
    "\n",
    "×¨××©×™×ª, ×¦×•×¨ Temporary View ×‘×××¦×¢×•×ª `createOrReplaceTempView()` ×•-`spark.sql()` ×›×“×™ ×œ×”×¨×™×¥ ××ª ×©××™×œ×ª×ª ×”-SQL:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×¦×™×¨×ª Temporary View\n",
    "df.createOrReplaceTempView(\"PERSON\")\n",
    "\n",
    "# ×”×¨×¦×ª SQL Query\n",
    "spark.sql(\"select SPLIT(name, ',') as NameArray from PERSON\").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ×“×•×’××” ××œ××” <a id='complete-example'></a>\n",
    "\n",
    "×”× ×” ×“×•×’××” ××§×™×¤×” ×”××©×œ×‘×ª ××ª ×›×œ ××” ×©×œ××“× ×•:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, substring\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"James, A, Smith\", \"2018\", \"M\", 3000),\n",
    "    (\"Michael, Rose, Jones\", \"2010\", \"M\", 4000),\n",
    "    (\"Robert, K, Williams\", \"2010\", \"M\", 4000),\n",
    "    (\"Maria, Anne, Jones\", \"2005\", \"F\", 4000),\n",
    "    (\"Jen, Mary, Brown\", \"2010\", \"\", -1)\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"dob_year\", \"gender\", \"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "# ×©×™××•×© ×‘-split() ×¢× select\n",
    "from pyspark.sql.functions import split, col\n",
    "df2 = df.select(split(col(\"name\"), \",\").alias(\"NameArray\")).alias(\"NameArray\")\n",
    "df2.printSchema()\n",
    "df2.show()\n",
    "\n",
    "# ×©×™××•×© ×‘-split() ×¢× withColumn\n",
    "df2 = df.select(split(col(\"name\"), \",\").alias(\"NameArray\"))\n",
    "df2.printSchema()\n",
    "df2.show()\n",
    "\n",
    "# ×©×™××•×© ×¢× SQL\n",
    "df.createOrReplaceTempView(\"PERSON\")\n",
    "spark.sql(\"select SPLIT(name, ',') as NameArray from PERSON\").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ¨ ×¡×™×›×•×\n",
    "\n",
    "×‘×©×™×¢×•×¨ ×–×” ×œ××“× ×• ×“×¨×›×™× ×©×•× ×•×ª ×œ×¤×™×¦×•×œ ×¢××•×“×ª ××—×¨×•×–×ª ×‘-PySpark DataFrame:\n",
    "\n",
    "**× ×§×•×“×•×ª ××¤×ª×—:**\n",
    "- âœ… `split()` - ×¤×™×¦×•×œ ××—×¨×•×–×ª ×œ××¢×¨×š ×¢×œ ×‘×¡×™×¡ delimiter\n",
    "- âœ… ×©×™××•×© ×¢× `select()` ×œ×™×¦×™×¨×ª DataFrame ×—×“×©\n",
    "- âœ… ×©×™××•×© ×¢× `withColumn()` ×œ×”×•×¡×¤×ª ×¢××•×“×” ×œ××¢×¨×š ×§×™×™×\n",
    "- âœ… ×©×™××•×© ×¢× SQL query ×•-`createOrReplaceTempView()`\n",
    "\n",
    "**××ª×™ ×œ×”×©×ª××©:**\n",
    "- ×›××©×¨ ×™×© ×œ×š ××—×¨×•×–×•×ª ××•×¤×¨×“×•×ª ×‘×¤×¡×™×§×™×, ×¨×•×•×—×™× ××• ×ª×•×•×™ ×”×¤×¨×“×” ××—×¨×™×\n",
    "- ×›××©×¨ ××ª×” ×¨×•×¦×” ×œ×”××™×¨ ×¢××•×“×” ×œ××¢×¨×š ×œ×¢×™×‘×•×“ × ×•×¡×£\n",
    "- ×›××©×¨ ××ª×” ×¦×¨×™×š ×œ×¤×¦×œ ×©××•×ª ××œ××™× ×œ×—×œ×§×™×”×\n",
    "\n",
    "×”×’×™×©×” ×”×–×• ×©×™××•×©×™×ª ×‘××™×•×—×“ ×œ×˜×¨× ×¡×¤×•×¨××¦×™×” ×©×œ ××—×¨×•×–×•×ª ××•×¤×¨×“×•×ª ×‘××‘× ×” ××¢×¨×š ×œ××˜×¨×•×ª ×¢×™×‘×•×“ × ×•×¡×¤×•×ª!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ××§×•×¨×•×ª <a id='references'></a>\n",
    "\n",
    "- [PySpark SQL Functions Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)\n",
    "- [SparkByExamples - PySpark split() Column into Multiple Columns](https://sparkbyexamples.com/pyspark/pyspark-convert-string-to-array-column/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
