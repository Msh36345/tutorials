{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    body, * {\n",
    "        direction: rtl !important;\n",
    "        text-align: right !important;\n",
    "    }\n",
    "</style>\n",
    "# ğŸ“˜ PySpark - expr() - ×‘×™×˜×•×™×™× ×“××•×™×™ SQL\n",
    "[PySpark SQL expr() (Expression) Function](https://sparkbyexamples.com/pyspark/pyspark-sql-expr-expression-function/)\n",
    "## ×ª×•×›×Ÿ ×¢× ×™×™× ×™×\n",
    "1. [×”×§×“××”](#introduction)\n",
    "2. [×ª×—×‘×™×¨ expr()](#syntax)\n",
    "3. [×©×¨×©×•×¨ ×¢××•×“×•×ª ×¢× || (concat)](#concat)\n",
    "4. [CASE WHEN ×¢× expr()](#case-when)\n",
    "5. [×©×™××•×© ×‘×¢×¨×š ×§×™×™× ××¢××•×“×”](#column-value)\n",
    "6. [××ª×Ÿ ×›×™× ×•×™ (alias) ×œ×¢××•×“×”](#alias)\n",
    "7. [×”××¨×ª ×˜×™×¤×•×¡×™× (cast)](#cast)\n",
    "8. [×¤×¢×•×œ×•×ª ×—×©×‘×•×Ÿ](#arithmetic)\n",
    "9. [×¡×™× ×•×Ÿ × ×ª×•× ×™× (filter)](#filter)\n",
    "10. [×“×•×’××” ××œ××”](#complete-example)\n",
    "11. [××§×•×¨×•×ª](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ×”×§×“××” <a id='introduction'></a>\n",
    "\n",
    "**PySpark expr()** ×”×™× ×¤×•× ×§×¦×™×™×ª SQL ×”×××¤×©×¨×ª ×œ×”×¨×™×¥ ×‘×™×˜×•×™×™× ×“××•×™×™ SQL ×•×œ×”×©×ª××© ×‘×¢×¨×š ×§×™×™× ××¢××•×“×ª DataFrame ×›××¨×’×•×× ×˜ ×œ×‘×™×˜×•×™.\n",
    "\n",
    "×¨×•×‘ ×”×¤×•× ×§×¦×™×•×ª ×”× ×¤×•×¦×•×ª ×‘-SQL ×”×Ÿ ×—×œ×§ ××”××—×œ×§×” `pyspark.sql.Column` ××• ××”-API ×©×œ `pyspark.sql.functions`. ×‘× ×•×¡×£ ×œ××œ×•, PySpark ×ª×•××›×ª ×’× ×‘×¤×•× ×§×¦×™×•×ª SQL ×¨×‘×•×ª × ×•×¡×¤×•×ª, ×•×œ×›×Ÿ ×›×“×™ ×œ×”×©×ª××© ×‘×”×Ÿ ×¦×¨×™×š ×œ×”×©×ª××© ×‘×¤×•× ×§×¦×™×” `expr()`.\n",
    "\n",
    "### ×©× ×™ ××§×¨×™ ×©×™××•×© ×¢×™×§×¨×™×™×:\n",
    "1. **×©×™××•×© ×‘×¤×•× ×§×¦×™×•×ª ×©××™× ×Ÿ ×§×™×™××•×ª ×‘-`PySpark Column`** (×œ××©×œ: `CASE WHEN`, `regexp_count()`)\n",
    "2. **×”×¨×—×‘×ª ×”×¤×•× ×§×¦×™×•× ×œ×™×•×ª ×©×œ PySpark SQL** ×¢×œ ×™×“×™ ××¤×©×¨×•×ª ×œ×”×©×ª××© ×‘×¢××•×“×•×ª DataFrame ×‘×ª×•×š ×‘×™×˜×•×™×™×"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ ×™×™×‘×•× ×¡×¤×¨×™×•×ª ×•×”×›× ×ª × ×ª×•× ×™×\n",
    "\n",
    "×¨××©×™×ª, × ×™×™×‘× ××ª ×”×¡×¤×¨×™×•×ª ×”× ×“×¨×©×•×ª ×•× ×™×¦×•×¨ DataFrame ×œ×“×•×’××”:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()\n",
    "\n",
    "# ×™×¦×™×¨×ª DataFrame ×¢× × ×ª×•× ×™ ×©××•×ª\n",
    "data = [\n",
    "    (\"James\", \"Bond\"),\n",
    "    (\"Scott\", \"Varsa\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=[\"col1\", \"col2\"])\n",
    "df.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ 1. ×ª×—×‘×™×¨ expr() <a id='syntax'></a>\n",
    "\n",
    "```python\n",
    "expr(str)\n",
    "```\n",
    "\n",
    "×”×¤×•× ×§×¦×™×” `expr()` ××§×‘×œ×ª ×‘×™×˜×•×™ SQL ×›××¨×’×•×× ×˜ ××¡×•×’ ××—×¨×•×–×ª, ××‘×¦×¢×ª ××ª ×”×‘×™×˜×•×™, ×•××—×–×™×¨×” ××•×‘×™×™×§×˜ ××¡×•×’ `pyspark.sql.Column`.\n",
    "\n",
    "×‘×™×˜×•×™×™× ×©×¡×•×¤×§×• ×¢× ×”×¤×•× ×§×¦×™×” ×”×–×• ××™× × ××”×•×•×™× ×‘×˜×™×—×•×ª compile-time ×›××• ×¤×¢×•×œ×•×ª DataFrame ×¨×’×™×œ×•×ª."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”— 2. ×©×¨×©×•×¨ ×¢××•×“×•×ª ×¢× || (×“×•××” ×œ-SQL) <a id='concat'></a>\n",
    "\n",
    "×× ×™×© ×œ×š ×¨×§×¢ ×‘-SQL, ××ª×” ×‘×•×•×“××™ ××›×™×¨ ××ª ×”×©×™××•×© ×‘-`||` ×œ×©×¨×©×•×¨ ×¢×¨×›×™× ××©×ª×™ ×¢××•×“×•×ª ××—×¨×•×–×ª. ××¤×©×¨ ×œ×¢×©×•×ª ×‘×“×™×•×§ ××ª ××•×ª×• ×”×“×‘×¨ ×¢× `expr()`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# ×©×¨×©×•×¨ ×©××•×ª\n",
    "data = [(\"James\", \"Bond\"), (\"Scott\", \"Varsa\")]\n",
    "df = spark.createDataFrame(data=data, schema=[\"col1\", \"col2\"])\n",
    "\n",
    "df.withColumn(\"Name\", expr(\"col1 || ',' || col2\")).show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš–ï¸ 3. ×©×™××•×© ×‘-CASE WHEN ×¢× expr() <a id='case-when'></a>\n",
    "\n",
    "**PySpark ×œ× ×ª×•××š ×‘-CASE WHEN ×‘×¦×•×¨×” ×™×©×™×¨×”** ×“×¨×š `withColumn()` ××• `select()`. ×¢×œ ×× ×ª ×œ×”×©×ª××© ×‘×‘×™×˜×•×™ CASE WHEN, ×¦×¨×™×š ×œ×”×©×ª××© ×‘-`expr()`.\n",
    "\n",
    "**×—×œ×•×¤×”:** ××¤×©×¨ ×’× ×œ×”×©×ª××© ×‘-`when().otherwise()` ×‘××§×•×."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×¦×™×¨×ª DataFrame ×¢× × ×ª×•× ×™ ××’×“×¨\n",
    "data = [(\"James\", \"M\"), (\"Michael\", \"F\"), (\"Jen\", \"\")]\n",
    "columns = [\"name\", \"gender\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "# ×©×™××•×© ×‘-CASE WHEN ×›×“×™ ×œ×”×—×œ×™×£ ×¢×¨×›×™×\n",
    "df2 = df.withColumn(\n",
    "    \"gender\",\n",
    "    expr(\n",
    "        \"CASE WHEN gender = 'M' THEN 'Male' \" +\n",
    "        \"WHEN gender = 'F' THEN 'Female' \" +\n",
    "        \"WHEN gender IS NULL THEN '' \" +\n",
    "        \"ELSE gender END\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df2.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ 4. ×©×™××•×© ×‘×¢×¨×š ×§×™×™× ××¢××•×“×” ××—×¨×ª <a id='column-value'></a>\n",
    "\n",
    "×¨×•×‘ ×”×¤×•× ×§×¦×™×•×ª ×œ×•×§×—×•×ª ×¢×¨×›×™ ×§×‘×•×¢×™× (literal), ××‘×œ ×œ×¤×¢××™× ×× ×—× ×• ×¦×¨×™×›×™× ×œ×”×©×ª××© ×‘×¢×¨×š ××¢××•×“×” ×§×™×™××ª ×‘××§×•× ×§×‘×•×¢. ×”× ×” ×“×•×’××” ×©××•×¡×™×¤×” ×¢×¨×š ×—×•×“×©×™× ××¢××•×“×” ×§×™×™××ª:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”×•×¡×¤×ª ×—×•×“×©×™× ××¢××•×“×”\n",
    "data = [\n",
    "    (\"2019-01-23\", 1),\n",
    "    (\"2019-06-24\", 2),\n",
    "    (\"2019-09-20\", 3)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=[\"date\", \"increment\"])\n",
    "\n",
    "df.select(\n",
    "    df.date,\n",
    "    df.increment,\n",
    "    expr(\"add_months(date, increment) as inc_date\")\n",
    ").alias(\"inc_date\").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**×”×¢×¨×”:** ×™×™×‘×•× ×¤×•× ×§×¦×™×•×ª SQL ×œ× × ×“×¨×© ×›××©×¨ ××©×ª××©×™× ×‘×”×Ÿ ×¢× `expr()`. ×¨×•××™× ×œ××¢×œ×” ×©-`add_months()` × ×¢×©×” ×‘×œ×™ ×œ×™×™×‘×."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ·ï¸ 5. ××ª×Ÿ ×›×™× ×•×™ (alias) ×œ×¢××•×“×” ×‘×××¦×¢×•×ª 'as' <a id='alias'></a>\n",
    "\n",
    "××¤×©×¨ ×œ×¡×¤×§ ×›×™× ×•×™ ×œ×¢××•×“×” ×‘×××¦×¢×•×ª ×”××™×œ×” 'as' ×‘×ª×•×š ×”×‘×™×˜×•×™ ×©×œ `expr()`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ××ª×Ÿ ×›×™× ×•×™ ×¢× 'as'\n",
    "df.select(\n",
    "    df.date,\n",
    "    df.increment,\n",
    "    expr(\"\"\"add_months(date, increment) as inc_date\"\"\")\n",
    ").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ 6. ×”××¨×ª ×˜×™×¤×•×¡×™× ×¢× cast() <a id='cast'></a>\n",
    "\n",
    "×”×“×•×’××” ×”×‘××” ×××™×¨×” ×˜×™×¤×•×¡ long ×œ×˜×™×¤×•×¡ String:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×”××¨×ª ×˜×™×¤×•×¡\n",
    "df.select(\n",
    "    \"increment\",\n",
    "    expr(\"cast(increment as string) as str_increment\")\n",
    ").printSchema()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â• 7. ×¤×¢×•×œ×•×ª ×—×©×‘×•×Ÿ <a id='arithmetic'></a>\n",
    "\n",
    "`expr()` ××©××© ×’× ×œ×‘×™×¦×•×¢ ×¤×¢×•×œ×•×ª ×—×©×‘×•×Ÿ. ×”×“×•×’××” ×”×‘××” ××•×¡×™×¤×” 5 ×œ×¢×¨×š `increment` ×•×™×•×¦×¨×ª ×¢××•×“×” ×—×“×©×” `new_increment`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×¤×¢×•×œ×•×ª ×—×©×‘×•×Ÿ\n",
    "df.select(\n",
    "    df.date,\n",
    "    df.increment,\n",
    "    expr(\"increment + 5 as new_increment\")\n",
    ").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” 8. ×¡×™× ×•×Ÿ ×©×•×¨×•×ª ×¢× filter() <a id='filter'></a>\n",
    "\n",
    "××¤×©×¨ ×œ×”×©×ª××© ×‘-`expr()` ×’× ×œ×¡×™× ×•×Ÿ ×©×•×¨×•×ª ×©×œ DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×¡×™× ×•×Ÿ × ×ª×•× ×™×\n",
    "data = [(100, 2), (200, 3000), (500, 500)]\n",
    "df = spark.createDataFrame(data=data, schema=[\"col1\", \"col2\"])\n",
    "\n",
    "df.filter(expr(\"col1 == col2\")).show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ 9. ×“×•×’××” ××œ××” <a id='complete-example'></a>\n",
    "\n",
    "×”× ×” ×“×•×’××” ××§×™×¤×” ×”××©×œ×‘×ª ××ª ×›×œ ××” ×©×œ××“× ×•:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples.com\").getOrCreate()\n",
    "\n",
    "# ×©×¨×©×•×¨ ×¢××•×“×•×ª\n",
    "data = [(\"James\", \"Bond\"), (\"Scott\", \"Varsa\")]\n",
    "df = spark.createDataFrame(data=data, schema=[\"col1\", \"col2\"])\n",
    "df.withColumn(\"Name\", expr(\"col1 || ',' || col2\")).show()\n",
    "\n",
    "# ×©×™××•×© ×‘-CASE WHEN\n",
    "data = [(\"James\", \"M\"), (\"Michael\", \"F\"), (\"Jen\", \"\")]\n",
    "columns = [\"name\", \"gender\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df2 = df.withColumn(\n",
    "    \"gender\",\n",
    "    expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n",
    "         \"WHEN gender = 'F' THEN 'Female' \" +\n",
    "         \"WHEN gender IS NULL THEN '' \" +\n",
    "         \"ELSE gender END\")\n",
    ")\n",
    "df2.show()\n",
    "\n",
    "# ×”×•×¡×¤×ª ×—×•×“×©×™× ××¢×¨×š ×‘×¢××•×“×”\n",
    "data = [(\"2019-01-23\", 1), (\"2019-06-24\", 2), (\"2019-09-20\", 3)]\n",
    "df = spark.createDataFrame(data=data, schema=[\"date\", \"increment\"])\n",
    "df.select(\n",
    "    df.date,\n",
    "    df.increment,\n",
    "    expr(\"add_months(date, increment) as inc_date\")\n",
    ").show()\n",
    "\n",
    "# ××ª×Ÿ ×›×™× ×•×™\n",
    "df.select(\n",
    "    df.date,\n",
    "    df.increment,\n",
    "    expr(\"\"\"add_months(date, increment) as inc_date\"\"\")\n",
    ").show()\n",
    "\n",
    "# ×¤×¢×•×œ×•×ª ×—×©×‘×•×Ÿ\n",
    "df.select(\n",
    "    df.date,\n",
    "    df.increment,\n",
    "    expr(\"increment + 5 as new_increment\")\n",
    ").show()\n",
    "\n",
    "# ×”××¨×ª ×˜×™×¤×•×¡×™×\n",
    "df.select(\n",
    "    \"increment\",\n",
    "    expr(\"cast(increment as string) as str_increment\")\n",
    ").printSchema()\n",
    "\n",
    "# ×¡×™× ×•×Ÿ\n",
    "data = [(100, 2), (200, 3000), (500, 500)]\n",
    "df = spark.createDataFrame(data=data, schema=[\"col1\", \"col2\"])\n",
    "df.filter(expr(\"col1 == col2\")).show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ¨ ×¡×™×›×•×\n",
    "\n",
    "×”×¤×•× ×§×¦×™×” `expr()` ×‘-PySpark ××¡×¤×§×ª ×“×¨×š ×œ×”×¨×™×¥ ×‘×™×˜×•×™×™× ×“××•×™×™ SQL ×¢× DataFrames, ×•×”×™× ×××¤×©×¨×ª:\n",
    "\n",
    "- âœ… ×©×™××•×© ×‘×¤×•× ×§×¦×™×•×ª SQL ×©××™× ×Ÿ ×–××™× ×•×ª ×™×©×™×¨×•×ª ×‘-PySpark\n",
    "- âœ… ×©×¨×©×•×¨ ×¢××•×“×•×ª ×¢× ××•×¤×¨×˜×•×¨ `||`\n",
    "- âœ… ×‘×™×˜×•×™×™ CASE WHEN\n",
    "- âœ… ×©×™××•×© ×‘×¢×¨×›×™× ××¢××•×“×•×ª ××—×¨×•×ª ×‘×ª×•×š ×‘×™×˜×•×™×™×\n",
    "- âœ… ×”××¨×ª ×˜×™×¤×•×¡×™×, ×¤×¢×•×œ×•×ª ×—×©×‘×•×Ÿ ×•×¡×™× ×•×Ÿ\n",
    "\n",
    "**×˜×™×¤ ×—×©×•×‘:** ×›××©×¨ ××©×ª××©×™× ×‘-`expr()`, ×”×‘×™×˜×•×™×™× ××™× × ×¢×•×‘×¨×™× ×‘×“×™×§×” ×‘×–××Ÿ ×§×•××¤×™×œ×¦×™×”, ×œ×›×Ÿ ×™×© ×œ×”×™×–×”×¨ ××˜×¢×•×™×•×ª ×›×ª×™×‘!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ××§×•×¨×•×ª <a id='references'></a>\n",
    "\n",
    "- [PySpark SQL Functions Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)\n",
    "- [SparkByExamples - PySpark expr() Function](https://sparkbyexamples.com/pyspark/pyspark-sql-expr-expression-function/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
