{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    body, * {\n",
    "        direction: rtl !important;\n",
    "        text-align: right !important;\n",
    "    }\n",
    "</style>\n",
    "# ğŸ“Š GroupBy ×•×¡×™×“×•×¨ ×‘×¡×“×¨ ×™×•×¨×“ ×‘-PySpark DataFrame\n",
    "[PySpark DataFrame groupBy and Sort by Descending Order](https://sparkbyexamples.com/pyspark/pyspark-dataframe-groupby-and-sort-by-descending-order/)\n",
    "## ğŸ“š ××‘×•×\n",
    "\n",
    "×‘-DataFrame ×©×œ PySpark, × ×¨××” ×›×™×¦×“ ×œ×¢×©×•×ª ××ª ×”×¤×¢×•×œ×•×ª ×”×‘××•×ª ×‘×¨×¦×£:\n",
    "\n",
    "1ï¸âƒ£ **×§×™×‘×•×¥ × ×ª×•× ×™×** - ×©×™××•×© ×‘×¤×•× ×§×¦×™×™×ª ××’×¨×’×¦×™×” `sum()`\n",
    "\n",
    "2ï¸âƒ£ **×¡×™× ×•×Ÿ ×”×ª×•×¦××•×ª** - ×©×™××•×© ×‘-`filter()` ×¢×œ ×§×‘×•×¦×•×ª\n",
    "\n",
    "3ï¸âƒ£ **××™×•×Ÿ ×”×ª×•×¦××•×ª** - ×©×™××•×© ×‘-`sort()` ××• `orderBy()` ×œ×¡×™×“×•×¨ ×‘×¡×“×¨ ×¢×•×œ×” ××• ×™×•×¨×“\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ ×”×›× ×ª ×”×¡×‘×™×‘×” ×•×”× ×ª×•× ×™×\n",
    "\n",
    "×‘×›×“×™ ×œ×”×“×’×™× ××ª ×›×œ ×”×¤×¢×•×œ×•×ª ×”×œ×œ×• ×™×—×“, × ×™×¦×•×¨ DataFrame ×©×œ PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×‘×•× ×¡×¤×¨×™×•×ª\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ×™×¦×™×¨×ª Spark Session\n",
    "spark = SparkSession.builder.appName(\"GroupByExample\").getOrCreate()\n",
    "\n",
    "# ×”×’×“×¨×ª × ×ª×•× ×™ ×“×•×’××”\n",
    "simpleData = [\n",
    "    (\"James\", \"Sales\", \"NY\", 90000, 34, 10000),\n",
    "    (\"Michael\", \"Sales\", \"NY\", 86000, 56, 20000),\n",
    "    (\"Robert\", \"Sales\", \"CA\", 81000, 30, 23000),\n",
    "    (\"Maria\", \"Finance\", \"CA\", 90000, 24, 23000),\n",
    "    (\"Raman\", \"Finance\", \"CA\", 99000, 40, 24000),\n",
    "    (\"Scott\", \"Finance\", \"NY\", 83000, 36, 19000),\n",
    "    (\"Jen\", \"Finance\", \"NY\", 79000, 53, 15000),\n",
    "    (\"Jeff\", \"Marketing\", \"CA\", 80000, 25, 18000),\n",
    "    (\"Kumar\", \"Marketing\", \"NY\", 91000, 50, 21000)\n",
    "]\n",
    "\n",
    "# ×”×’×“×¨×ª ×©××•×ª ×”×¢××•×“×•×ª\n",
    "columns = [\"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\"]\n",
    "\n",
    "# ×™×¦×™×¨×ª DataFrame\n",
    "df = spark.createDataFrame(data=simpleData, schema=columns)\n",
    "\n",
    "# ×”×¦×’×ª ×”× ×ª×•× ×™×\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1ï¸âƒ£ ×©×™××•×© ×‘-groupBy(), filter() ×•-sort()\n",
    "\n",
    "×œ×”×œ×Ÿ ×“×•×’××” ××œ××” ×©×œ ××™×š ×œ×¢×©×•×ª ×§×™×‘×•×¥ × ×ª×•× ×™× ×‘-DataFrame, ×¡×™× ×•×Ÿ ×•×¡×™×“×•×¨ ×‘×¡×“×¨ ×™×•×¨×“.\n",
    "\n",
    "### ğŸ“‹ ×”×¡×‘×¨ ×¢×œ ×”×ª×”×œ×™×š:\n",
    "\n",
    "×¨××©×™×ª, × ×¢×©×” **×§×™×‘×•×¥ PySpark ×¢×œ DataFrame** ×‘×××¦×¢×•×ª ×¤×•× ×§×¦×™×™×ª ××’×¨×’×¦×™×” `sum(\"salary\")`.\n",
    "\n",
    "`groupBy()` ××—×–×™×¨ ××•×‘×™×™×§×˜ `GroupedData` ×©××›×™×œ ×¤×•× ×§×¦×™×•×ª ××’×¨×’×¦×™×” ×›××•:\n",
    "- `sum()`\n",
    "- `max()`\n",
    "- `min()`\n",
    "- `mean()`\n",
    "- `count()`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×§×™×‘×•×¥ ×œ×¤×™ state ×•×—×™×©×•×‘ ×¡×›×•×\n",
    "from pyspark.sql.functions import sum, col, desc\n",
    "\n",
    "df.groupBy(\"state\").sum(\"salary\").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” ×”×¡×‘×¨ ××¤×•×¨×˜ ×¢×œ ×”×“×•×’××”\n",
    "\n",
    "×‘×“×•×’××” ×œ××¢×œ×”, ××™ ××¤×©×¨ ×œ×ª×ª ×©× alias ×œ×¢××•×“×ª ×”××’×¨×’×¦×™×” ×•×©× ×”×¢××•×“×” ×œ× ×™×“×™×“×•×ª×™ ×œ××©×ª××©.\n",
    "\n",
    "××›×™×•×•×Ÿ ×©×–×” ××—×–×™×¨ ×¡×›×•× ×©×œ ×©×›×¨, ×¢×“×™×£ ×œ×”×©×ª××© ×‘×¤×•× ×§×¦×™×™×ª SQL `sum()` ×©×œ ×”××—×œ×§×” ×”×–×•.\n",
    "\n",
    "×¢×›×©×™×• × ×¨××” ×›×™×¦×“ ×œ×”×©×ª××© ×‘-`agg()` ×›×“×™ ×œ×ª×ª alias ×‘×××¦×¢×•×ª ×¤×•× ×§×¦×™×” `alias()` ×©×œ ×”××—×œ×§×” ×”×–×•.\n",
    "\n",
    "×›××• ×›×Ÿ, ×›×“×™ ×œ×¡× ×Ÿ ×•×œ××™×™×Ÿ ×œ××—×¨ ×”×§×™×‘×•×¥, ×¦×¨×™×š ×œ×”×›×™×¨ ××ª ×©× ×”×¢××•×“×” ×”××“×•×™×§, ×œ×›×Ÿ × ×©×ª××© ×‘-`alias()` ×›×“×™ ×œ×¡×¤×§ ×©× ×¢××•×“×”:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ ×’×™×©×” ××©×•×¤×¨×ª - ×©×™××•×© ×‘-alias()\n",
    "\n",
    "×œ×”×œ×Ÿ ×“×•×’××” ×œ×§×™×‘×•×¥ ×¢× ××ª×Ÿ ×©× ×‘×¨×•×¨ ×œ×¢××•×“×”:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Group by ×‘×××¦×¢×•×ª agg() ×•-alias\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "dfGrouped = df.groupBy(\"state\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"))\n",
    "\n",
    "dfGrouped.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**×”×¡×‘×¨:**\n",
    "- ×©×™××•×© ×‘-`agg()` ×××¤×©×¨ ×œ× ×• ×œ×”×©×ª××© ×‘×¤×•× ×§×¦×™×•×ª ××’×¨×’×¦×™×” ××ª×§×“××•×ª\n",
    "- ×©×™××•×© ×‘-`alias()` × ×•×ª×Ÿ ×©× ××ª××™× ×œ×¢××•×“×ª ×”×ª×•×¦××”\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ ×¡×™× ×•×Ÿ ×œ××—×¨ ×§×™×‘×•×¥\n",
    "\n",
    "×œ×”×œ×Ÿ ×“×•×’××” ×œ×‘×—×™×¨×ª ×”× ×ª×•× ×™× ×©×¡×›×•× ×”×©×›×¨ ×©×œ×”× ×’×“×•×œ ×-100,000:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×¡×™× ×•×Ÿ ×œ××—×¨ ×§×™×‘×•×¥\n",
    "dfFilter = dfGrouped.filter(col(\"sum_salary\") > 100000)\n",
    "dfFilter.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ ××™×•×Ÿ ×‘×¡×“×¨ ×¢×•×œ×”\n",
    "\n",
    "×¢×›×©×™×• × ×©×ª××© ×‘-DataFrame sort() transformation ×©×œ PySpark ×›×“×™ ×œ××™×™×Ÿ ×œ×¤×™ ×¡×“×¨ ×¢×•×œ×” ×‘×‘×¨×™×¨×ª ××—×“×œ:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ××™×•×Ÿ ×‘×¡×“×¨ ×¢×•×œ×”\n",
    "\n",
    "dfFilter.sort(\"sum_salary\").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**×”×¢×¨×”:** ××™×•×Ÿ ×‘×¡×“×¨ ×¢×•×œ×” ×”×•× ×‘×¨×™×¨×ª ×”××—×“×œ.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ ××™×•×Ÿ ×‘×¡×“×¨ ×™×•×¨×“ - ×©×™××•×© ×‘-desc()\n",
    "\n",
    "×‘×›×“×™ ×œ×¢×©×•×ª ×¡×™×“×•×¨ ×‘×¡×“×¨ ×™×•×¨×“, × ×©×ª××© ×‘×¤×•× ×§×¦×™×™×ª Spark SQL `desc()`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ××™×•×Ÿ ×‘×¡×“×¨ ×™×•×¨×“\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "dfFilter.sort(desc(\"sum_salary\")).show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ ×—×œ×•×¤×”: ×©×™××•×© ×‘-SQL\n",
    "\n",
    "×œ×—×œ×•×¤×™×Ÿ, ××¤×©×¨ ×’× ×œ×”×©×ª××© ×‘×‘×™×˜×•×™ SQL ×›×“×™ ×œ×”×©×™×’ ××ª ××•×ª×” ×”×ª×•×¦××”:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×©×™××•×© ×‘-SQL\n",
    "df.createOrReplaceTempView(\"EMP\")\n",
    "\n",
    "spark.sql(\n",
    "    \"SELECT state, sum(salary) as sum_salary FROM EMP \" +\n",
    "    \"GROUP BY state \" +\n",
    "    \"HAVING sum(salary) > 100000 \" +\n",
    "    \"ORDER BY sum_salary DESC\"\n",
    ").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**×”×¢×¨×”:** ×’×™×©×ª SQL ××¡×¤×§×ª ×ª×—×‘×™×¨ ××•×›×¨ ×œ××™ ×©××›×™×¨ SQL ××¡×•×¨×ª×™.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ×§×•×“ ××œ× - ×“×•×’××” ××§×™×¤×”\n",
    "\n",
    "×œ×”×œ×Ÿ ×”×§×•×“ ×”××œ× ×”××©×œ×‘ ××ª ×›×œ ×”×©×œ×‘×™×:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×§×•×“ ××œ×\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, col, desc\n",
    "\n",
    "spark = SparkSession.builder.appName(\"GroupByExample\").getOrCreate()\n",
    "\n",
    "simpleData = [\n",
    "    (\"James\", \"Sales\", \"NY\", 90000, 34, 10000),\n",
    "    (\"Michael\", \"Sales\", \"NY\", 86000, 56, 20000),\n",
    "    (\"Robert\", \"Sales\", \"CA\", 81000, 30, 23000),\n",
    "    (\"Maria\", \"Finance\", \"CA\", 90000, 24, 23000),\n",
    "    (\"Raman\", \"Finance\", \"CA\", 99000, 40, 24000),\n",
    "    (\"Scott\", \"Finance\", \"NY\", 83000, 36, 19000),\n",
    "    (\"Jen\", \"Finance\", \"NY\", 79000, 53, 15000),\n",
    "    (\"Jeff\", \"Marketing\", \"CA\", 80000, 25, 18000),\n",
    "    (\"Kumar\", \"Marketing\", \"NY\", 91000, 50, 21000)\n",
    "]\n",
    "\n",
    "columns = [\"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "# ×”×¦×¢×” 1: ×©×™××•×© ×‘×¡×™×¡×™\n",
    "df.groupBy(\"state\").sum(\"salary\").show()\n",
    "\n",
    "# ×”×¦×¢×” 2: ×¢× alias ×•×¡×™× ×•×Ÿ\n",
    "dfGrouped = df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"bonus\").alias(\"sum_bonus\"))\n",
    "\n",
    "dfFilter = dfGrouped.filter(col(\"sum_bonus\") > 100000)\n",
    "dfFilter.show()\n",
    "\n",
    "# ××™×•×Ÿ ×‘×¡×“×¨ ×™×•×¨×“\n",
    "dfFilter.sort(desc(\"sum_bonus\")).show()\n",
    "\n",
    "# ×’×™×©×ª SQL\n",
    "df.createOrReplaceTempView(\"EMP\")\n",
    "spark.sql(\n",
    "    \"SELECT state, sum(salary) as sum_salary FROM EMP \" +\n",
    "    \"GROUP BY state \" +\n",
    "    \"HAVING sum(salary) > 100000 \" +\n",
    "    \"ORDER BY sum_salary DESC\"\n",
    ").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ×¡×™×›×•×\n",
    "\n",
    "×‘×“×•×’××ª PySpark ×–×• ×”×¡×‘×¨×ª×™ ×›×™×¦×“ ×œ×¢×©×•×ª groupBy(), filter() ×•-sort() ×‘×¡×“×¨ ×™×•×¨×“. ×× ×™ ××§×•×•×” ×©××”×‘×ª× ××ª ×–×”.\n",
    "\n",
    "**×œ×¡×™×›×•× ×”××•×©×’×™× ×©×œ××“× ×•:**\n",
    "\n",
    "âœ… **groupBy()** - ××§×‘×¥ × ×ª×•× ×™× ×œ×¤×™ ×¢××•×“×” ××• ××¡×¤×¨ ×¢××•×“×•×ª\n",
    "\n",
    "âœ… **agg()** - ×××¤×©×¨ ×©×™××•×© ×‘×¤×•× ×§×¦×™×•×ª ××’×¨×’×¦×™×” ××•×¨×›×‘×•×ª\n",
    "\n",
    "âœ… **alias()** - × ×•×ª×Ÿ ×©× ×‘×¨×•×¨ ×œ×¢××•×“×•×ª ××—×•×©×‘×•×ª\n",
    "\n",
    "âœ… **filter()** - ××¡× ×Ÿ ××ª ×”×ª×•×¦××•×ª ×œ×¤×™ ×ª× ××™\n",
    "\n",
    "âœ… **sort()/orderBy()** - ×××™×™×Ÿ ××ª ×”×ª×•×¦××•×ª\n",
    "\n",
    "âœ… **desc()** - ×××¤×©×¨ ××™×•×Ÿ ×‘×¡×“×¨ ×™×•×¨×“\n",
    "\n",
    "### ğŸ“š ××××¨×™× ×§×©×•×¨×™×\n",
    "- [PySpark Groupby Count Distinct](https://sparkbyexamples.com/pyspark/pyspark-groupby-count-distinct/)\n",
    "- [PySpark Groupby on Multiple Columns](https://sparkbyexamples.com/pyspark/pyspark-groupby-on-multiple-columns/)\n",
    "- [PySpark Groupby Agg (aggregate) â€“ Explained](https://sparkbyexamples.com/pyspark/pyspark-groupby-agg-aggregate/)\n",
    "- [PySpark GroupBy Count â€“ Explained](https://sparkbyexamples.com/pyspark/pyspark-groupby-count/)\n",
    "- [PySpark GroupBy Explained with Example](https://sparkbyexamples.com/pyspark/pyspark-groupby-explained-with-example/)\n",
    "- [PySpark Column alias after groupBy() Example](https://sparkbyexamples.com/pyspark/pyspark-column-alias-after-groupby-example/)\n",
    "- [PySpark distinct vs dropDuplicates](https://sparkbyexamples.com/pyspark/pyspark-distinct-vs-dropduplicates/)\n",
    "- [PySpark Get Number of Rows and Columns](https://sparkbyexamples.com/pyspark/pyspark-get-number-of-rows-and-columns/)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Learning! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
