{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    body, * {\n",
    "        direction: rtl !important;\n",
    "        text-align: right !important;\n",
    "    }\n",
    "</style>\n",
    "# ğŸ“Š PySpark: ×—×™×©×•×‘ ×”×”×‘×“×œ ×‘×™×Ÿ ×©× ×™ ×ª××¨×™×›×™× (×™××™×, ×—×•×“×©×™×, ×©× ×™×)\n",
    "[PySpark â€“ Difference between two dates (days, months, years)](https://sparkbyexamples.com/pyspark/pyspark-difference-between-two-dates-days-months-years/#datediff)\n",
    "## ×ª×•×›×Ÿ ×¢× ×™×™× ×™×\n",
    "1. [×§×‘×œ×ª ×”×”×‘×“×œ ×‘×™××™×](#section1)\n",
    "2. [×§×‘×œ×ª ×”×”×‘×“×œ ×‘×—×•×“×©×™×](#section2)\n",
    "3. [×§×‘×œ×ª ×”×”×‘×“×œ ×‘×©× ×™×](#section3)\n",
    "4. [×—×™×©×•×‘ ×”×‘×“×œ×™× ×›×©×”×ª××¨×™×›×™× ×‘×¤×•×¨××˜ ××•×ª××](#section4)\n",
    "5. [×“×•×’××” SQL ×œ×—×™×©×•×‘ ×”×”×‘×“×œ](#section5)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ××‘×•×\n",
    "\n",
    "×‘×××¦×¢×•×ª ×¤×•× ×§×¦×™×•×ª PySpark SQL ×›××• `datediff()` ×•-`months_between()`, × ×™×ª×Ÿ ×œ×—×©×‘ ××ª ×”×”×‘×“×œ ×‘×™×Ÿ ×©× ×™ ×ª××¨×™×›×™× ×‘×™××™×, ×‘×—×•×“×©×™× ×•×‘×©× ×™×.\n",
    "\n",
    "### ğŸ’¡ ×¡×§×™×¨×ª ×”×¤×•× ×§×¦×™×•×ª:\n",
    "- **`datediff()`** - ××—×©×‘×ª ×”×‘×“×œ ×‘×™××™× ×‘×™×Ÿ ×©× ×™ ×ª××¨×™×›×™×\n",
    "- **`months_between()`** - ××—×©×‘×ª ×”×‘×“×œ ×‘×—×•×“×©×™× ×‘×™×Ÿ ×©× ×™ ×ª××¨×™×›×™×\n",
    "\n",
    "×‘×•××• × ×¨××” ××ª ×–×” ×‘×××¦×¢×•×ª ×“×•×’××ª DataFrame. × ×™×ª×Ÿ ×’× ×œ×”×©×ª××© ×‘×¤×•× ×§×¦×™×•×ª ××œ×” ×œ×—×™×©×•×‘ ×’×™×œ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”§ ×”×›× ×ª ×”×¡×‘×™×‘×” ×•×™×¦×™×¨×ª DataFrame\n",
    "\n",
    "×¨××©×™×ª, × ×™×¦×•×¨ DataFrame ×¢× ×ª××¨×™×›×™× ×œ×“×•×’××”:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×™×‘×•× ×”×¡×¤×¨×™×•×ª ×”× ×“×¨×©×•×ª\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_date, datediff\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "# Create DataFrame\n",
    "data = [\n",
    "    ('1', '2019-07-01'),\n",
    "    ('2', '2019-06-24'),\n",
    "    ('3', '2019-08-24')\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=[\"id\", \"date\"])\n",
    "df.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section1\"></a>\n",
    "## 1ï¸âƒ£ ×§×‘×œ×ª ×”×”×‘×“×œ ×‘×™×Ÿ ×ª××¨×™×›×™× ×‘×™××™×\n",
    "\n",
    "### ğŸ“ ×”×¡×‘×¨\n",
    "\n",
    "×”×¤×•× ×§×¦×™×” `datediff()` ×”×™× ×¤×•× ×§×¦×™×” ×©×œ PySpark SQL ×”××©××©×ª ×œ×—×™×©×•×‘ ×”×”×‘×“×œ ×‘×™××™× ×‘×™×Ÿ ×©× ×™ ×ª××¨×™×›×™× ×©×¡×•×¤×§×•.\n",
    "\n",
    "×”×¤×•× ×§×¦×™×” `datediff()` × ×¤×•×¦×” ×‘×©×™××•×© ×‘×©××™×œ×ª×•×ª SQL ××• ×‘×¤×¢×•×œ×•×ª DataFrame ×›×“×™ ×œ×—×©×‘ ××ª ×”××©×š ×”×–××Ÿ ×‘×™×Ÿ ×©× ×™ timestamps ××• ×¢×¨×›×™ ×ª××¨×™×š.\n",
    "\n",
    "×‘×“×•×’××” ×œ××˜×”, ××—×©×‘ ××ª ×”×”×‘×“×œ×™× ×‘×™×Ÿ ×¢××•×“×ª ×”×ª××¨×™×š ×•×”×ª××¨×™×š ×”× ×•×›×—×™. ×›×“×™ ×œ×§×‘×œ ××ª ×”×ª××¨×™×š ×”× ×•×›×—×™, ××©×ª××© ×‘×¤×•× ×§×¦×™×” `current_date()`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×—×™×©×•×‘ ×”×”×‘×“×œ ×‘×™×Ÿ ×©× ×™ ×”×ª××¨×™×›×™×\n",
    "df2 = df.select(\n",
    "    col(\"date\"),\n",
    "    current_date().alias(\"current_date\"),\n",
    "    datediff(current_date(), col(\"date\")).alias(\"datediff\")\n",
    ")\n",
    "\n",
    "df2.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section2\"></a>\n",
    "## 2ï¸âƒ£ ×§×‘×œ×ª ×”×”×‘×“×œ ×‘×™×Ÿ ×ª××¨×™×›×™× ×‘×—×•×“×©×™×\n",
    "\n",
    "### ğŸ“ ×”×¡×‘×¨\n",
    "\n",
    "×”×©×ª××© ×‘×¤×•× ×§×¦×™×™×ª `months_between()` ×©×œ PySpark SQL ×›×“×™ ×œ×§×‘×œ ××ª ××¡×¤×¨ ×”×—×•×“×©×™× ×‘×™×Ÿ ×©× ×™ ×ª××¨×™×›×™×.\n",
    "\n",
    "×§×˜×¢ ×”×§×•×“ ×œ××˜×” ××—×©×‘ ×”×‘×“×œ×™ ×—×•×“×©×™× ×‘×™×Ÿ ×¢××•×“×ª ×”×ª××¨×™×š ××”-DataFrame ×•×”×ª××¨×™×š ×”× ×•×›×—×™, ×›×•×œ×œ ×”×‘×“×œ×™× ×‘×™××™× ×•×‘×—×•×“×©×™×."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×—×™×©×•×‘ ×”×”×‘×“×œ ×‘×™×Ÿ ×©× ×™ ×ª××¨×™×›×™× ×‘×—×•×“×©×™×\n",
    "from pyspark.sql.functions import col, current_date, datediff, months_between, round\n",
    "\n",
    "df3 = df.withColumn(\n",
    "    \"dateDiff\", \n",
    "    datediff(current_date(), col(\"date\"))\n",
    ").withColumn(\n",
    "    \"monthsDiff\", \n",
    "    months_between(current_date(), col(\"date\"))\n",
    ").withColumn(\n",
    "    \"monthsDiff_round\", \n",
    "    round(months_between(current_date(), col(\"date\")), 2)\n",
    ")\n",
    "\n",
    "df3.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” ×”×¡×‘×¨ ×¢×œ ×”×§×•×“:\n",
    "\n",
    "×–×” ×× ×™×‘ ××ª ×”×¤×œ×˜ ×”×‘×. ×–×” ××•×¡×™×£ ×¢××•×“×•×ª ×—×“×©×•×ª ×‘×©×:\n",
    "- **`dateDiff`** - ×”×”×‘×“×œ ×‘×™××™×\n",
    "- **`monthsDiff`** - ×”×”×‘×“×œ ×‘×—×•×“×©×™× (××“×•×™×§)\n",
    "- **`monthsDiff_round`** - ×”×”×‘×“×œ ×‘×—×•×“×©×™× ××¢×•×’×œ ×œ×©× ×™ ××§×•××•×ª ×¢×©×¨×•× ×™×™×\n",
    "\n",
    "×œ-DataFrame, ×”-`monthsDiff_round` ××™×™×¦×’ ××ª ×”×”×‘×“×œ ×‘×—×•×“×©×™× ×‘×™×Ÿ ×”×ª××¨×™×š ×”× ×•×›×—×™ ×œ×‘×™×Ÿ ×”×ª××¨×™×›×™× ×‘×¢××•×“×ª \"date\", ××¢×•×’×œ ×œ×©× ×™ ××§×•××•×ª ×¢×©×¨×•× ×™×™×."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section3\"></a>\n",
    "## 3ï¸âƒ£ ×§×‘×œ×ª ×”×”×‘×“×œ ×‘×™×Ÿ ×ª××¨×™×›×™× ×‘×©× ×™×\n",
    "\n",
    "### ğŸ“ ×”×¡×‘×¨\n",
    "\n",
    "×›×“×™ ×œ×—×©×‘ ××ª ×”×”×‘×“×œ ×‘×™×Ÿ ×©× ×™ ×ª××¨×™×›×™× ×‘×©× ×™× ×‘×××¦×¢×•×ª PySpark, × ×™×ª×Ÿ ×œ×”×©×ª××© ×‘×¤×•× ×§×¦×™×” `months_between()` ×›×“×™ ×œ×§×‘×œ ××ª ×”×”×‘×“×œ ×‘×—×•×“×©×™× ×•××– ×œ×”××™×¨ ××•×ª×• ×œ×©× ×™×.\n",
    "\n",
    "### ğŸ”¢ ×©×™×˜×•×ª ×—×™×©×•×‘:\n",
    "\n",
    "**×©×™×˜×” 1: ×—×œ×•×§×” ×‘-12**\n",
    "```python\n",
    "months_between() / 12\n",
    "```\n",
    "\n",
    "**×©×™×˜×” 2 (××œ×˜×¨× ×˜×™×‘×™×ª): ×©×™××•×© ×‘-datediff()**\n",
    "```python\n",
    "datediff() / 365.25\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×—×™×©×•×‘ ×”×”×‘×“×œ ×‘×™×Ÿ ×©× ×™ ×ª××¨×™×›×™× ×‘×©× ×™×\n",
    "from pyspark.sql.functions import col, current_date, datediff, months_between, round, lit\n",
    "\n",
    "df4 = df.withColumn(\n",
    "    \"dateDiff\", \n",
    "    datediff(current_date(), col(\"date\"))\n",
    ").withColumn(\n",
    "    \"yearsDiff\", \n",
    "    months_between(current_date(), col(\"date\")) / lit(12)\n",
    ").withColumn(\n",
    "    \"yearsDiff_round\", \n",
    "    round(months_between(current_date(), col(\"date\")) / lit(12), 2)\n",
    ")\n",
    "\n",
    "df4.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” ×”×¡×‘×¨ ×¢×œ ×”×§×•×“:\n",
    "\n",
    "×”×§×•×“ ××•×¡×™×£ ×©×œ×•×© ×¢××•×“×•×ª ×—×“×©×•×ª:\n",
    "1. **`dateDiff`** - ×”×”×‘×“×œ ×‘×™××™×\n",
    "2. **`yearsDiff`** - ×”×”×‘×“×œ ×‘×©× ×™× (××“×•×™×§)\n",
    "3. **`yearsDiff_round`** - ×”×”×‘×“×œ ×‘×©× ×™× ××¢×•×’×œ ×œ×©× ×™ ××§×•××•×ª ×¢×©×¨×•× ×™×™×\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ ×“×¨×š ×—×œ×•×¤×™×ª - ×©×™××•×© ×‘-datediff()\n",
    "\n",
    "×œ×—×œ×•×¤×™×Ÿ, × ×™×ª×Ÿ ×’× ×œ×”×©×ª××© ×‘×¤×•× ×§×¦×™×” `datediff()` ×›×“×™ ×œ×§×‘×œ ××ª ×”×”×‘×“×œ ×‘×™××™× ×•××– ×œ×”××™×¨ ××•×ª×• ×œ×©× ×™×:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×©×™××•×© ×‘-datediff()\n",
    "from pyspark.sql.functions import expr\n",
    "result = df.withColumn(\n",
    "    \"years_diff\", \n",
    "    expr(\"datediff(current_date(), date) / 365.25\")\n",
    ").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section4\"></a>\n",
    "## 4ï¸âƒ£ ×—×™×©×•×‘ ×”×‘×“×œ×™× ×›×©×”×ª××¨×™×›×™× ×‘×¤×•×¨××˜ ××•×ª×× ××™×©×™×ª\n",
    "\n",
    "### ğŸ“ ×”×¡×‘×¨\n",
    "\n",
    "×‘×•××• × ×¨××” ×“×•×’××” × ×•×¡×¤×ª ×©×œ ×”×”×‘×“×œ ×‘×™×Ÿ ×©× ×™ ×ª××¨×™×›×™× ×›××©×¨ ×”×ª××¨×™×›×™× ××™× × ×‘×¤×•×¨××˜ DateType `yyyy-MM-dd`.\n",
    "\n",
    "### âš ï¸ ×—×©×•×‘!\n",
    "×›××©×¨ ×”×ª××¨×™×›×™× ××™× × ×‘×¤×•×¨××˜ DateType, ×›×œ ×¤×•× ×§×¦×™×•×ª ×”×ª××¨×™×š ××—×–×™×¨×•×ª null. \n",
    "\n",
    "×œ×›×Ÿ, ×¦×¨×™×š ×ª×—×™×œ×” ×œ×”××™×¨ ××ª ×ª××¨×™×š ×”×§×œ×˜ ×œ×¡×•×’ DateType ×‘×××¦×¢×•×ª ×”×¤×•× ×§×¦×™×” `to_date()` ×•××– ×œ×—×©×‘ ××ª ×”×”×‘×“×œ×™×."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×™×¦×™×¨×ª DataFrame ×¢× ×¤×•×¨××˜ ×ª××¨×™×š ××•×ª×× ××™×©×™×ª\n",
    "from pyspark.sql.functions import current_date, to_date\n",
    "\n",
    "data2 = [\n",
    "    ('1', '07-01-2019'),\n",
    "    ('2', '06-24-2019'),\n",
    "    ('3', '08-24-2019')\n",
    "]\n",
    "\n",
    "df2 = spark.createDataFrame(data=data2, schema=[\"id\", \"date\"])\n",
    "\n",
    "# ×”××¨×” ×•×—×™×©×•×‘\n",
    "df2.select(\n",
    "    months_between(current_date(),to_date(col(\"date\"), \"MM-dd-yyyy\")).alias(\"monthsDiff\")\n",
    ").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section5\"></a>\n",
    "## 5ï¸âƒ£ ×“×•×’××” SQL ×œ×§×‘×œ×ª ×”×”×‘×“×œ ×‘×™×Ÿ ×ª××¨×™×›×™×\n",
    "\n",
    "### ğŸ“ ×”×¡×‘×¨\n",
    "\n",
    "×‘×•××• × ×©×ª××© ×‘×“×•×’××ª SQL ×œ×—×™×©×•×‘ ×”×”×‘×“×œ ×‘×™×Ÿ ×©× ×™ ×ª××¨×™×›×™× ×‘×©× ×™×. ×‘××•×¤×Ÿ ×“×•××”, × ×™×ª×Ÿ ×œ×—×©×‘ ××ª ×”×™××™× ×•×”×—×•×“×©×™× ×‘×™×Ÿ ×©× ×™ ×ª××¨×™×›×™×.\n",
    "\n",
    "×›××Ÿ, ××©×ª××©×™× ×‘-`spark.sql()` ×›×“×™ ×œ×”×¨×™×¥ ××ª ×©××™×œ×ª×•×ª ×”-SQL ×‘×××¦×¢×•×ª ×”×¤×•× ×§×¦×™×•×ª ×©×”×•×¡×‘×¨×• ×œ××¢×œ×”."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×“×•×’××ª SQL\n",
    "spark.sql(\n",
    "    \"select round(months_between(current_date(),'2019-07-01')/12, 2) as years_diff\"\n",
    ").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“‹ ×“×•×’××” ××œ××” - ×›×œ ×”×§×•×“ ×‘×™×—×“\n",
    "\n",
    "### ğŸ“ ×§×•×“ ×©×œ×\n",
    "\n",
    "×œ×”×œ×Ÿ ×”×§×•×“ ×”××œ× ×¢× ×›×œ ×”×“×•×’×××•×ª:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ×§×•×“ ××œ× - ×›×œ ×”×“×•×’×××•×ª ×‘×™×—×“\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_date, datediff, months_between, round, lit, to_date\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "# Create DataFrame\n",
    "data = [\n",
    "    ('1', '2019-07-01'),\n",
    "    ('2', '2019-06-24'),\n",
    "    ('3', '2019-08-24')\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=[\"id\", \"date\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1. ×”×”×‘×“×œ ×‘×™××™×\n",
    "df2 = df.select(\n",
    "    col(\"date\"),\n",
    "    current_date().alias(\"current_date\"),\n",
    "    datediff(current_date(), col(\"date\")).alias(\"datediff\")\n",
    ")\n",
    "df2.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 2. ×”×”×‘×“×œ ×‘×—×•×“×©×™×\n",
    "df3 = df.withColumn(\n",
    "    \"dateDiff\", \n",
    "    datediff(current_date(), col(\"date\"))\n",
    ").withColumn(\n",
    "    \"monthsDiff\", \n",
    "    months_between(current_date(), col(\"date\"))\n",
    ").withColumn(\n",
    "    \"monthsDiff_round\", \n",
    "    round(months_between(current_date(), col(\"date\")), 2)\n",
    ")\n",
    "df3.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 3. ×”×”×‘×“×œ ×‘×©× ×™×\n",
    "df4 = df.withColumn(\n",
    "    \"dateDiff\", \n",
    "    datediff(current_date(), col(\"date\"))\n",
    ").withColumn(\n",
    "    \"yearsDiff\", \n",
    "    months_between(current_date(), col(\"date\")) / lit(12)\n",
    ").withColumn(\n",
    "    \"yearsDiff_round\", \n",
    "    round(months_between(current_date(), col(\"date\")) / lit(12), 2)\n",
    ")\n",
    "df4.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 4. SQL ×“×•×’××”\n",
    "spark.sql(\n",
    "    \"select round(months_between('2019-07-01', current_date())/12, 2) as years_diff\"\n",
    ").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ×¡×™×›×•×\n",
    "\n",
    "×‘××“×¨×™×š ×–×”, ×œ××“×ª ×›×™×¦×“ ×œ×—×©×‘ ×™××™×, ×—×•×“×©×™× ×•×©× ×™× ×‘×™×Ÿ ×©× ×™ ×ª××¨×™×›×™× ×‘×××¦×¢×•×ª:\n",
    "\n",
    "âœ… **`datediff()`** - ×œ×—×™×©×•×‘ ×”×‘×“×œ ×‘×™××™×\n",
    "\n",
    "âœ… **`months_between()`** - ×œ×—×™×©×•×‘ ×”×‘×“×œ ×‘×—×•×“×©×™× ×•×©× ×™×\n",
    "\n",
    "âœ… ×“×•×’×××•×ª DataFrame ×•-SQL\n",
    "\n",
    "âœ… ×˜×™×¤×•×œ ×‘×¤×•×¨××˜×™× ××•×ª×××™× ××™×©×™×ª\n",
    "\n",
    "× ×™×ª×Ÿ ×œ××¦×•× ××™×“×¢ × ×•×¡×£ ×¢×œ ×¤×•× ×§×¦×™×•×ª ××œ×” ×‘-[×¤×•× ×§×¦×™×•×ª ×ª××¨×™×š ×•×–××Ÿ ×©×œ PySpark](https://sparkbyexamples.com/pyspark/pyspark-sql-date-and-timestamp-functions/).\n",
    "\n",
    "**ğŸ‰ ×œ××™×“×” ××”× ×”!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š ××××¨×™× ×§×©×•×¨×™×\n",
    "\n",
    "- [PySpark SQL â€“ How to Get Current Date & Timestamp](https://sparkbyexamples.com/pyspark/pyspark-sql-how-to-get-current-date-timestamp/)\n",
    "- [PySpark SQL â€“ Working with Unix Time](https://sparkbyexamples.com/pyspark/pyspark-sql-unix-timestamp/)\n",
    "- [PySpark Timestamp Difference (seconds, minutes, hours)](https://sparkbyexamples.com/pyspark/pyspark-timestamp-difference/)\n",
    "- [PySpark SQL â€“ Convert Date to String Format](https://sparkbyexamples.com/pyspark/pyspark-sql-convert-date-to-string-format/)\n",
    "- [PySpark SQL â€“ Convert String to Date Format](https://sparkbyexamples.com/pyspark/pyspark-sql-convert-string-to-date-format/)\n",
    "- [PySpark SQL â€“ Convert Timestamp to Date](https://sparkbyexamples.com/pyspark/pyspark-to-date-convert-timestamp-to-date/)\n",
    "- [Pyspark to_date() vs date_format()](https://sparkbyexamples.com/pyspark/pyspark-to-date-vs-date-format/)\n",
    "\n",
    "---\n",
    "\n",
    "**Â© SparkByExamples.com - All rights reserved**\n",
    "\n",
    "*××“×¨×™×š ×–×” ×ª×•×¨×’× ×•×¢×•×‘×“ ×œ×¢×‘×¨×™×ª ×œ××˜×¨×•×ª ×œ×™××•×“*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
